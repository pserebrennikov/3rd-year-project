{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pserebrennikov/3rd-year-project/blob/master/5_automatic_differentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "green-beginning",
      "metadata": {
        "id": "green-beginning"
      },
      "source": [
        "# Tutorial 5 - Automatic differentiation\n",
        "### Course on Optimization for Machine Learning - Dr. F. Ballarin\n",
        "### Master Degree in Data Analytics for Business, Catholic University of the Sacred Heart, Milano"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "federal-harassment",
      "metadata": {
        "id": "federal-harassment"
      },
      "source": [
        "In this notebook we discuss automatic differentiation and also introduce the [`JAX library`](https://github.com/google/jax), a Google research project that includes automatic differentiation capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30dd78f5",
      "metadata": {
        "id": "30dd78f5"
      },
      "outputs": [],
      "source": [
        "import typing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "numerous-arizona",
      "metadata": {
        "id": "numerous-arizona"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "configured-netherlands",
      "metadata": {
        "id": "configured-netherlands"
      },
      "source": [
        "## Exercise 5.1\n",
        "\n",
        "1. Implement forward and backward passes for the expression $(a + b) \\cdot c$, with $a = -2$, $b = 5$, $c = -4$.\n",
        "\n",
        "*Solution*\n",
        "> We follow the computational graph we have derived for the the evaluation of the expression..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "willing-message",
      "metadata": {
        "id": "willing-message"
      },
      "outputs": [],
      "source": [
        "# Nodes in the first layer\n",
        "a = - 2\n",
        "b = 5\n",
        "c = - 4\n",
        "\n",
        "# Nodes in the second layer\n",
        "d = a + b\n",
        "\n",
        "# Nodes in the third layer\n",
        "e = d * c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rural-wallet",
      "metadata": {
        "id": "rural-wallet"
      },
      "outputs": [],
      "source": [
        "e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "underlying-vaccine",
      "metadata": {
        "id": "underlying-vaccine"
      },
      "outputs": [],
      "source": [
        "assert e == -12"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "patent-triangle",
      "metadata": {
        "id": "patent-triangle"
      },
      "source": [
        "> ... and its gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "saving-maria",
      "metadata": {
        "id": "saving-maria"
      },
      "outputs": [],
      "source": [
        "# Nodes in the third layer\n",
        "de_de = 1\n",
        "\n",
        "# Edges starting from any node in the third layer\n",
        "de_dd = c\n",
        "de_dc = d\n",
        "\n",
        "# Nodes in the second layer\n",
        "de_dd = de_de * de_dd\n",
        "\n",
        "# Edges starting from any node in the second layer\n",
        "dd_da = 1\n",
        "dd_db = 1\n",
        "\n",
        "# Nodes in the first layer\n",
        "de_da = de_dd * dd_da\n",
        "de_db = de_dd * dd_db\n",
        "de_dc = de_de * de_dc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "substantial-antigua",
      "metadata": {
        "id": "substantial-antigua"
      },
      "outputs": [],
      "source": [
        "(de_da, de_db, de_dc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "military-smoke",
      "metadata": {
        "id": "military-smoke"
      },
      "outputs": [],
      "source": [
        "assert (de_da, de_db, de_dc) == (-4, -4, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fifth-socket",
      "metadata": {
        "id": "fifth-socket"
      },
      "source": [
        "2. Implement forward and backward passes for the expression $(a + b) \\cdot (b + 1)$, with $a = 2$, $b = 1$.\n",
        "\n",
        "*Solution*\n",
        "> We follow the computational graph we have derived for the the evaluation of the expression..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "violent-civilian",
      "metadata": {
        "id": "violent-civilian"
      },
      "outputs": [],
      "source": [
        "# Nodes in the first layer\n",
        "a = 2\n",
        "b = 1\n",
        "\n",
        "# Nodes in the second layer\n",
        "c = a + b\n",
        "d = b + 1\n",
        "\n",
        "# Nodes in the third layer\n",
        "e = c * d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cross-armor",
      "metadata": {
        "id": "cross-armor"
      },
      "outputs": [],
      "source": [
        "e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "appropriate-suspension",
      "metadata": {
        "id": "appropriate-suspension"
      },
      "outputs": [],
      "source": [
        "assert e == 6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ancient-canberra",
      "metadata": {
        "id": "ancient-canberra"
      },
      "source": [
        "> ... and its gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "organizational-traveler",
      "metadata": {
        "id": "organizational-traveler"
      },
      "outputs": [],
      "source": [
        "# Nodes in the third layer\n",
        "de_de = 1\n",
        "\n",
        "# Edges starting from any node in the third layer\n",
        "de_dc = d\n",
        "de_dd = c\n",
        "\n",
        "# Nodes in the second layer\n",
        "de_dc = de_de * de_dc\n",
        "de_dd = de_de * de_dd\n",
        "\n",
        "# Edges starting from any node in the second layer\n",
        "dc_da = 1\n",
        "dc_db = 1\n",
        "dd_db = 1\n",
        "\n",
        "# Nodes in the first layer\n",
        "de_da = de_dc * dc_da\n",
        "de_db = de_dc * dc_db + de_dd * dd_db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "working-subdivision",
      "metadata": {
        "id": "working-subdivision"
      },
      "outputs": [],
      "source": [
        "(de_da, de_db)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "local-blink",
      "metadata": {
        "id": "local-blink"
      },
      "outputs": [],
      "source": [
        "assert (de_da, de_db) == (2, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "subject-spanking",
      "metadata": {
        "id": "subject-spanking"
      },
      "source": [
        "3. Let $\\boldsymbol{w} \\in \\mathbb{R}^2$. Consider the *Ackley function*\n",
        "$$f(\\boldsymbol{w}) = - 20 \\exp\\left(- 0.2 \\sqrt{\\frac{[w^{(0)}]^2 + [w^{(1)}]^2}{2}}\\right) - \\exp\\left(\\frac{\\cos(2 \\pi w^{(0)}) + \\cos(2 \\pi w^{(1)})}{2}\\right) + 20 + \\exp(1).$$\n",
        "\n",
        "   Compute the relative error between the evaluation of the exact derivative, which have used in Exercises 1.4 and 1.6, and the result of reverse automatic differentiation\n",
        "\n",
        "*Solution*:\n",
        "> The evaluation of the Ackley function requires the following operations:\n",
        "> * 5 sums\n",
        "> * 8 multiplications or divisions\n",
        "> * 2 squares\n",
        "> * 1 square root\n",
        "> * 2 cosine evaluations\n",
        "> * 3 applications of $\\exp$\n",
        ">\n",
        "> for a total of 21 operations. This means that the computational graph will have 23 nodes (2 inputs + 21 operations). Manually drawing and manually coding the computational graph would be a very tedious and error prone task. Fortunately, there are several automatic differentation libraries that carry out this task. Today we will use the [`JAX library`](https://github.com/google/jax), a Google research project that includes automatic differentiation capabilities.\n",
        ">\n",
        "> `jax` defines an interface very similar to `numpy` in the submodule `jax.numpy`, which we imported as `jnp`.\n",
        "> Using `jnp` rather than `np` is a technical detail which `jax` needs so that it can create the computational graph.\n",
        ">\n",
        "> We then implement the Ackley function using `jnp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "confused-cliff",
      "metadata": {
        "id": "confused-cliff"
      },
      "outputs": [],
      "source": [
        "def f_ex_5_1(w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate f(w).\"\"\"\n",
        "    return (\n",
        "        - 20 * jnp.exp(- 0.2 * jnp.sqrt((w[0]**2 + w[1]**2) / 2))\n",
        "        - jnp.exp((jnp.cos(2 * jnp.pi * w[0]) + jnp.cos(2 * jnp.pi * w[1])) / 2)\n",
        "        + 20 + jnp.exp(1)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "embedded-decrease",
      "metadata": {
        "id": "embedded-decrease"
      },
      "source": [
        "> Note that if we evaluate the Ackley function at the point $(1, 1)$, the result is stored in a new datatype `DeviceArray`, which is again part of `jax`. Again, this is a technical detail related to their implementation, and for our goals we will consider it simply a representation of a scalar (for arrays with a single entry) or vectors (for arrays with multiple entries)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "numerous-fountain",
      "metadata": {
        "id": "numerous-fountain"
      },
      "outputs": [],
      "source": [
        "f_ex_5_1(np.array([1.0, 1.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "located-thriller",
      "metadata": {
        "id": "located-thriller"
      },
      "source": [
        "> `jax` offers a `grad` command that will create the computational graph and perform reverse automatic differentiation of the function provided as the first input argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spatial-reasoning",
      "metadata": {
        "id": "spatial-reasoning"
      },
      "outputs": [],
      "source": [
        "grad_f_ex_5_1_ad = jax.grad(f_ex_5_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "average-casino",
      "metadata": {
        "id": "average-casino"
      },
      "outputs": [],
      "source": [
        "grad_f_ex_5_1_ad(np.array([1.0, 1.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "progressive-approach",
      "metadata": {
        "id": "progressive-approach"
      },
      "source": [
        "> To compare the result of the automatic differentation with the exact differentiation, we copy the gradient $\\nabla f$ from Exercises 1.4 and 1.6.\n",
        ">\n",
        "> Should we be using `np` or should we be using `jnp` here? The rule of thumb is: use `jnp` for any expression that we would like to differentiate (like the implementation of $f$, above). Use `np` for any expression that you are not interested in differentiating (like the implementation of $\\nabla f$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "widespread-philosophy",
      "metadata": {
        "id": "widespread-philosophy"
      },
      "outputs": [],
      "source": [
        "def grad_f_ex_5_1_exact(w: np.ndarray) -> np.ndarray:\n",
        "    r\"\"\"Evaluate \\nabla f(w) on paper, for comparison with the result from jax.\"\"\"\n",
        "    return np.array([\n",
        "        2.0 * w[0] * np.exp(- 0.2 * np.sqrt(w[0]**2 / 2 + w[1]**2 / 2)) / np.sqrt(w[0]**2 / 2 + w[1]**2 / 2)\n",
        "        + np.pi * np.exp(np.cos(2 * np.pi * w[0]) / 2 + np.cos(2 * np.pi * w[1]) / 2) * np.sin(2 * np.pi * w[0]),\n",
        "        2.0 * w[1] * np.exp(- 0.2 * np.sqrt(w[0]**2 / 2 + w[1]**2 / 2)) / np.sqrt(w[0]**2 / 2 + w[1]**2 / 2)\n",
        "        + np.pi * np.exp(np.cos(2 * np.pi * w[0]) / 2 + np.cos(2 * np.pi * w[1]) / 2) * np.sin(2 * np.pi * w[1])\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "extended-miller",
      "metadata": {
        "id": "extended-miller"
      },
      "outputs": [],
      "source": [
        "grad_f_ex_5_1_exact(np.array([1.0, 1.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "broadband-passenger",
      "metadata": {
        "id": "broadband-passenger"
      },
      "source": [
        "> We compare the automatic differentiation and the exact differentiation on $14^2$ points on an equispaced rectangular grid in $[-1, 1]^2$. The comparison is carried out in terms of relative error. Given a number $a$ and its approximation $b$, the relative error is defined as\n",
        "$$ \\frac{\\left|b - a\\right|}{\\left|a\\right|}.$$\n",
        "Similarly, given a vector $\\boldsymbol{v}$ and its approximation $\\boldsymbol{z}$, the relative error is defined as\n",
        "$$ \\frac{\\left\\|\\boldsymbol{z} - \\boldsymbol{v}\\right\\|}{\\left\\|\\boldsymbol{v}\\right\\|}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "massive-preference",
      "metadata": {
        "id": "massive-preference"
      },
      "outputs": [],
      "source": [
        "grad_relative_errors = []\n",
        "for w_i in np.linspace(-1, 1, 14):\n",
        "    for w_j in np.linspace(-1, 1, 14):\n",
        "        w_ij = np.array([w_i, w_j])\n",
        "        grad_f_ij_exact = grad_f_ex_5_1_exact(w_ij)\n",
        "        grad_f_ij_ad = grad_f_ex_5_1_ad(w_ij)\n",
        "        grad_f_ij_error = grad_f_ij_exact - grad_f_ij_ad\n",
        "        grad_relative_errors.append(np.linalg.norm(grad_f_ij_error) / np.linalg.norm(grad_f_ij_exact))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "applied-statement",
      "metadata": {
        "id": "applied-statement"
      },
      "outputs": [],
      "source": [
        "np.max(grad_relative_errors)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "foster-establishment",
      "metadata": {
        "id": "foster-establishment"
      },
      "source": [
        "> The relative error in the evaluation of the gradient is $O(10^{-6})$, which is a reasonable accuracy for most applications in machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "contrary-crash",
      "metadata": {
        "id": "contrary-crash"
      },
      "source": [
        "## Exercise 5.2\n",
        "\n",
        "The California Housing Data Set contains data collected during the 1990 U.S. Census. The dataset has divided California in neighborhoods (called blocks), and contains the following fields:\n",
        "* <code><font color=\"blue\">longitude</font></code>: longitudinal position of the block (neighborhood, composed of several houses), i.e. a measure of how far west the block is,\n",
        "* <code><font color=\"blue\">latitude</font></code>: latitudinal position of the block, i.e. a measure of how far north the block is,\n",
        "* <code><font color=\"blue\">housing_median_age</font></code>: median age of houses (measured in years) within the block,\n",
        "* <code><font color=\"blue\">total_rooms</font></code>: total number of rooms within the block,\n",
        "* <code><font color=\"blue\">total_bedrooms</font></code>: total number of bedrooms within the block,\n",
        "* <code><font color=\"red\">population</font></code>: total number of people residing within the block,\n",
        "* <code><font color=\"blue\">households</font></code>: total number of households, i.e. a number of home units or apartments, for the block,\n",
        "* <code><font color=\"red\">median_income</font></code>: median income for households within the block of houses (measured in tens of thousands of US dollars),\n",
        "* <code><font color=\"blue\">median_house_value</font></code>: median house value for households within the block (measured in US dollars).\n",
        "\n",
        "A non-profit company, based in California, would like to fund urban requalification projects for <code><font color=\"red\">highly populated</font></code> blocks with <code><font color=\"red\">low income</font></code> residents. Unfortunately, they cannot just use an up-to-date (updated to last year, not 1990) version of the <code><font color=\"red\">population</font></code> and <code><font color=\"red\">median_income</font></code> fields, because:\n",
        "* interviewing every single household to ask for a precise and an up-to-date quantification of the <code><font color=\"red\">population</font></code> would take too much time,\n",
        "* their ethics committee has advised against asking every single resident their income (from which <code><font color=\"red\">median_income</font></code> could be computed).\n",
        "\n",
        "They are currently collecting up-to-date information on the remaining fields, written in <code><font color=\"blue\">blue</font></code>:\n",
        "* the fields <code><font color=\"blue\">longitude</font></code> and <code><font color=\"blue\">latitude</font></code> will not have changed since 1990,\n",
        "* the fields <code><font color=\"blue\">housing_median_age</font></code>, <code><font color=\"blue\">total_rooms</font></code>, <code><font color=\"blue\">total_bedrooms</font></code>, <code><font color=\"blue\">households</font></code> can be inferred from public records,\n",
        "* they are willing to have technicians provide an up-to-date estimate of the current <code><font color=\"blue\">median_house_value</font></code>. (The ethics committee has not advised against collecting this data, and the company feels that this field can surely be used to describe the impact of their urban requalification project to the residents. Indeed, after requalification the house value is expected to increase).\n",
        "\n",
        "We have been contacted and asked to provide them with a model that uses the fields written in <code><font color=\"blue\">blue</font></code> to predict <code><font color=\"red\">highly populated</font></code> blocks with <code><font color=\"red\">low income</font></code> residents. Our model will be trained and tested on the 1990 dataset. After validation, the company will then deploy it on the updated data they are currently collecting, and it will be used to help them identify which blocks might be candidates for one of their urban requalification project.\n",
        "\n",
        "1. Load and clean the training dataset.\n",
        "\n",
        "*Solution*:\n",
        "> The dataset is already available on Colab, in the `sample_data` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "passing-adolescent",
      "metadata": {
        "id": "passing-adolescent"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "340a9cd0",
      "metadata": {
        "id": "340a9cd0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hydraulic-pattern",
      "metadata": {
        "id": "hydraulic-pattern"
      },
      "outputs": [],
      "source": [
        "if os.path.isfile(\"data/california_housing_train.csv\"):\n",
        "    csv_path = \"data/california_housing_train.csv\"\n",
        "elif os.path.isfile(\"sample_data/california_housing_train.csv\"):\n",
        "    csv_path = \"sample_data/california_housing_train.csv\"\n",
        "else:\n",
        "    csv_path = (\n",
        "        \"https://dmf.unicatt.it/~fball/public/optimization_for_machine_learning\"\n",
        "        + \"/california_housing_train.csv\"\n",
        "    )\n",
        "train = pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "christian-seafood",
      "metadata": {
        "id": "christian-seafood"
      },
      "source": [
        "> We display the first few lines of the dataset, and some basic information from which we can see basic statistics and check that there are no missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mounted-fence",
      "metadata": {
        "id": "mounted-fence"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "reflected-convert",
      "metadata": {
        "id": "reflected-convert"
      },
      "outputs": [],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "charitable-perth",
      "metadata": {
        "id": "charitable-perth"
      },
      "outputs": [],
      "source": [
        "train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "right-baltimore",
      "metadata": {
        "id": "right-baltimore"
      },
      "source": [
        "> Looking at the maximum values we see that there are two fields that seem odd:\n",
        "> * the maximum of `housing_median_age` is only 52. It seems a bit odd that none of the blocks have been built before 1938, so we should better investigate this field.\n",
        "> * the maximum of `median_house_value` (the field we are interested in predicting) is 500 001, which is strangely close to the round number 500 000.\n",
        ">\n",
        "> We graphically investigate these two issues using the `seaborn` library, to help us in the visualization of histograms of each field and an estimation of the density function of the field distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "champion-donna",
      "metadata": {
        "id": "champion-donna"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "editorial-judges",
      "metadata": {
        "id": "editorial-judges"
      },
      "source": [
        "> We start from the median house value field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "criminal-scout",
      "metadata": {
        "id": "criminal-scout"
      },
      "outputs": [],
      "source": [
        "sns.histplot(train[\"median_house_value\"], kde=True, stat=\"density\", linewidth=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "moved-sacramento",
      "metadata": {
        "id": "moved-sacramento"
      },
      "source": [
        "> From the plot we are lead to believe that the median house value was clipped at $500 000$, and values above that threshold were reported as $500 001$. Since we do not have the original data to replace the clipped values, for simplicity we will simply discard blocks with a median house value larger than $500 000$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "minute-tiger",
      "metadata": {
        "id": "minute-tiger"
      },
      "outputs": [],
      "source": [
        "train = train[train[\"median_house_value\"] < 500001]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iraqi-strength",
      "metadata": {
        "id": "iraqi-strength"
      },
      "outputs": [],
      "source": [
        "sns.histplot(train[\"median_house_value\"], kde=True, stat=\"density\", linewidth=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "specified-cable",
      "metadata": {
        "id": "specified-cable"
      },
      "source": [
        "> We then perform a similar analysis for the housing median age field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mathematical-patient",
      "metadata": {
        "id": "mathematical-patient"
      },
      "outputs": [],
      "source": [
        "sns.histplot(train[\"housing_median_age\"], kde=True, stat=\"density\", linewidth=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "driven-reason",
      "metadata": {
        "id": "driven-reason"
      },
      "source": [
        "> Also in this case, it seems that a median age equal to $52$ has been introduced to denote values $> 51$. We discard such blocks as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acting-jordan",
      "metadata": {
        "id": "acting-jordan"
      },
      "outputs": [],
      "source": [
        "train = train[train[\"housing_median_age\"] < 52]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "protecting-michael",
      "metadata": {
        "id": "protecting-michael"
      },
      "outputs": [],
      "source": [
        "sns.histplot(train[\"housing_median_age\"], kde=True, stat=\"density\", linewidth=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "persistent-basics",
      "metadata": {
        "id": "persistent-basics"
      },
      "source": [
        "> After this cleaning, the dataset size has decreased from $17 000$ to $15 000$. Still, this size is large enough for our goals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "federal-nowhere",
      "metadata": {
        "id": "federal-nowhere"
      },
      "outputs": [],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "metallic-multiple",
      "metadata": {
        "id": "metallic-multiple"
      },
      "outputs": [],
      "source": [
        "train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "similar-democrat",
      "metadata": {
        "id": "similar-democrat"
      },
      "source": [
        "> We finally standardize the dataset. In previous tutorials we have used the following normalization formula\n",
        "> `(train - train.min()) / (train.max() - train.min())`, here we use instead a standardization `(train - train.mean()) / train.std()`. On one hand this choice shows you different options for the same standardization task; on the other, the reasons for this specific choice here will be clear in the following. Note that we are defining two variables `train_mean` and `train_std` because we will need them during the testing phase as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "standard-thesaurus",
      "metadata": {
        "id": "standard-thesaurus"
      },
      "outputs": [],
      "source": [
        "train_mean = train.mean()\n",
        "train_std = train.std()\n",
        "train = (train - train_mean) / train_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "north-bones",
      "metadata": {
        "id": "north-bones"
      },
      "outputs": [],
      "source": [
        "train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "continuing-conspiracy",
      "metadata": {
        "id": "continuing-conspiracy"
      },
      "source": [
        "> We finally separate features and observations into $(\\boldsymbol{X}_{\\text{train}}, \\boldsymbol{Y}_{\\text{train}})$. Note that, for the first time in this course, we have more than one observation for each data point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cellular-mechanics",
      "metadata": {
        "id": "cellular-mechanics"
      },
      "outputs": [],
      "source": [
        "Y_train = train[[\"population\", \"median_income\"]].to_numpy()\n",
        "Y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "featured-webmaster",
      "metadata": {
        "id": "featured-webmaster"
      },
      "outputs": [],
      "source": [
        "X_train = train.drop([\"population\", \"median_income\"], axis=1).to_numpy()\n",
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sweet-understanding",
      "metadata": {
        "id": "sweet-understanding"
      },
      "source": [
        "2. Define a function to evaluate a feedforward neural network with two hidden layers, with 16 neurons in the first hidden layer and 12 neurons in the second hidden layer. The $\\tanh$ activation function should be used for the hidden layers, while no activation function (i.e., the identity) should be used in the output layer.\n",
        "\n",
        "*Solution*:\n",
        "> Note that the text does not say how many neurons there are in the input and output layer. Indeed, it is understood that we should use as many neurons in the input layer as features (i.e., 7 neurons) and as many neurons in the output layer as observations (i.e., 2 neurons).\n",
        ">\n",
        "> The following (optimization variables, for us) define the neural network:\n",
        "> * a matrix $\\boldsymbol{W}_1 \\in \\mathbb{R}^{16 \\times 7}$, which collects the weights connecting nodes in the input layer to the first hidden layer;\n",
        "> * a vector $\\boldsymbol{b}_1 \\in \\mathbb{R}^{16}$, which collects the biases associated to each node in the first hidden layer;\n",
        "> * a matrix $\\boldsymbol{W}_2 \\in \\mathbb{R}^{12 \\times 16}$, which collects the weights connecting nodes in the first hidden layer to the second hidden layer;\n",
        "> * a vector $\\boldsymbol{b}_2 \\in \\mathbb{R}^{12}$, which collects the biases associated to each node in the second hidden layer;\n",
        "> * a matrix $\\boldsymbol{W}_3 \\in \\mathbb{R}^{2 \\times 12}$, which collects the weights connecting nodes in the second hidden layer to the output layer;\n",
        "> * a vector $\\boldsymbol{b}_3 \\in \\mathbb{R}^{2}$, which collects the biases associated to each node in the output layer.\n",
        ">\n",
        "> To closely resemble previous implementations (e.g., the prediction function $\\hat{y}$ in a linear or logistic regression exercises), we will collect all such arguments in a list `w`.\n",
        "> Note that, since the prediction function will be used in the the definition of the empirical risk (which is our cost function, of which we need to compute the gradient), here we use `jnp` to enable automatic differentiation in `jax`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "round-train",
      "metadata": {
        "id": "round-train"
      },
      "outputs": [],
      "source": [
        "def feedforward_neural_network_regression(x: np.ndarray, w: typing.List[np.ndarray]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Evaluate the feedforward neural.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : 1d or 2d numpy array\n",
        "        a single feature vector (1d array) or multiple feature vectors (2d array) for which we desire a prediction\n",
        "        by evaluation of the neural network.\n",
        "    w : list of 2d numpy arrays\n",
        "        weights and biases of the neural network.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    1d or 2d numpy array\n",
        "        prediction associated to the feature vector (1d array) or multiple feature vectors (2d array) which\n",
        "        were provided as inputs.\n",
        "    \"\"\"\n",
        "    (W_1, b_1, W_2, b_2, W_3, b_3) = w\n",
        "\n",
        "    # Handle x of different shapes (come back to this after point 3)\n",
        "    if len(x.shape) == 2:\n",
        "        x = x.T\n",
        "    else:\n",
        "        x = x.reshape(-1, 1)\n",
        "\n",
        "    # Layer 0 is composed by the input features x\n",
        "    layer_0 = x\n",
        "\n",
        "    # Use layer 0, the weights W_1 and the biases b_1 to activate layer 1\n",
        "    layer_1 = jnp.tanh(jnp.dot(W_1, layer_0) + b_1)\n",
        "\n",
        "    # Use layer 1, the weights W_2 and the biases b_2 to activate layer 2\n",
        "    layer_2 = jnp.tanh(jnp.dot(W_2, layer_1) + b_2)\n",
        "\n",
        "    # Use layer 2, the weights W_3 and the biases b_3 to compute (without activation) the output layer\n",
        "    layer_3 = jnp.dot(W_3, layer_2) + b_3\n",
        "\n",
        "    # Apply the transformation back before returning\n",
        "    if len(x.shape) == 2:\n",
        "        return layer_3.T\n",
        "    else:\n",
        "        return layer_3.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hispanic-great",
      "metadata": {
        "id": "hispanic-great"
      },
      "source": [
        "3. Initialize the weights and biases using the following procedure, known as Glorot initialization or Xavier initialization (from the name of the researcher who proposed it, Xavier Glorot):\n",
        "   * inizialize biases to zero,\n",
        "   * inizialize weights at layer $l$ sampling from a Gaussian distribution with zero mean and with standard deviation \n",
        "$$\n",
        "\\sqrt{\\frac{2}{d_{l - 1} + d_l}}\n",
        "$$\n",
        "where the vector $\\boldsymbol{d}$ contains the number of neurons per layer (e.g., in our example $\\boldsymbol{d} = [7, 16, 12, 2])$.\n",
        "\n",
        "*Solution*:\n",
        "> We use [`numpy.random.normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html) to perform the initialization. For technical reasons we define the biases as matrices with one columns (rather than vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "treated-calibration",
      "metadata": {
        "id": "treated-calibration"
      },
      "outputs": [],
      "source": [
        "np.random.seed(52 + 300)\n",
        "\n",
        "d = [7, 16, 12, 2]\n",
        "\n",
        "W_1 = np.random.normal(0, np.sqrt(2 / (d[0] + d[1])), size=(d[1], d[0]))\n",
        "print(\"W_1 shape:\", W_1.shape)\n",
        "\n",
        "b_1 = np.zeros((d[1], 1))\n",
        "print(\"b_1 shape:\", b_1.shape)\n",
        "\n",
        "W_2 = np.random.normal(0, np.sqrt(2 / (d[1] + d[2])), size=(d[2], d[1]))\n",
        "print(\"W_2 shape:\", W_2.shape)\n",
        "\n",
        "b_2 = np.zeros((d[2], 1))\n",
        "print(\"b_2 shape:\", b_2.shape)\n",
        "\n",
        "W_3 = np.random.normal(0, np.sqrt(2 / (d[2] + d[3])), size=(d[3], d[2]))\n",
        "print(\"W_3 shape:\", W_3.shape)\n",
        "\n",
        "b_3 = np.zeros((d[3], 1))\n",
        "print(\"b_3 shape:\", b_3.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "applied-talent",
      "metadata": {
        "id": "applied-talent"
      },
      "source": [
        "> We collect them in a variable `w_0`, which we will use as initial point for the optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cutting-fusion",
      "metadata": {
        "id": "cutting-fusion"
      },
      "outputs": [],
      "source": [
        "w_0 = [W_1, b_1, W_2, b_2, W_3, b_3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "enclosed-inquiry",
      "metadata": {
        "id": "enclosed-inquiry"
      },
      "source": [
        "> With this initialization, we may test the evaluation of the neural network for a specific value of $\\boldsymbol{x}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "norman-speaker",
      "metadata": {
        "id": "norman-speaker"
      },
      "outputs": [],
      "source": [
        "feedforward_neural_network_regression(np.ones(7), w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "convenient-tragedy",
      "metadata": {
        "id": "convenient-tragedy"
      },
      "source": [
        "> `numpy` and `jax` are able to optimize repeated function evaluations when passed multiple points (this is called vectorization). We need multiple function evaluations for instance to compute the empirical risk. This is the reason why there is a `if` case at the beginning of the implementation of `feedforward_neural_network`. We may try to evaluate the network for a matrix composed by a single row (e.g., one row of the training set) or for a matrix composed by two rows (e.g. three rows of the training dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "traditional-treaty",
      "metadata": {
        "id": "traditional-treaty"
      },
      "outputs": [],
      "source": [
        "feedforward_neural_network_regression(np.ones((1, 7)), w_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "solved-surprise",
      "metadata": {
        "id": "solved-surprise"
      },
      "outputs": [],
      "source": [
        "feedforward_neural_network_regression(\n",
        "    np.vstack((np.ones((1, 7)), 2 * np.ones((1, 7)), 3 * np.ones((1, 7)))), w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "catholic-runner",
      "metadata": {
        "id": "catholic-runner"
      },
      "source": [
        "4. Implement the evaluation of the empirical risk associated to a least squares regression, and its gradient.\n",
        "\n",
        "*Solution*:\n",
        "> The least squares loss is defined here as\n",
        "> $$ \\ell(\\boldsymbol{x}, \\boldsymbol{y}; \\boldsymbol{w}) = \\left\\|\\hat{\\boldsymbol{y}}(\\boldsymbol{x}; \\boldsymbol{w}) - \\boldsymbol{y}\\right\\|^2,$$\n",
        "> where $\\hat{\\boldsymbol{y}}$ is a symbol that encodes the evaluation of the neural network. Note that we are using the squared norm (rather than the square of a number) because the observation is now a vector with two entries!\n",
        ">\n",
        "> We are used to define the empirical risk by summing the least squares loss over the training dataset. However, this is typically very slow when the dataset is large. For performance reasons, we do this by using the `jnp.mean` function and the vectorized evaluation of the prediction.\n",
        "> A call to the slow (but more readable! Note the use of `np.linalg.norm`, because the observation is now a vector with two entries) version of the empirical risk is left commented below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "overall-coupon",
      "metadata": {
        "id": "overall-coupon"
      },
      "outputs": [],
      "source": [
        "def empirical_risk_regression_slow(w: typing.List[np.ndarray]) -> float:\n",
        "    \"\"\"\n",
        "    Evaluate the empirical risk on the training dataset.\n",
        "\n",
        "    This is implementation follows what we did in previous tutorials, but may be slow on large datasets.\n",
        "    \"\"\"\n",
        "    m = X_train.shape[0]\n",
        "    return 1 / m * sum(\n",
        "        np.linalg.norm(feedforward_neural_network_regression(X_train[j], w) - Y_train[j])**2\n",
        "        for j in range(m)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sunset-ceramic",
      "metadata": {
        "id": "sunset-ceramic"
      },
      "outputs": [],
      "source": [
        "# empirical_risk_regression_slow(w_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "actual-valuable",
      "metadata": {
        "id": "actual-valuable"
      },
      "outputs": [],
      "source": [
        "def empirical_risk_regression(w: typing.List[np.ndarray]) -> float:\n",
        "    \"\"\"\n",
        "    Evaluate the empirical risk on the training dataset.\n",
        "\n",
        "    This is implementation relies on jax for faster computations.\n",
        "    \"\"\"\n",
        "    return jnp.mean(\n",
        "        jnp.linalg.norm(feedforward_neural_network_regression(X_train, w) - Y_train, axis=1)**2\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "familiar-egypt",
      "metadata": {
        "id": "familiar-egypt"
      },
      "outputs": [],
      "source": [
        "empirical_risk_regression(w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rural-glance",
      "metadata": {
        "id": "rural-glance"
      },
      "source": [
        "> When using the gradient method, we are interested in computing the derivative w.r.t. the optimization variables of the empirical risk associated to the whole dataset. Therefore, we apply `jax.grad` to `empirical_risk_regression`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "global-guitar",
      "metadata": {
        "id": "global-guitar"
      },
      "outputs": [],
      "source": [
        "grad_empirical_risk_regression = jax.grad(empirical_risk_regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "defined-census",
      "metadata": {
        "id": "defined-census"
      },
      "source": [
        "> Note that, since `w` contains $\\boldsymbol{W}_1$, $\\boldsymbol{b}_1$, etc, `jax` has returned the gradient w.r.t. $\\boldsymbol{W}_1$, $\\boldsymbol{b}_1$, ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wired-health",
      "metadata": {
        "id": "wired-health"
      },
      "outputs": [],
      "source": [
        "grad_empirical_risk_regression(w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "italian-regular",
      "metadata": {
        "id": "italian-regular"
      },
      "source": [
        "> When using the mini-batch stochastic gradient method, we are interested in computing the derivative w.r.t. the optimization variables of the empirical risk associated to a subset of the dataset. Therefore, we need to replicate the definition of the `empirical_risk_regression` function, but with an additional argument `X` and `Y` that encodes the rows selected by the mini-batch index selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exotic-correction",
      "metadata": {
        "id": "exotic-correction"
      },
      "outputs": [],
      "source": [
        "def mini_batch_empirical_risk_regression(X: np.ndarray, Y: np.ndarray, w: typing.List[np.ndarray]) -> float:\n",
        "    \"\"\"Evaluate the empirical risk on a mini-batch of the training dataset.\"\"\"\n",
        "    return jnp.mean(\n",
        "        jnp.linalg.norm(feedforward_neural_network_regression(X, w) - Y, axis=1)**2\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "patent-hebrew",
      "metadata": {
        "id": "patent-hebrew"
      },
      "source": [
        "> For instance, using a mini-batch made of a four rows we obtain the following result..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unusual-honduras",
      "metadata": {
        "id": "unusual-honduras"
      },
      "outputs": [],
      "source": [
        "mini_batch_empirical_risk_regression(X_train[:4], Y_train[:4], w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "recovered-coach",
      "metadata": {
        "id": "recovered-coach"
      },
      "source": [
        "> ... or a mini batch made up of all rows, we obtain the same value as calling `empirical_risk_regression`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pleased-electricity",
      "metadata": {
        "id": "pleased-electricity"
      },
      "outputs": [],
      "source": [
        "mini_batch_empirical_risk_regression(X_train, Y_train, w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "standing-charm",
      "metadata": {
        "id": "standing-charm"
      },
      "source": [
        "> However, when calling `jax.grad` to compute the gradient of the `mini_batch_empirical_risk_regression`, we should inform `jax` that we are only interested in the gradient w.r.t. the argument `w` (and not `X` or `Y`). This can be done as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "played-albany",
      "metadata": {
        "id": "played-albany"
      },
      "outputs": [],
      "source": [
        "grad_mini_batch_empirical_risk_regression = jax.grad(mini_batch_empirical_risk_regression, argnums=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "african-deposit",
      "metadata": {
        "id": "african-deposit"
      },
      "outputs": [],
      "source": [
        "grad_mini_batch_empirical_risk_regression(X_train, Y_train, w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "weighted-letters",
      "metadata": {
        "id": "weighted-letters"
      },
      "source": [
        "5. Implement the mini-batch stochastic gradient method with constant momentum and constant step length in a Python function. Such function should:\n",
        "   * take as input the features and observations in the training dataset, the percentage of training features to use in a mini-batch, the function $f$ to be used for the evaluation of the empirical risk on a mini-batch, its gradient $\\nabla f$, the value $\\alpha$ of the step length, the value $\\beta$ of the momentum coefficient, the maximum number $E_{\\max}$ of allowed epochs, and the initial condition $\\boldsymbol{w}_{0}$;\n",
        "   * return as outputs the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$ and the history of the function values $\\{f(\\boldsymbol{w}_k)\\}_k$.\n",
        "\n",
        "   Do not use any stopping criteria: termination should occur soon as the maximum number of epochs is reached.\n",
        " \n",
        "*Solution*:\n",
        "> We start from the `mini_batch_stochastic_gradient` implementation we have used in the previous tutorial, and change it as follows:\n",
        "> * since the variable `w` is now a list containing the optimization variables $\\boldsymbol{W}_1$, $\\boldsymbol{b}_1$, ... as components, we need to carry out the update step from $k$ to $k + 1$ for each component separately (`for c in range(len(w_k))`),\n",
        "> * the signature of `f` and `grad_f` is different from the previous tutorials: in previous tutorials we used to pass a single index $j \\in J_k$ to the function evaluation (and then sum all function evaluations), instead here the restriction to $J_k$ of the training dataset should be provided to the function evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "swedish-lindsay",
      "metadata": {
        "id": "swedish-lindsay"
      },
      "outputs": [],
      "source": [
        "def mini_batch_stochastic_gradient_momentum(\n",
        "    X: np.ndarray, Y: np.ndarray, perc: float, f: typing.Callable, grad_f: typing.Callable,\n",
        "    alpha: float, beta: float, maxep: float, w_0: typing.List[np.ndarray]\n",
        ") -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run the mini-batch stochastic gradient method with constant step length and constant momentum coefficient.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X, Y : np.ndarray\n",
        "        features and observations of the training dataset.\n",
        "    perc : float\n",
        "        percentage of training features to use a mini-batch.\n",
        "    f, grad_f : Python function\n",
        "        callable evaluating the cost function and its gradient, respectively.\n",
        "    alpha : float\n",
        "        constant step length.\n",
        "    beta : float\n",
        "        constant momentum coefficient.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the norm of the gradient of the cost.\n",
        "    maxep : int\n",
        "        maximum number of allowed epochs.\n",
        "    w_0 : list of 2d numpy arrays\n",
        "        initial condition for weights and biases of the neural network.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of 2d numpy arrays\n",
        "        history of the weights and biases of the neural network over the optimization iterations.\n",
        "    1d numpy array\n",
        "        history of the empirical risk function values.\n",
        "    \"\"\"\n",
        "    # Determine m and m_b from input arguments\n",
        "    assert X.shape[0] == Y.shape[0]\n",
        "    m = Y.shape[0]\n",
        "    m_b = int(perc * m)\n",
        "\n",
        "    # Use JAX just-in-time compilation to improve performance\n",
        "    f = jax.jit(f)\n",
        "    grad_f = jax.jit(grad_f)\n",
        "\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    assert isinstance(w_0, list)\n",
        "    all_w = [w_0]\n",
        "    all_f = [f(X, Y, w_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Use the epoch number as stopping criterion\n",
        "    while k < maxep * m / m_b:\n",
        "        w_k = all_w[k]\n",
        "        w_k_minus_1 = all_w[k - 1]\n",
        "\n",
        "        # Draw random indices\n",
        "        J_k = np.random.choice(m, size=m_b, replace=False)\n",
        "\n",
        "        # Compute the update direction\n",
        "        g_k = grad_f(X[J_k], Y[J_k], w_k)\n",
        "\n",
        "        # Compute w_{k + 1}\n",
        "        w_k_plus_1 = [\n",
        "            w_k[c] - alpha * g_k[c] + beta * (w_k[c] - w_k_minus_1[c])\n",
        "            for c in range(len(w_k))\n",
        "        ]\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f(X, Y, w_k))\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "    # Return the history of the optimization variables and costs\n",
        "    return all_w, np.array(all_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "discrete-denial",
      "metadata": {
        "id": "discrete-denial"
      },
      "source": [
        "6. Train the neural network using mini-batches made of $10\\%$ of the training dataset, step length $\\alpha = 0.05$ and momentum coefficient $\\beta = 0.9$. Limit your training to 150 epochs.\n",
        "\n",
        "*Solution*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "accredited-wallpaper",
      "metadata": {
        "id": "accredited-wallpaper"
      },
      "outputs": [],
      "source": [
        "np.random.seed(52 + 600)\n",
        "all_w_regression, all_f_regression = mini_batch_stochastic_gradient_momentum(\n",
        "    X_train, Y_train, 0.1, mini_batch_empirical_risk_regression, grad_mini_batch_empirical_risk_regression,\n",
        "    0.05, 0.9, 150, w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "national-experiment",
      "metadata": {
        "id": "national-experiment"
      },
      "source": [
        "> We compare the square root of the training loss at the first epoch and at the last epoch. Note that, since we are using a least squares loss function, we can easily print the RMSE on the training dataset by taking the square root of the empirical risk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "colonial-affiliate",
      "metadata": {
        "id": "colonial-affiliate"
      },
      "outputs": [],
      "source": [
        "np.sqrt(all_f_regression[0]), np.sqrt(all_f_regression[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "prompt-burning",
      "metadata": {
        "id": "prompt-burning"
      },
      "source": [
        "> We plot the history of the RMSE on the training set. We notice that there are some oscillations in the plot. Therefore, rather than using the optimization variable `all_w[-1]` at the last iteration, we will use `all_w[k_best]`, where `k_best` is the iteration index at which the RMSE is lowest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cardiac-anxiety",
      "metadata": {
        "id": "cardiac-anxiety"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "fig.add_scatter(x=np.arange(all_f_regression.shape[0]), y=np.sqrt(all_f_regression))\n",
        "fig.update_layout(title=\"History of RMSE on training set\")\n",
        "fig.update_xaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wrong-michael",
      "metadata": {
        "id": "wrong-michael"
      },
      "outputs": [],
      "source": [
        "K_regression = all_f_regression.shape[0] - 1\n",
        "k_best_regression = np.argmin(np.sqrt(all_f_regression))\n",
        "print(k_best_regression, \"vs\", K_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nuclear-machine",
      "metadata": {
        "id": "nuclear-machine"
      },
      "outputs": [],
      "source": [
        "np.sqrt(all_f_regression[0]), np.sqrt(all_f_regression[-1]), np.sqrt(all_f_regression[k_best_regression])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "heated-cache",
      "metadata": {
        "id": "heated-cache"
      },
      "source": [
        "7. After loading and cleaning the test dataset, assess the accuracy of the prediction on the test dataset.\n",
        "\n",
        "*Solution*:\n",
        "> We load the test dataset with `pandas`, and carry out the same filtering based on the `median_house_value` and `housing_median_age` features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "amino-occurrence",
      "metadata": {
        "id": "amino-occurrence"
      },
      "outputs": [],
      "source": [
        "if os.path.isfile(\"data/california_housing_test.csv\"):\n",
        "    csv_path = \"data/california_housing_test.csv\"\n",
        "elif os.path.isfile(\"sample_data/california_housing_test.csv\"):\n",
        "    csv_path = \"sample_data/california_housing_test.csv\"\n",
        "else:\n",
        "    csv_path = (\n",
        "        \"https://dmf.unicatt.it/~fball/public/optimization_for_machine_learning\"\n",
        "        + \"/california_housing_test.csv\"\n",
        "    )\n",
        "test = pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "informative-college",
      "metadata": {
        "id": "informative-college"
      },
      "outputs": [],
      "source": [
        "test = test[test[\"median_house_value\"] < 500001]\n",
        "test = test[test[\"housing_median_age\"] < 52]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparable-provision",
      "metadata": {
        "id": "comparable-provision"
      },
      "source": [
        "> We then proceed to the standardization. Watch out that, in order to be consistent with the training phase, the mean and standard deviation of the *training* dataset should be used to carry out this standardization, even if we are operating on the test dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "competent-joining",
      "metadata": {
        "id": "competent-joining"
      },
      "outputs": [],
      "source": [
        "test = (test - train_mean) / train_std"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "separate-showcase",
      "metadata": {
        "id": "separate-showcase"
      },
      "source": [
        "> We finally separate features and observation into $(\\boldsymbol{X}_{\\text{test}}, \\boldsymbol{Y}_{\\text{test}})$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "processed-capability",
      "metadata": {
        "id": "processed-capability"
      },
      "outputs": [],
      "source": [
        "Y_test = test[[\"population\", \"median_income\"]].to_numpy()\n",
        "X_test = test.drop([\"population\", \"median_income\"], axis=1).to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "valid-complaint",
      "metadata": {
        "id": "valid-complaint"
      },
      "source": [
        "> We then evaluate the RMSE on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eastern-process",
      "metadata": {
        "id": "eastern-process"
      },
      "outputs": [],
      "source": [
        "RMSE_test = np.sqrt(np.mean(np.linalg.norm(\n",
        "    feedforward_neural_network_regression(X_test, all_w_regression[k_best_regression]) - Y_test, axis=1\n",
        ")**2))\n",
        "RMSE_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "organic-generator",
      "metadata": {
        "id": "organic-generator"
      },
      "source": [
        "> To present the results to the company, we consider the two outputs separately, and convert the results back into their original units multiplying by their standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "crucial-simon",
      "metadata": {
        "id": "crucial-simon"
      },
      "outputs": [],
      "source": [
        "RMSE_test_normalized_population = np.sqrt(np.mean(\n",
        "    (feedforward_neural_network_regression(X_test, all_w_regression[k_best_regression])[0] - Y_test[0])**2\n",
        "))\n",
        "RMSE_test_normalized_income = np.sqrt(np.mean(\n",
        "    (feedforward_neural_network_regression(X_test, all_w_regression[k_best_regression])[1] - Y_test[1])**2\n",
        "))\n",
        "RMSE_test_normalized_population, RMSE_test_normalized_income"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thrown-birmingham",
      "metadata": {
        "id": "thrown-birmingham"
      },
      "outputs": [],
      "source": [
        "std_population = train_std[[\"population\"]].to_numpy()[0]\n",
        "std_income = train_std[[\"median_income\"]].to_numpy()[0]\n",
        "std_population, std_income"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sitting-nature",
      "metadata": {
        "id": "sitting-nature"
      },
      "outputs": [],
      "source": [
        "RMSE_test_population = RMSE_test_normalized_population * std_population\n",
        "RMSE_test_income = RMSE_test_normalized_income * std_income\n",
        "RMSE_test_population, RMSE_test_income"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lucky-jersey",
      "metadata": {
        "id": "lucky-jersey"
      },
      "source": [
        "> What would we need to change in this notebook if, instead of running a regression by neural networks, we were interested in running a classification?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dimensional-departure",
      "metadata": {
        "id": "dimensional-departure"
      },
      "source": [
        "8. The RMSE values on the test set are reported back to the company, which sends us back the two following remarks:\n",
        "   * they are not satisfied by the accuracy of the estimation on the <code><font color=\"red\">population</font></code> field. Indeed, they define a block as <code><font color=\"red\">highly populated</font></code> if the number of its residents lies in the right distribution tail starting from half a standard deviation of the <code><font color=\"red\">population</font></code> field. Thus, having a RMSE of 500 people on a field that has a standard deviation of 1000 people is unsatisfactory for their goals;\n",
        "   * their ethics committee feels that taking decisions based on a estimated value for <code><font color=\"red\">median_income</font></code> is as unethical as directly asking people what their income is. What they are looking for instead is to detect <code><font color=\"red\">low income</font></code> blocks, defined as blocks which lie in the left distribution tail ending at half a standard deviation of the <code><font color=\"red\">median_income</font></code> field.\n",
        "\n",
        "   What should we do to address these remarks?\n",
        " \n",
        "*Solution*:\n",
        "> Thanks to the choice of the normalization (w.r.t. mean and standard deviation, rather than min and max) the requirement\n",
        "> * \"number of its residents lies in the right distribution tail starting from half a standard deviation of the <code><font color=\"red\">population</font></code> field\" corresponds to `Y_train[:, 0] > 0.5`, and\n",
        "> * \"lie in the left distribution tail ending at half a standard deviation of the <code><font color=\"red\">median_income</font></code>\" corresponds to `Y_train[:, 1] < - 0.5`.\n",
        ">\n",
        "> We then multiply the two arrays together to detect blocks in which both conditions hold. For technical reasons we also reshape the resulting vector as a matrix with one column, so that we can reuse some of the previous implementations (in which the two outputs were stored each in a separate column)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "juvenile-syndication",
      "metadata": {
        "id": "juvenile-syndication"
      },
      "outputs": [],
      "source": [
        "y_train = ((Y_train[:, 0] > 0.5) * (Y_train[:, 1] < - 0.5)).astype(np.float64).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "simplified-kidney",
      "metadata": {
        "id": "simplified-kidney"
      },
      "outputs": [],
      "source": [
        "np.sum(y_train) / y_train.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "medieval-passport",
      "metadata": {
        "id": "medieval-passport"
      },
      "source": [
        "> We notice that only a small percentage of the dataset satisfies both conditions, and thus is eligible for the urban requalification project."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "excessive-exhibit",
      "metadata": {
        "id": "excessive-exhibit"
      },
      "source": [
        "9. Define a function to evaluate a feedforward neural network with two hidden layers, with 16 neurons in the first hidden layer and 12 neurons in the second hidden layer. The sigmoid activation function should be used for the hidden layers, but, since we are interested in a classification task, also in the output layer.\n",
        "\n",
        "*Solution*:\n",
        "> There are two differences compared to the previous implementation:\n",
        "> * the activation function is a sigmoid (while previous we used a $\\tanh$ activation function). We implement the sigmoid function as we did in previous tutorials (using `jnp` rather than `np`!);\n",
        "> * the activation function should be applied to the output layer too, otherwise the output may note be between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "external-syracuse",
      "metadata": {
        "id": "external-syracuse"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z: float) -> float:\n",
        "    \"\"\"Evaluate the sigmoid function.\"\"\"\n",
        "    return 1 / (1 + jnp.exp(-z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "peripheral-permission",
      "metadata": {
        "id": "peripheral-permission"
      },
      "outputs": [],
      "source": [
        "def feedforward_neural_network_classification(x: np.ndarray, w: typing.List[np.ndarray]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Evaluate the feedforward neural.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : 1d or 2d numpy array\n",
        "        a single feature vector (1d array) or multiple feature vectors (2d array) for which we desire a prediction\n",
        "        by evaluation of the neural network.\n",
        "    w : list of 2d numpy arrays\n",
        "        weights and biases of the neural network.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    1d or 2d numpy array\n",
        "        prediction associated to the feature vector (1d array) or multiple feature vectors (2d array) which\n",
        "        were provided as inputs.\n",
        "    \"\"\"\n",
        "    (W_1, b_1, W_2, b_2, W_3, b_3) = w\n",
        "\n",
        "    # Handle x of different shapes (come back to this after point 3)\n",
        "    if len(x.shape) == 2:\n",
        "        x = x.T\n",
        "    else:\n",
        "        x = x.reshape(-1, 1)\n",
        "\n",
        "    # Layer 0 is composed by the input features x\n",
        "    layer_0 = x\n",
        "\n",
        "    # Use layer 0, the weights W_1 and the biases b_1 to activate layer 1\n",
        "    layer_1 = sigmoid(jnp.dot(W_1, layer_0) + b_1)\n",
        "\n",
        "    # Use layer 1, the weights W_2 and the biases b_2 to activate layer 2\n",
        "    layer_2 = sigmoid(jnp.dot(W_2, layer_1) + b_2)\n",
        "\n",
        "    # Use layer 2, the weights W_3 and the biases b_3 to compute (without activation) the output layer\n",
        "    layer_3 = sigmoid(jnp.dot(W_3, layer_2) + b_3)\n",
        "\n",
        "    # Apply the transformation back before returning\n",
        "    if len(x.shape) == 2:\n",
        "        return layer_3.T\n",
        "    else:\n",
        "        return layer_3.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "economic-shell",
      "metadata": {
        "id": "economic-shell"
      },
      "source": [
        "10. Initialize the weights and biases for the classification neural network using the Glorot initialization.\n",
        "\n",
        "*Soution*:\n",
        "> The code is very similar to the one used for the regression task. The main difference is that here the output layer has only one neuron (instead of two)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "endangered-wilderness",
      "metadata": {
        "id": "endangered-wilderness"
      },
      "outputs": [],
      "source": [
        "np.random.seed(52 + 1000)\n",
        "\n",
        "d = [7, 16, 12, 1]\n",
        "\n",
        "W_1 = np.random.normal(0, np.sqrt(2 / (d[0] + d[1])), size=(d[1], d[0]))\n",
        "print(\"W_1 shape:\", W_1.shape)\n",
        "\n",
        "b_1 = np.zeros((d[1], 1))\n",
        "print(\"b_1 shape:\", b_1.shape)\n",
        "\n",
        "W_2 = np.random.normal(0, np.sqrt(2 / (d[1] + d[2])), size=(d[2], d[1]))\n",
        "print(\"W_2 shape:\", W_2.shape)\n",
        "\n",
        "b_2 = np.zeros((d[2], 1))\n",
        "print(\"b_2 shape:\", b_2.shape)\n",
        "\n",
        "W_3 = np.random.normal(0, np.sqrt(2 / (d[2] + d[3])), size=(d[3], d[2]))\n",
        "print(\"W_3 shape:\", W_3.shape)\n",
        "\n",
        "b_3 = np.zeros((d[3], 1))\n",
        "print(\"b_3 shape:\", b_3.shape)\n",
        "\n",
        "w_0 = [W_1, b_1, W_2, b_2, W_3, b_3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "average-gardening",
      "metadata": {
        "id": "average-gardening"
      },
      "source": [
        "11. Implement the evaluation of the empirical risk associated to the cross-entropy loss (i.e., the loss function of a logistic regression problem), and its gradient.\n",
        "\n",
        "*Solution*:\n",
        "> We implement already the mini-batch version of the empirical risk, since it is the one that we use in the training of the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cathedral-musical",
      "metadata": {
        "id": "cathedral-musical"
      },
      "outputs": [],
      "source": [
        "def mini_batch_empirical_risk_classification(X: np.ndarray, y: np.ndarray, w: typing.List[np.ndarray]) -> float:\n",
        "    \"\"\"Evaluate the empirical risk on a mini-batch of the training dataset.\"\"\"\n",
        "    y_hat = feedforward_neural_network_classification(X, w)\n",
        "    return jnp.mean(- y * jnp.log(y_hat) - (1 - y) * jnp.log(1 - y_hat))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hollywood-malta",
      "metadata": {
        "id": "hollywood-malta"
      },
      "source": [
        "> We test the implemented function on a mini-batch and on a batch application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bound-chair",
      "metadata": {
        "id": "bound-chair"
      },
      "outputs": [],
      "source": [
        "mini_batch_empirical_risk_classification(X_train[:4], y_train[:4], w_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "french-factory",
      "metadata": {
        "id": "french-factory"
      },
      "outputs": [],
      "source": [
        "mini_batch_empirical_risk_classification(X_train, y_train, w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "reflected-butter",
      "metadata": {
        "id": "reflected-butter"
      },
      "source": [
        "> We define the gradient of the empirical risk, to be employed by the stochastic gradient method, using `jax`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "brave-notice",
      "metadata": {
        "id": "brave-notice"
      },
      "outputs": [],
      "source": [
        "grad_mini_batch_empirical_risk_classification = jax.grad(mini_batch_empirical_risk_classification, argnums=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "filled-syntax",
      "metadata": {
        "id": "filled-syntax"
      },
      "outputs": [],
      "source": [
        "grad_mini_batch_empirical_risk_classification(X_train, y_train, w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "outside-wichita",
      "metadata": {
        "id": "outside-wichita"
      },
      "source": [
        "12. Train the (classification) neural network using mini-batches made of $10\\%$ of the training dataset, step length $\\alpha = 2$ and momentum coefficient $\\beta = 0.9$. Limit your training to 150 epochs.\n",
        "\n",
        "*Solution*:\n",
        "> We may reuse our previous implementation of `mini_batch_stochastic_gradient_momentum` by passing `y_train` as the second argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hungarian-texture",
      "metadata": {
        "id": "hungarian-texture"
      },
      "outputs": [],
      "source": [
        "np.random.seed(52 + 1200)\n",
        "all_w_classification, all_f_classification = mini_batch_stochastic_gradient_momentum(\n",
        "    X_train, y_train, 0.1, mini_batch_empirical_risk_classification, grad_mini_batch_empirical_risk_classification,\n",
        "    2.0, 0.9, 150, w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "painful-emperor",
      "metadata": {
        "id": "painful-emperor"
      },
      "source": [
        "> We compare the cross-entropy loss at the beginning and at the end, as well as plot its history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tired-affair",
      "metadata": {
        "id": "tired-affair"
      },
      "outputs": [],
      "source": [
        "all_f_classification[0], all_f_classification[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "capable-validity",
      "metadata": {
        "id": "capable-validity"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "fig.add_scatter(x=np.arange(all_f_classification.shape[0]), y=all_f_classification)\n",
        "fig.update_layout(title=\"History of cross-entropy loss on training set\")\n",
        "fig.update_xaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "flexible-mailing",
      "metadata": {
        "id": "flexible-mailing"
      },
      "source": [
        "> We notice some oscillations at the end of the training. Therefore, also here we determine the iteration index `k_best` that minimizes the cross-entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "removed-zambia",
      "metadata": {
        "id": "removed-zambia"
      },
      "outputs": [],
      "source": [
        "K_classification = all_f_classification.shape[0] - 1\n",
        "k_best_classification = np.argmin(all_f_classification)\n",
        "print(k_best_classification, \"vs\", K_classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acquired-explanation",
      "metadata": {
        "id": "acquired-explanation"
      },
      "source": [
        "13. Present the prediction results on the test dataset by means of the following *table of confusion*\n",
        "<table>\n",
        " <tr>\n",
        "  <th></th>\n",
        "  <th>$\\hat{y} \\leq \\text{tr}$ (blocks not suggested for urban requalification)</th>\n",
        "  <th>$\\hat{y} > \\text{tr}$ (blocks suggested for urban requalification)</th>\n",
        " </tr>\n",
        " <tr>\n",
        "  <td>$y = 0$ (block is not eligible for urban requalification)</td>\n",
        "  <td><i># of true negative</i></td>\n",
        "  <td># of false positive</td>\n",
        " </tr>\n",
        " <tr>\n",
        "  <td>$y = 1$ (block is eligible for urban requalification)</td>\n",
        "  <td># of false negative</td>\n",
        "  <td><i># of true positive</i></td>\n",
        " </tr>\n",
        "</table>\n",
        "and choose the threshold value $\\text{tr}$ in the logistic regression accounting for the following policies of the company:\n",
        "\n",
        "   * the company would like to have as many true positive cases as possible, i.e. blocks which are eligible for urban requalification and are suggested as such by the model\n",
        "   * when comparing the results of the classification to the former ones obtained with the regression, the compay is willing to consider the new model based on classification provided that it has a lower number of false negative cases, as long as the number of false positive cases predicted by the classification model are not larger than twice the number of false positive cases predicted by the regression model. This means that the company is worried about wrongly excluding blocks that are indeed eligible (false negative), and to avoid this they are willing to mistakenly evaluate a moderate number of blocks that in the end will turn out to be not eligible (false positive).\n",
        " \n",
        "*Solution*:\n",
        "> We first determine the entries that fit the requalification criteria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "swiss-johnston",
      "metadata": {
        "id": "swiss-johnston"
      },
      "outputs": [],
      "source": [
        "y_test = ((Y_test[:, 0] > 0.5) * (Y_test[:, 1] < - 0.5)).astype(np.float64).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "outside-drilling",
      "metadata": {
        "id": "outside-drilling"
      },
      "outputs": [],
      "source": [
        "np.sum(y_test) / y_test.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "soviet-facility",
      "metadata": {
        "id": "soviet-facility"
      },
      "source": [
        "> As requested, we use the regression model as base model. We compute the corresponding confusion matrix. This requires evaluating the regression neural network, and checking whether its first output is above 0.5 and its second output is below -0.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "elect-calgary",
      "metadata": {
        "id": "elect-calgary"
      },
      "outputs": [],
      "source": [
        "confusion_regression = np.zeros((2, 2))\n",
        "for (x_j, y_j) in zip(X_test, y_test):\n",
        "    Y_hat_j = feedforward_neural_network_regression(x_j, all_w_regression[k_best_regression])\n",
        "    y_hat_j = ((Y_hat_j[0, 0] > 0.5) * (Y_hat_j[0, 1] < - 0.5)).astype(np.float32)\n",
        "    confusion_regression[int(y_j > 0.5), int(y_hat_j > 0.5)] += 1\n",
        "confusion_regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "median-wireless",
      "metadata": {
        "id": "median-wireless"
      },
      "source": [
        "> We then compare classification models obtained using different thresholds $0.1$, $0.3$ or the default $0.5$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rural-roller",
      "metadata": {
        "id": "rural-roller"
      },
      "outputs": [],
      "source": [
        "confusion_classification_1 = np.zeros((2, 2))\n",
        "confusion_classification_3 = np.zeros((2, 2))\n",
        "confusion_classification_5 = np.zeros((2, 2))\n",
        "for (x_j, y_j) in zip(X_test, y_test):\n",
        "    y_hat_j = feedforward_neural_network_classification(x_j, all_w_classification[k_best_classification])\n",
        "    confusion_classification_1[int(y_j > 0.1), int(y_hat_j > 0.1)] += 1\n",
        "    confusion_classification_3[int(y_j > 0.3), int(y_hat_j > 0.3)] += 1\n",
        "    confusion_classification_5[int(y_j > 0.5), int(y_hat_j > 0.5)] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "close-reset",
      "metadata": {
        "id": "close-reset"
      },
      "outputs": [],
      "source": [
        "confusion_classification_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tutorial-geology",
      "metadata": {
        "id": "tutorial-geology"
      },
      "outputs": [],
      "source": [
        "confusion_classification_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exact-pearl",
      "metadata": {
        "id": "exact-pearl"
      },
      "outputs": [],
      "source": [
        "confusion_classification_5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conscious-vacation",
      "metadata": {
        "id": "conscious-vacation"
      },
      "source": [
        "> * We see that the classification model with threshold 0.5 is characterized by a number of true positive cases that is lower than the regression model. This goes against the first policy of the company, and thus we discard the threshold 0.5\n",
        "> * The two remaining classification models (thresholds 0.1 or 0.3) are both characterized by a lower number of false negative than the regression model. However the threshold 0.1 leads to a very large number of false positive cases. Therfore, we suggest the company to use the classification model with a threshold equal to 0.3."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}