{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pserebrennikov/3rd-year-project/blob/master/2_second_order_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "israeli-warrior",
      "metadata": {
        "id": "israeli-warrior"
      },
      "source": [
        "# Tutorial 2 - Second order methods\n",
        "### Course on Optimization for Machine Learning - Dr. F. Ballarin\n",
        "### Master Degree in Data Analytics for Business, Catholic University of the Sacred Heart, Milano"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "valued-irish",
      "metadata": {
        "id": "valued-irish"
      },
      "source": [
        "In this notebook we implement some second order methods for unconstrained optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e705eb3",
      "metadata": {
        "id": "7e705eb3"
      },
      "outputs": [],
      "source": [
        "import typing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gentle-butter",
      "metadata": {
        "id": "gentle-butter"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import plotly.colors\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "regulation-rwanda",
      "metadata": {
        "id": "regulation-rwanda"
      },
      "source": [
        "## Exercise 2.1 (continuation of Exercises 1.1 and 1.5)\n",
        "Let $\\boldsymbol{w} \\in \\mathbb{R}^2$. Consider the *Booth function*\n",
        "$$f(\\boldsymbol{w}) = (w^{(0)} + 2 w^{(1)} - 7)^2 + (2 w^{(0)} + w^{(1)} - 5)^2.$$\n",
        "\n",
        "10. Introduce a new variable $\\widetilde{\\boldsymbol{w}}$\n",
        "$$ \\widetilde{\\boldsymbol{w}} = \\boldsymbol{A} \\boldsymbol{w} $$\n",
        "where\n",
        "$$ \\boldsymbol{A} = \\sqrt{2} \\begin{bmatrix}2 & 1\\\\1 & 2\\end{bmatrix}.$$\n",
        "Denote by $\\widetilde{f}$ the Booth function with respect to the new variable\n",
        "$$\\widetilde{f}(\\widetilde{\\boldsymbol{w}}) = f(\\boldsymbol{A}^{-1} \\widetilde{\\boldsymbol{w}})$$\n",
        "Draw a contour plot of the function $\\widetilde{f}$ on the square domain $[-3, 17] \\times [0, 20]$ (in $\\widetilde{\\boldsymbol{w}}$).\n",
        "\n",
        "*Solution*:\n",
        "> We define a Python function for the evaluation $\\widetilde{f}$. Since the input variable is now $\\widetilde{f}$, we first transform it to $\\boldsymbol{w}$, and use the previous implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "reliable-rover",
      "metadata": {
        "id": "reliable-rover"
      },
      "outputs": [],
      "source": [
        "A = np.sqrt(2) * np.array([[2, 1], [1, 2]])\n",
        "A_inv = np.linalg.inv(A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "overall-relation",
      "metadata": {
        "id": "overall-relation"
      },
      "outputs": [],
      "source": [
        "def f_tilde_ex_2_1(w_tilde: np.ndarray) -> float:\n",
        "    r\"\"\"Evaluate \\tilde{f}(w).\"\"\"\n",
        "    w = np.dot(A_inv, w_tilde)  # Watch out! np.dot vs * with matrix/vector operations\n",
        "    return (w[0] + 2 * w[1] - 7)**2 + (2 * w[0] + w[1] - 5)**2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "peripheral-desperate",
      "metadata": {
        "id": "peripheral-desperate"
      },
      "source": [
        "> Afterwards we prepare the contour plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "virtual-error",
      "metadata": {
        "id": "virtual-error"
      },
      "outputs": [],
      "source": [
        "domain_tilde_component_0 = [-3, 17]\n",
        "domain_tilde_component_1 = [0, 20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "modern-grain",
      "metadata": {
        "id": "modern-grain"
      },
      "outputs": [],
      "source": [
        "w_tilde_component_0 = np.linspace(domain_tilde_component_0[0], domain_tilde_component_0[1], 100)\n",
        "w_tilde_component_1 = np.linspace(domain_tilde_component_1[0], domain_tilde_component_1[1], 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "friendly-austria",
      "metadata": {
        "id": "friendly-austria"
      },
      "outputs": [],
      "source": [
        "f_tilde_w_tilde = np.zeros((len(w_tilde_component_0), len(w_tilde_component_1)))\n",
        "for i in range(f_tilde_w_tilde.shape[0]):\n",
        "    for j in range(f_tilde_w_tilde.shape[1]):\n",
        "        f_tilde_w_tilde[j, i] = f_tilde_ex_2_1([w_tilde_component_0[i], w_tilde_component_1[j]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "defined-puppy",
      "metadata": {
        "id": "defined-puppy"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Contour(x=w_tilde_component_0, y=w_tilde_component_1, z=f_tilde_w_tilde)])\n",
        "fig.update_layout(\n",
        "    title=\"Function exercise 2.1 - contour plot in the w_tilde variables\",\n",
        "    width=512, height=512, autosize=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "varied-bundle",
      "metadata": {
        "id": "varied-bundle"
      },
      "source": [
        "> Notice how the contour lines of $\\widetilde{f}$ are circles, while the ones of $f$ were very elongated ellipses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "operating-summer",
      "metadata": {
        "id": "operating-summer"
      },
      "source": [
        "11. Recall that the gradient of $\\nabla f = \\nabla_{\\boldsymbol{w}}f $  is\n",
        "\\begin{equation*}\n",
        "\\nabla f(\\boldsymbol{w}) = \\begin{bmatrix}\n",
        "10 w^{(0)} + 8 w^{(1)} - 34\\\\\n",
        "8 w^{(0)} + 10 w^{(1)} - 38\\\\\n",
        "\\end{bmatrix},\n",
        "\\end{equation*}\n",
        "the hessian $\\nabla^2 f = \\nabla_\\boldsymbol{w}^2 f$ is\n",
        "\\begin{equation*}\n",
        "\\nabla^2 f(\\boldsymbol{w}) =\n",
        "\\begin{bmatrix}\n",
        "10 & 8\\\\\n",
        "8 & 10\\\\\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "and the global minima of $f$ is $\\boldsymbol{w}^* = (1,3)$. Determine the corresponding expressions for $\\nabla \\widetilde{f} = \\nabla_{\\widetilde{\\boldsymbol{w}}} \\widetilde{f}$, $\\nabla^2 \\widetilde{f} = \\nabla_{\\widetilde{\\boldsymbol{w}}}^2 \\widetilde{f}$ and the global minima $\\widetilde{\\boldsymbol{w}}^*$ of $\\widetilde{f}$.\n",
        "\n",
        "*Solution*:\n",
        "> From the chain rule in calculus we have that\n",
        "$$ \\nabla_{\\widetilde{\\boldsymbol{w}}} \\widetilde{f}|_{\\widetilde{\\boldsymbol{w}}} = \n",
        "\\boldsymbol{A}^{-1} \\ \\nabla_{\\boldsymbol{w}}f|_{\\boldsymbol{w} = \\boldsymbol{A}^{-1} \\widetilde{\\boldsymbol{w}}}$$\n",
        "Therefore, when implementing in Python a function for the evaluation of $\\nabla_{\\widetilde{\\boldsymbol{w}}} \\widetilde{f}$, we should\n",
        "> 1. transform the input $\\widetilde{\\boldsymbol{w}}$ into $\\boldsymbol{w} = \\boldsymbol{A}^{-1} \\widetilde{\\boldsymbol{w}}$,\n",
        "> 2. pass the value of $\\boldsymbol{w}$ to the evaluation of $\\nabla_{\\boldsymbol{w}}f$,\n",
        "> 3. premultiply the result of such evaluation by $\\boldsymbol{A}^{-1}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "built-harmony",
      "metadata": {
        "id": "built-harmony"
      },
      "outputs": [],
      "source": [
        "def grad_f_tilde_ex_2_1(w_tilde: np.ndarray) -> np.ndarray:\n",
        "    r\"\"\"Evaluate \\nabla \\tilde{f}(w).\"\"\"\n",
        "    w = np.dot(A_inv, w_tilde)\n",
        "    grad_f = np.array([10 * w[0] + 8 * w[1] - 34, 8 * w[0] + 10 * w[1] - 38])\n",
        "    return np.dot(A_inv, grad_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lovely-tattoo",
      "metadata": {
        "id": "lovely-tattoo"
      },
      "source": [
        "> The global minima $\\widetilde{\\boldsymbol{w}}^*$ of $\\widetilde{f}$ can be obtained from $\\widetilde{\\boldsymbol{w}}^*$ by using the transformation of variable formula\n",
        "$$ \\widetilde{\\boldsymbol{w}} = \\boldsymbol{A} \\boldsymbol{w} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "front-federation",
      "metadata": {
        "id": "front-federation"
      },
      "outputs": [],
      "source": [
        "w_tilde_star = np.dot(A, np.array([1, 3]))\n",
        "w_tilde_star"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nervous-right",
      "metadata": {
        "id": "nervous-right"
      },
      "source": [
        "> We check our implementation of $\\widetilde{f}$ and $\\nabla_{\\widetilde{\\boldsymbol{w}}} \\widetilde{f}$ by evaluating them at $\\widetilde{\\boldsymbol{w}}^*$. Both should give zero as an answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "civilian-maldives",
      "metadata": {
        "id": "civilian-maldives"
      },
      "outputs": [],
      "source": [
        "f_tilde_ex_2_1(w_tilde_star)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "latin-apollo",
      "metadata": {
        "id": "latin-apollo"
      },
      "outputs": [],
      "source": [
        "grad_f_tilde_ex_2_1(w_tilde_star)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "otherwise-document",
      "metadata": {
        "id": "otherwise-document"
      },
      "source": [
        "> Taking a further derivative and applying the chain rule again results in\n",
        "$$ \\nabla_{\\widetilde{\\boldsymbol{w}}}^2 \\widetilde{f}|_{\\widetilde{\\boldsymbol{w}}} = \n",
        "\\boldsymbol{A}^{-2} \\ \\nabla_{\\boldsymbol{w}}^2f|_{\\boldsymbol{w} = \\boldsymbol{A}^{-1} \\widetilde{\\boldsymbol{w}}}.$$\n",
        "Since $\\nabla_{\\boldsymbol{w}}^2f$ is constant we can simply compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "voluntary-jamaica",
      "metadata": {
        "id": "voluntary-jamaica"
      },
      "outputs": [],
      "source": [
        "A_minus_2 = np.dot(A_inv, A_inv)\n",
        "hessian_f_tilde = np.dot(A_minus_2, np.array([[10, 8], [8, 10]]))\n",
        "hessian_f_tilde"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "educational-pennsylvania",
      "metadata": {
        "id": "educational-pennsylvania"
      },
      "source": [
        "12. Apply the gradient descent with the value of $\\alpha$ suggested by the convergence theory and any choice of $\\varepsilon$ and $\\widetilde{\\boldsymbol{w}}_0$. How many iterations does it take to converge to $\\widetilde{\\boldsymbol{w}}^*$?\n",
        "\n",
        "*Solution*:\n",
        "> We copy the implementation of the gradient descent method from the first tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vertical-armor",
      "metadata": {
        "id": "vertical-armor"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_ex_2_1(alpha: float, epsilon: float, w_tilde_0: np.ndarray) -> typing.Tuple[\n",
        "        np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run the gradient descent method with constant step length.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : float\n",
        "        constant step length.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the cost.\n",
        "    w_tilde_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    \"\"\"\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w_tilde = [w_tilde_0]\n",
        "    all_f_tilde = [f_tilde_ex_2_1(w_tilde_0)]\n",
        "    all_grad_f_tilde = [grad_f_tilde_ex_2_1(w_tilde_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Use the error on the cost to determine when the while loop should stop.\n",
        "    while all_f_tilde[k] > epsilon:\n",
        "        w_tilde_k = all_w_tilde[k]\n",
        "        grad_f_tilde_k = all_grad_f_tilde[k]\n",
        "        w_tilde_k_plus_1 = w_tilde_k - alpha * grad_f_tilde_k\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w_tilde.append(w_tilde_k_plus_1)\n",
        "        all_f_tilde.append(f_tilde_ex_2_1(w_tilde_k_plus_1))\n",
        "        all_grad_f_tilde.append(grad_f_tilde_ex_2_1(w_tilde_k_plus_1))\n",
        "\n",
        "        # Bail out if the descent condition is not satisfied\n",
        "        if all_f_tilde[k + 1] >= all_f_tilde[k]:\n",
        "            print(\"WARNING: descent conditions is not satisfied\")\n",
        "            break\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w_tilde), np.array(all_f_tilde), np.array(all_grad_f_tilde)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "meaning-international",
      "metadata": {
        "id": "meaning-international"
      },
      "source": [
        "> From the expression of $\\nabla_{\\widetilde{\\boldsymbol{w}}}^2 \\widetilde{f}$ we get that the smoothness parameter is $L = 1$, because the matrix is diagonal (therefore its eigenvalues are on the diagonal) and the maximum eigenvalue (actually, both of them) is 1. So we choose $\\alpha = 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tight-leisure",
      "metadata": {
        "id": "tight-leisure"
      },
      "outputs": [],
      "source": [
        "all_w_tilde, all_f_tilde, all_grad_f_tilde = gradient_descent_ex_2_1(1, 1e-5, np.array([1.0, 2.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vocal-catch",
      "metadata": {
        "id": "vocal-catch"
      },
      "outputs": [],
      "source": [
        "all_w_tilde"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "neutral-broadway",
      "metadata": {
        "id": "neutral-broadway"
      },
      "source": [
        "> The gradient method applied to $\\widetilde{f}$ converges in one iteration. Try to change $\\varepsilon$ and $\\widetilde{\\boldsymbol{w}}_0$ and confirm that just one iteration will be required regardless of your choice.\n",
        "> By changing the optimization variable we have managed to improve over all the methods discussed in the lecture 1, and get convergence in just one iteration!\n",
        ">\n",
        "> Why does it take only 1 iteration? Recall from the convergence theory that the gradient method is lineary convergent when applied to $L$-smooth and $\\mu$-strongly convex functions, and with $\\alpha = 1/L$ the following formula holds\n",
        "$$ f(\\boldsymbol{w}_{k+1}) - f(\\boldsymbol{w}^*) \\leq \\left(1 - \\frac{\\mu}{L}\\right) \\left(f(\\boldsymbol{w}_k) - f(\\boldsymbol{w}^*)\\right). $$\n",
        "> Here we have $\\mu = L = 1$, so\n",
        "$$ 1 - \\frac{\\mu}{L} = 0.$$\n",
        "Therefore, regardless of the initial error\n",
        "$$ f(\\boldsymbol{w}_0) - f(\\boldsymbol{w}^*),$$\n",
        "the function error at the first iteration will be zero."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "logical-sociology",
      "metadata": {
        "id": "logical-sociology"
      },
      "source": [
        "## Exercise 2.2\n",
        "Let $\\boldsymbol{w} \\in \\mathbb{R}^2$. Consider the following functions:\n",
        "* the *Rosenbrock function* defined as\n",
        "$$r(\\boldsymbol{w}) = 10 \\left(w^{(1)} - [w^{(0)}]^2\\right)^2 + (1 - w^{(0)})^2,$$\n",
        "* the *three-hump Camel function* defined as\n",
        "$$h(\\boldsymbol{w}) = 2 [w^{(0)}]^2 - 1.05 [w^{(0)}]^4 + \\frac{1}{6} [w^{(0)}]^6 + w^{(0)} w^{(1)} + [w^{(1)}]^2.$$\n",
        "\n",
        "1. Draw a contour plot of the functions $r$ and $h$ on the square domain $[-2, 2]^2$.\n",
        "\n",
        "*Solution*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "chicken-lunch",
      "metadata": {
        "id": "chicken-lunch"
      },
      "outputs": [],
      "source": [
        "domain_component_0 = [-2, 2]\n",
        "domain_component_1 = [-2, 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "existing-cosmetic",
      "metadata": {
        "id": "existing-cosmetic"
      },
      "outputs": [],
      "source": [
        "w_component_0 = np.linspace(domain_component_0[0], domain_component_0[1], 200)\n",
        "w_component_1 = np.linspace(domain_component_1[0], domain_component_1[1], 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "internal-favor",
      "metadata": {
        "id": "internal-favor"
      },
      "outputs": [],
      "source": [
        "def r_ex_2_2(w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate r(w).\"\"\"\n",
        "    return 10 * (w[1] - w[0]**2)**2 + (1 - w[0])**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unauthorized-vermont",
      "metadata": {
        "id": "unauthorized-vermont"
      },
      "outputs": [],
      "source": [
        "def h_ex_2_2(w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate h(w).\"\"\"\n",
        "    return 2 * w[0]**2 - 1.05 * w[0]**4 + 1 / 6 * w[0]**6 + w[0] * w[1] + w[1]**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "immediate-crawford",
      "metadata": {
        "id": "immediate-crawford"
      },
      "outputs": [],
      "source": [
        "r_w = np.zeros((len(w_component_0), len(w_component_1)))\n",
        "h_w = np.zeros((len(w_component_0), len(w_component_1)))\n",
        "for i in range(r_w.shape[0]):\n",
        "    for j in range(r_w.shape[1]):\n",
        "        r_w[j, i] = r_ex_2_2([w_component_0[i], w_component_1[j]])\n",
        "        h_w[j, i] = h_ex_2_2([w_component_0[i], w_component_1[j]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "careful-boston",
      "metadata": {
        "id": "careful-boston"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Contour(\n",
        "    x=w_component_0, y=w_component_1, z=np.log10(r_w),\n",
        "    hovertemplate=\"x: %{x:.2f}<br>y: %{y:.2f}<br>z: 10^%{z:.2f} = %{customdata}\", customdata=r_w,\n",
        "    colorbar=dict(tickprefix=\"10^\")\n",
        ")])\n",
        "fig.update_layout(\n",
        "    title=\"Rosenbrock function - contour plot (log scale)\", width=512, height=512, autosize=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caroline-sally",
      "metadata": {
        "id": "caroline-sally"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Contour(\n",
        "    x=w_component_0, y=w_component_1, z=np.log10(h_w),\n",
        "    hovertemplate=\"x: %{x:.2f}<br>y: %{y:.2f}<br>z: 10^%{z:.2f} = %{customdata}\", customdata=h_w,\n",
        "    colorbar=dict(tickprefix=\"10^\")\n",
        ")])\n",
        "fig.update_layout(\n",
        "    title=\"Three-hump Camel function - contour plot (log scale)\", width=512, height=512, autosize=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "coordinate-cache",
      "metadata": {
        "id": "coordinate-cache"
      },
      "source": [
        "2. Compute the gradient $\\nabla r$, the hessian $\\nabla^2 r$ and determine the global minimum of the function $r$.\n",
        "\n",
        "*Solution*:\n",
        "> We compute the partial derivatives of $r(\\boldsymbol{w}) = 10 \\left(w^{(1)} - [w^{(0)}]^2\\right)^2 + (1 - w^{(0)})^2$ to obtain the gradient\n",
        "\\begin{equation*}\n",
        "\\nabla r(\\boldsymbol{w}) = \\begin{bmatrix}\n",
        "-40 w^{(0)} \\left(w^{(1)} - [w^{(0)}]^2\\right) - 2 (1 - w^{(0)})\\\\\n",
        "20 \\left(w^{(1)} - [w^{(0)}]^2\\right)\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "and the hessian\n",
        "\\begin{equation*}\n",
        "\\nabla^2 r(\\boldsymbol{w}) = \\begin{bmatrix}\n",
        "120 [w^{(0)}]^2 - 40 w^{(1)} +2 & - 40 w^{(0)}\\\\\n",
        "-40 w^{(0)} & 20\n",
        "\\end{bmatrix}.\n",
        "\\end{equation*}\n",
        "By solving $\\nabla r(\\boldsymbol{w}) = 0$ we see that $\\boldsymbol{w}_r^* = (1, 1)$ is the only stationary point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "little-contributor",
      "metadata": {
        "id": "little-contributor"
      },
      "outputs": [],
      "source": [
        "w_star_r = np.array([1, 1])\n",
        "w_star_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spread-conversation",
      "metadata": {
        "id": "spread-conversation"
      },
      "outputs": [],
      "source": [
        "r_ex_2_2(w_star_r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "weekly-today",
      "metadata": {
        "id": "weekly-today"
      },
      "outputs": [],
      "source": [
        "def grad_r_ex_2_2(w: np.ndarray) -> np.ndarray:\n",
        "    r\"\"\"Evaluate \\nabla r(w).\"\"\"\n",
        "    return np.array([\n",
        "        - 40 * w[0] * (w[1] - w[0]**2) - 2 * (1 - w[0]),\n",
        "        20 * (w[1] - w[0]**2)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "latin-julian",
      "metadata": {
        "id": "latin-julian"
      },
      "outputs": [],
      "source": [
        "grad_r_ex_2_2(w_star_r)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dependent-bathroom",
      "metadata": {
        "id": "dependent-bathroom"
      },
      "source": [
        "> We check the sufficient condition for $\\boldsymbol{w}^*$ to be (the global) minimum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "genuine-geology",
      "metadata": {
        "id": "genuine-geology"
      },
      "outputs": [],
      "source": [
        "def hessian_r_ex_2_2(w: np.ndarray) -> np.ndarray:\n",
        "    r\"\"\"Evaluate \\nabla^2 r(w).\"\"\"\n",
        "    return np.array([\n",
        "        [120 * w[0]**2 - 40 * w[1] + 2, - 40 * w[0]],\n",
        "        [- 40 * w[0], 20]\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ongoing-relation",
      "metadata": {
        "id": "ongoing-relation"
      },
      "outputs": [],
      "source": [
        "eigs, _ = np.linalg.eig(hessian_r_ex_2_2(w_star_r))\n",
        "eigs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "grateful-trauma",
      "metadata": {
        "id": "grateful-trauma"
      },
      "outputs": [],
      "source": [
        "assert (eigs > 0).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "included-judge",
      "metadata": {
        "id": "included-judge"
      },
      "source": [
        "> Notice how the eigenvalues at the global minimum are almost three orders of magnitude apart. This indicates that the minimum is in a very narrow valley."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "innocent-candle",
      "metadata": {
        "id": "innocent-candle"
      },
      "source": [
        "3. Compute the gradient $\\nabla h$, the hessian $\\nabla^2 h$ and determine the global minimum of the function $h$.\n",
        "\n",
        "*Solution*:\n",
        "> We compute the partial derivatives of $h(\\boldsymbol{w}) = 2 [w^{(0)}]^2 - 1.05 [w^{(0)}]^4 + \\frac{1}{6} [w^{(0)}]^6 + w^{(0)} w^{(1)} + [w^{(1)}]^2$ to obtain the gradient\n",
        "\\begin{equation*}\n",
        "\\nabla h(\\boldsymbol{w}) = \\begin{bmatrix}\n",
        "4 w^{(0)} - 4.2 [w^{(0)}]^3 + [w^{(0)}]^5 + w^{(1)}\\\\\n",
        "w^{(0)} + 2 w^{(1)}\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "and the hessian\n",
        "\\begin{equation*}\n",
        "\\nabla^2 h(\\boldsymbol{w}) = \\begin{bmatrix}\n",
        "4 - 12.6 [w^{(0)}]^2 + 5 [w^{(0)}]^4 & 1\\\\\n",
        "1 & 2\n",
        "\\end{bmatrix}.\n",
        "\\end{equation*}\n",
        "The system $\\nabla r(\\boldsymbol{w}) = 0$ results in\n",
        "\\begin{equation*}\n",
        "\\begin{cases}\n",
        "3.5 w^{(0)}  - 4.2 [w^{(0)}]^3 + [w^{(0)}]^5 = 0\\\\\n",
        "w^{(1)} = - 0.5 w^{(0)}\n",
        "\\end{cases}\n",
        "\\end{equation*}\n",
        "We can easily solve the polynomial equation in the first line by collecting the common factor $w^{(0)}$ and reducing the resulting fourth order term to a second order equation. Alternatively, we query the rootfinding facilities for polynomials provided in [`numpy.roots`](https://numpy.org/doc/stable/reference/generated/numpy.roots.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lyric-biodiversity",
      "metadata": {
        "id": "lyric-biodiversity"
      },
      "outputs": [],
      "source": [
        "roots = np.roots([1, 0, -4.2, 0, 3.5, 0])\n",
        "roots = np.sort(roots)\n",
        "roots"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "equivalent-ozone",
      "metadata": {
        "id": "equivalent-ozone"
      },
      "source": [
        "> The function $h$ has 5 stationary points, which we collect in a `numpy array` below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "accomplished-simulation",
      "metadata": {
        "id": "accomplished-simulation"
      },
      "outputs": [],
      "source": [
        "w_star_h = np.vstack([roots, - 0.5 * roots]).T\n",
        "w_star_h"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "loved-composite",
      "metadata": {
        "id": "loved-composite"
      },
      "source": [
        "> We determine the corresponding function $h$ values at each stationary point by means of a `for` loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "direct-delay",
      "metadata": {
        "id": "direct-delay"
      },
      "outputs": [],
      "source": [
        "for row_h in range(w_star_h.shape[0]):\n",
        "    print(\"h(\" + str(w_star_h[row_h, :]) + \" = \" + str(h_ex_2_2(w_star_h[row_h, :])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "absent-slovakia",
      "metadata": {
        "id": "absent-slovakia"
      },
      "source": [
        "> We implement a Python function for the evaluation of $\\nabla h$, and test it on the stationary points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nervous-retrieval",
      "metadata": {
        "id": "nervous-retrieval"
      },
      "outputs": [],
      "source": [
        "def grad_h_ex_2_2(w: np.ndarray) -> np.ndarray:\n",
        "    r\"\"\"Evaluate \\nabla h(w).\"\"\"\n",
        "    return np.array([\n",
        "        4 * w[0] - 4.2 * w[0]**3 + w[0]**5 + w[1],\n",
        "        w[0] + 2 * w[1]\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dominant-beach",
      "metadata": {
        "id": "dominant-beach"
      },
      "outputs": [],
      "source": [
        "for row_h in range(w_star_h.shape[0]):\n",
        "    print(\"grad h(\" + str(w_star_h[row_h, :]) + \" = \" + str(grad_h_ex_2_2(w_star_h[row_h, :])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "theoretical-bidder",
      "metadata": {
        "id": "theoretical-bidder"
      },
      "source": [
        "> Finally, we implement a Python function for the evaluation of $\\nabla^2 h$, and use it to determine the nature (minimum, maximum, saddle) of the stationary points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "entitled-humidity",
      "metadata": {
        "id": "entitled-humidity"
      },
      "outputs": [],
      "source": [
        "def hessian_h_ex_2_2(w: np.ndarray) -> np.ndarray:\n",
        "    r\"\"\"Evaluate \\nabla^2 h(w).\"\"\"\n",
        "    return np.array([\n",
        "        [4 - 12.6 * w[0]**2 + 5 * w[0]**4, 1],\n",
        "        [1, 2]\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "compressed-ranking",
      "metadata": {
        "id": "compressed-ranking"
      },
      "outputs": [],
      "source": [
        "for row_h in range(w_star_h.shape[0]):\n",
        "    eigs, _ = np.linalg.eig(hessian_h_ex_2_2(w_star_h[row_h, :]))\n",
        "    print(\"eigenvalues of hessian h(\" + str(w_star_h[row_h, :]) + \" = \" + str(eigs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "color-proposition",
      "metadata": {
        "id": "color-proposition"
      },
      "source": [
        "> From these information we determine that there is a global minimum at $(0, 0)$, two addiational local minima, and two saddle points."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "geological-spokesman",
      "metadata": {
        "id": "geological-spokesman"
      },
      "source": [
        "4. Implement minimization of a function $f$ using the gradient descent or the Newton method with backtracking line search in a Python function. Such function should:\n",
        "   * take as input the method to use (`Newton` or `gradient`), the function $f$, its gradient $\\nabla f$, its hessian $\\nabla^2 f$, the constants $\\alpha$, $c_1$ and $c_2$ of the backtracking algorithm, the tolerance $\\varepsilon$ for the stopping criterion and the initial condition $\\boldsymbol{w}_{0}$;\n",
        "   * return as outputs the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$, the corresponding function values $\\{f(\\boldsymbol{w}_k)\\}_k$ and gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$.\n",
        "\n",
        "   Use the following stopping criteria:\n",
        "   * the main stopping criterion is based on the norm of the gradient. Continue the iterations while such norm is above $\\varepsilon$;\n",
        "   * however, when applying the Newton method in combination with backtracking, it may well be possible that search direction $\\boldsymbol{p}_k$ is *not* a descent direction for some iterations $k$, and backtracking will finish only when $\\alpha_k = 0$. To avoid getting stuck in an infinite loop, we adapt the following safeguard: if the norm of the increment between two iterations is below the tolerance $\\varepsilon$ then terminate early, and print a warning. Similar safeguards to avoid infinite loops are very common in practical implementations of the methods we see in this course. \n",
        "\n",
        "*Solution*:\n",
        "> We start from the implementation of the gradient descent with bracktracking we discussed in Tutorial 1, and change the definition of the search direction $\\boldsymbol{p}_k$ according to the `method` input. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "popular-posting",
      "metadata": {
        "id": "popular-posting"
      },
      "outputs": [],
      "source": [
        "def optimization_with_backtracking_line_search_ex_2_2(\n",
        "    method: str, f: typing.Callable, grad_f: typing.Callable, hess_f: typing.Callable, alpha: float,\n",
        "    c_1: float, c_2: float, epsilon: float, w_0: np.ndarray\n",
        ") -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run the gradient descent method or Newton method with backtracking line search.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    method : str\n",
        "        optimization method to be used, either \"gradient\" or \"Newton\".\n",
        "    f, grad_f, hess_f : Python function\n",
        "        callable evaluating the cost function, its gradient and its hessian, respectively.\n",
        "    alpha : float\n",
        "        initial step length.\n",
        "    c_1, c_2 : float\n",
        "        constants of the backtracking algorithm.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the norm of the gradient of the cost.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    \"\"\"\n",
        "    assert method in (\"gradient\", \"Newton\")\n",
        "\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_f = [f(w_0)]\n",
        "    all_grad_f = [grad_f(w_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Use the norm of the gradient as stopping criterion.\n",
        "    while np.linalg.norm(all_grad_f[k]) > epsilon:\n",
        "        w_k = all_w[k]\n",
        "        f_k = all_f[k]\n",
        "        grad_f_k = all_grad_f[k]\n",
        "\n",
        "        # Define the search direction p_k\n",
        "        if method == \"gradient\":\n",
        "            p_k = - grad_f_k\n",
        "            p_k_dot_g_k = - np.linalg.norm(grad_f_k)**2\n",
        "        elif method == \"Newton\":\n",
        "            hess_f_k = hess_f(w_k)\n",
        "            p_k = - np.linalg.solve(hess_f_k, grad_f_k)\n",
        "            p_k_dot_g_k = np.dot(p_k, grad_f_k)\n",
        "\n",
        "        # Carry out a backtracking line search\n",
        "        alpha_k = alpha\n",
        "        while f(w_k + alpha_k * p_k) > f_k + c_1 * alpha_k * p_k_dot_g_k:\n",
        "            alpha_k = c_2 * alpha_k\n",
        "\n",
        "        # Compute w_{k+1}\n",
        "        w_k_plus_1 = w_k + alpha_k * p_k\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f(w_k_plus_1))\n",
        "        all_grad_f.append(grad_f(w_k_plus_1))\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "        # Bail out if backtracking failed\n",
        "        if np.linalg.norm(all_grad_f[k] - all_grad_f[k - 1]) < epsilon:\n",
        "            print(\"WARNING: \" + method + \" method terminated early due to not having found a descent step. \"\n",
        "                  + \"Backtracking terminated with alpha_k = \" + str(alpha_k))\n",
        "            break\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_f), np.array(all_grad_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "split-clarity",
      "metadata": {
        "id": "split-clarity"
      },
      "source": [
        "5. Choose $\\alpha = 1$, $c_1 = 0.1$, $c_2 = 0.7$, $\\varepsilon = 10^{-5}$, $\\boldsymbol{w}_0 = (-1, 1.5)$, and the Rosenbrock function $r$. Run gradient and Newton methods, and visualize a semilogarithimic plot of error in the function value $\\{r(\\boldsymbol{w}_k) - r(\\boldsymbol{w}^*)\\}_k$ versus the iteration counter $k$.\n",
        "\n",
        "*Solution*:\n",
        "> We run our previous implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "intensive-schedule",
      "metadata": {
        "id": "intensive-schedule"
      },
      "outputs": [],
      "source": [
        "all_w_gradient, all_r_gradient, all_grad_r_gradient = optimization_with_backtracking_line_search_ex_2_2(\n",
        "    \"gradient\", r_ex_2_2, grad_r_ex_2_2, hessian_r_ex_2_2, 1, 0.1, 0.7, 1e-5, np.array([-1.0, 1.5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nutritional-testing",
      "metadata": {
        "id": "nutritional-testing"
      },
      "outputs": [],
      "source": [
        "all_w_gradient.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ruled-myrtle",
      "metadata": {
        "id": "ruled-myrtle"
      },
      "outputs": [],
      "source": [
        "all_w_gradient[-1, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "primary-satin",
      "metadata": {
        "id": "primary-satin"
      },
      "outputs": [],
      "source": [
        "all_w_newton, all_r_newton, all_grad_r_newton = optimization_with_backtracking_line_search_ex_2_2(\n",
        "    \"Newton\", r_ex_2_2, grad_r_ex_2_2, hessian_r_ex_2_2, 1, 0.1, 0.7, 1e-5, np.array([-1.0, 1.5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fresh-spring",
      "metadata": {
        "id": "fresh-spring"
      },
      "outputs": [],
      "source": [
        "all_w_newton.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scenic-absorption",
      "metadata": {
        "id": "scenic-absorption"
      },
      "outputs": [],
      "source": [
        "all_w_newton[-1, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "charitable-oliver",
      "metadata": {
        "id": "charitable-oliver"
      },
      "source": [
        "> Both methods converge to the global minimum. However, Newton method converges in roughly 10 iterations, while the gradient method takes more than 1000 iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "destroyed-julian",
      "metadata": {
        "id": "destroyed-julian"
      },
      "source": [
        "> The semilog plot of the error on the function value clearly shows this fast convergence. Compared to the line of the gradient method, the line corresponding to the Newton method is almost vertical close to convergence. To better appreciate this uncomment the line that restricts the horizontal axis to the interval $[0, 15]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "extensive-russian",
      "metadata": {
        "id": "extensive-russian"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "all_methods = [\"Gradient\", \"Newton\"]\n",
        "all_r_method = [all_r_gradient, all_r_newton]\n",
        "for method_index in range(2):\n",
        "    fig.add_scatter(\n",
        "        x=np.arange(all_r_method[method_index].shape[0]), y=all_r_method[method_index],\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[method_index], size=10),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[method_index], width=2),\n",
        "        mode=\"lines+markers\", name=all_methods[method_index] + \" method\"\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=\"Rosenbrock function - error on the function value - different methods\",\n",
        "    width=768, height=768, autosize=False\n",
        ")\n",
        "# fig.update_xaxes(range=[0, 15])\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "heard-proof",
      "metadata": {
        "id": "heard-proof"
      },
      "source": [
        "> With this case we have concluded the graphical representation of the concepts of sublinear, linear and superlinear convergence. The curve of the error measure in a semilog plot, at least for $k$ big enough:\n",
        "> * resembles an almost horizontal line when there is sublinear convergence (that we had seen in Exercise 1.3, as motivation for acceleration techniques);\n",
        "> * resembles an almost vertical line when there is superlinear convergence (as the quadratic convergence of the Newton method in this exercise);\n",
        "> * resembles a \"diagonal\" line when there is linear convergence corresponds (as the convergence of the gradient method in this exercise)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nominated-syracuse",
      "metadata": {
        "id": "nominated-syracuse"
      },
      "source": [
        "6. Choose $\\alpha = 1$, $c_1 = 0.1$, $c_2 = 0.7$, $\\varepsilon = 10^{-5}$, two choices for $\\boldsymbol{w}_0 = $ $(-1, 0.7)$ or $(0.7, 0.7)$, and the three-hump Camel function $h$. To which stationary point do the gradient and Newton methods converge?\n",
        "\n",
        "*Solution*:\n",
        "> We run our previous implementation $\\boldsymbol{w}_0 = (-1, 0.7)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fatal-ebony",
      "metadata": {
        "id": "fatal-ebony"
      },
      "outputs": [],
      "source": [
        "all_w_gradient, all_h_gradient, all_grad_h_gradient = optimization_with_backtracking_line_search_ex_2_2(\n",
        "    \"gradient\", h_ex_2_2, grad_h_ex_2_2, hessian_h_ex_2_2, 1, 0.1, 0.7, 1e-5, np.array([-1.0, 0.7]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "several-consistency",
      "metadata": {
        "id": "several-consistency"
      },
      "outputs": [],
      "source": [
        "all_w_gradient.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "composed-gross",
      "metadata": {
        "id": "composed-gross"
      },
      "outputs": [],
      "source": [
        "all_w_gradient[-1, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "geological-territory",
      "metadata": {
        "id": "geological-territory"
      },
      "outputs": [],
      "source": [
        "all_w_newton, all_h_newton, all_grad_h_newton = optimization_with_backtracking_line_search_ex_2_2(\n",
        "    \"Newton\", h_ex_2_2, grad_h_ex_2_2, hessian_h_ex_2_2, 1, 0.1, 0.7, 1e-5, np.array([-1.0, 0.7]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fallen-hamburg",
      "metadata": {
        "id": "fallen-hamburg"
      },
      "outputs": [],
      "source": [
        "all_w_newton.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "assisted-rabbit",
      "metadata": {
        "id": "assisted-rabbit"
      },
      "outputs": [],
      "source": [
        "all_w_newton[-1, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "julian-mixer",
      "metadata": {
        "id": "julian-mixer"
      },
      "source": [
        "> From the same initial point $\\boldsymbol{w}_0 = (-1, 0.7)$, the gradient method converged to the global minimum, while the Newton method got stuck very close to a saddle point, unable to find a descent direction anymore."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "extended-nutrition",
      "metadata": {
        "id": "extended-nutrition"
      },
      "source": [
        "> We next run our previous implementation $\\boldsymbol{w}_0 = (0.7, 0.7)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exciting-reason",
      "metadata": {
        "id": "exciting-reason"
      },
      "outputs": [],
      "source": [
        "all_w_gradient, all_h_gradient, all_grad_h_gradient = optimization_with_backtracking_line_search_ex_2_2(\n",
        "    \"gradient\", h_ex_2_2, grad_h_ex_2_2, hessian_h_ex_2_2, 1, 0.1, 0.7, 1e-5, np.array([0.7, 0.7]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "earned-removal",
      "metadata": {
        "id": "earned-removal"
      },
      "outputs": [],
      "source": [
        "all_w_gradient.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "formal-latex",
      "metadata": {
        "id": "formal-latex"
      },
      "outputs": [],
      "source": [
        "all_w_gradient[-1, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "desirable-courage",
      "metadata": {
        "id": "desirable-courage"
      },
      "outputs": [],
      "source": [
        "all_w_newton, all_h_newton, all_grad_h_newton = optimization_with_backtracking_line_search_ex_2_2(\n",
        "    \"Newton\", h_ex_2_2, grad_h_ex_2_2, hessian_h_ex_2_2, 1, 0.1, 0.7, 1e-5, np.array([0.7, 0.7]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "living-labor",
      "metadata": {
        "id": "living-labor"
      },
      "outputs": [],
      "source": [
        "all_w_newton.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stainless-median",
      "metadata": {
        "id": "stainless-median"
      },
      "outputs": [],
      "source": [
        "all_w_newton[-1, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tutorial-communication",
      "metadata": {
        "id": "tutorial-communication"
      },
      "source": [
        "> From the same initial point $\\boldsymbol{w}_0 = (0.7, 0.7)$, the gradient method converged to the global minimum, while Newton method converged to a local minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "strategic-career",
      "metadata": {
        "id": "strategic-career"
      },
      "source": [
        "7. Apply both gradient and Newton method starting from $6^2$ different inital conditions, equispaced in the domain $[-2, 2]$. Compare the two methods in terms of the number of runs that converge to the global minimum, local minima and saddle point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "equivalent-chancellor",
      "metadata": {
        "id": "equivalent-chancellor"
      },
      "outputs": [],
      "source": [
        "solutions = {\"gradient\": dict(), \"Newton\": dict()}\n",
        "for i in np.linspace(-2, 2, 6):\n",
        "    for j in np.linspace(-2, 2, 6):\n",
        "        all_w_gradient, _, _ = optimization_with_backtracking_line_search_ex_2_2(\n",
        "            \"gradient\", h_ex_2_2, grad_h_ex_2_2, hessian_h_ex_2_2, 1, 0.1, 0.7, 1e-5, np.array([i, j]))\n",
        "        all_w_newton, _, _ = optimization_with_backtracking_line_search_ex_2_2(\n",
        "            \"Newton\", h_ex_2_2, grad_h_ex_2_2, hessian_h_ex_2_2, 1, 0.1, 0.7, 1e-5, np.array([i, j]))\n",
        "        optimal_w = {\n",
        "            \"gradient\": tuple(np.round(all_w_gradient[-1], 2)),\n",
        "            \"Newton\": tuple(np.round(all_w_newton[-1], 2))\n",
        "        }\n",
        "        for method in (\"gradient\", \"Newton\"):\n",
        "            if optimal_w[method] not in solutions[method]:\n",
        "                solutions[method][optimal_w[method]] = 0\n",
        "            solutions[method][optimal_w[method]] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "agricultural-buffalo",
      "metadata": {
        "id": "agricultural-buffalo"
      },
      "outputs": [],
      "source": [
        "solutions_np = dict()\n",
        "for method in (\"gradient\", \"Newton\"):\n",
        "    solutions_keys_np = np.array(list(solutions[method].keys()))\n",
        "    solutions_values_np = np.array(list(solutions[method].values())).reshape(-1, 1)\n",
        "    solutions_np[method] = np.hstack((solutions_keys_np, solutions_values_np))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "loose-importance",
      "metadata": {
        "id": "loose-importance"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Contour(\n",
        "    x=w_component_0, y=w_component_1, z=np.log10(h_w),\n",
        "    hovertemplate=\"x: %{x:.2f}<br>y: %{y:.2f}<br>z: 10^%{z:.2f} = %{customdata}\", customdata=h_w,\n",
        "    showscale=False\n",
        ")])\n",
        "for (method_index, method) in enumerate((\"gradient\", \"Newton\")):\n",
        "    fig.add_scatter(\n",
        "        x=solutions_np[method][:, 0], y=solutions_np[method][:, 1],\n",
        "        marker=dict(\n",
        "            size=5 * np.sqrt(solutions_np[method][:, 2]), color=plotly.colors.qualitative.Set1[method_index]),\n",
        "        mode=\"markers\",\n",
        "        hovertemplate=\"(%{x}, %{y}) found %{customdata} times\", name=method + \" method\",\n",
        "        customdata=solutions_np[method][:, 2]\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=\"Three-hump Camel function - convergence to minima\", width=512, height=512, autosize=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "different-sharing",
      "metadata": {
        "id": "different-sharing"
      },
      "source": [
        "> Both methods have converged to local minima (rather than the desired global minimum) in around 30\\% of the runs.\n",
        "> The gradient method has never converged to a saddle point; in contrast more than 30\\% runs of the Newton method have converged to a saddle point. This observation is more general than the current example, and has been found in several practical applications (especially in machine learning)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "alpha-killing",
      "metadata": {
        "id": "alpha-killing"
      },
      "source": [
        "## Exercise 2.3\n",
        "A US university has collected data of past admission into graduate school, and their researchers are interested in how the following variables:\n",
        "* GRE (Graduate Record Exam) subject tests scores, a standardized test that assesses technical knowledge related to a specific discipline,\n",
        "* GPA (grade point average) of the candidate, i.e. the average of their marks, and\n",
        "* prestige of the undergraduate institution that the student attended\n",
        "\n",
        "are correlated to the admission (or non-admission) of a prospective student. Therefore, we can try model this binary classification problem via a logistic regression.\n",
        "\n",
        "The response variable is a binary variable (admit/don't admit).\n",
        "\n",
        "1. Load the dataset of historical data from the [UCLA website](https://stats.idre.ucla.edu/r/dae/logit-regression/).\n",
        "\n",
        "*Solution*:\n",
        "> To carry out this task we introduce the [`pandas`](https://pandas.pydata.org/) library, a fast, powerful, flexible and easy to use open source data analysis and manipulation tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "earlier-pilot",
      "metadata": {
        "id": "earlier-pilot"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aba20117",
      "metadata": {
        "id": "aba20117"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "german-radar",
      "metadata": {
        "id": "german-radar"
      },
      "source": [
        "> We download the `csv` dataset from the UCLA website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fantastic-nelson",
      "metadata": {
        "id": "fantastic-nelson"
      },
      "outputs": [],
      "source": [
        "if os.path.isfile(\"data/binary.csv\"):\n",
        "    csv_path = \"data/binary.csv\"\n",
        "else:\n",
        "    # csv_path = \"https://stats.idre.ucla.edu/stat/data/binary.csv\"\n",
        "    csv_path = (\n",
        "        \"https://dmf.unicatt.it/~fball/public/optimization_for_machine_learning\"\n",
        "        + \"/binary.csv\"\n",
        "    )\n",
        "df = pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "precise-integral",
      "metadata": {
        "id": "precise-integral"
      },
      "source": [
        "> Once the dataset is imported in pandas, we can print the first few entries of it..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "textile-vehicle",
      "metadata": {
        "id": "textile-vehicle"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "early-nerve",
      "metadata": {
        "id": "early-nerve"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ranging-knitting",
      "metadata": {
        "id": "ranging-knitting"
      },
      "source": [
        "> ... print its main descriptive statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vocal-better",
      "metadata": {
        "id": "vocal-better"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "classical-indication",
      "metadata": {
        "id": "classical-indication"
      },
      "outputs": [],
      "source": [
        "df.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "understood-filename",
      "metadata": {
        "id": "understood-filename"
      },
      "outputs": [],
      "source": [
        "df.std()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "reasonable-publisher",
      "metadata": {
        "id": "reasonable-publisher"
      },
      "source": [
        "> ... visualize it by means of histograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "distinguished-boost",
      "metadata": {
        "id": "distinguished-boost"
      },
      "outputs": [],
      "source": [
        "df.hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chemical-float",
      "metadata": {
        "id": "chemical-float"
      },
      "source": [
        "> ... and much more!\n",
        "\n",
        "2. GRE and GPA are continuous variables, while the rank of the home university is a categorical variable, where the top-ranked universities are denote by 1 and lowest-ranked institutions by 4. The standard way to handle categorical variables in a regression task is to introduce [dummy variables ](https://en.wikipedia.org/wiki/Dummy_variable_(statistics))\n",
        "$$r_1, r_2, r_3 \\text{ and } r_4$$\n",
        "where rank 1 universities are represented by\n",
        "$$r_1 = 1, \\quad r_2 = 0, \\quad r_3 = 0, \\quad r_4 = 0,$$\n",
        "rank 2 universities are represented by\n",
        "$$r_1 = 0, \\quad r_2 = 1, \\quad r_3 = 0, \\quad r_4 = 0,$$\n",
        "and similarly for ranks 3 and 4. In machine learning this process is often called [one-hot encoding](https://en.wikipedia.org/wiki/One-hot).\n",
        "\n",
        "   One easily notices that introducing all such four variables in a regression task would lead to a linear dependence between the independent variables (features) of the regression. Indeed, one can discard the variable $r_1$ because just by knowning $r_2 = r_3 = r_4 = 0$ we deduce that the student comes from a rank 1 university.\n",
        " \n",
        "   Prepare a new dataset with the following columns: admit, GRE, GPA, rank_2, rank_3 and rank_4.\n",
        " \n",
        "*Solution*:\n",
        "> `pandas` offer a simple way to generate dummy variables associated to a categorical variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "virgin-upper",
      "metadata": {
        "id": "virgin-upper"
      },
      "outputs": [],
      "source": [
        "dummy_ranks = pd.get_dummies(df[\"rank\"], prefix=\"rank\")\n",
        "dummy_ranks.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "derived-concentration",
      "metadata": {
        "id": "derived-concentration"
      },
      "source": [
        "> We then exclude the rank_1 column, and collect all required columns in a new dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "respected-dayton",
      "metadata": {
        "id": "respected-dayton"
      },
      "outputs": [],
      "source": [
        "data = df[[\"admit\", \"gre\", \"gpa\"]].join(dummy_ranks[[\"rank_2\", \"rank_3\", \"rank_4\"]])\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "geographic-appeal",
      "metadata": {
        "id": "geographic-appeal"
      },
      "source": [
        "3. Separate the labels (admit/don't admit) from the features (GRE, GPA, ranks from 2 to 4), as follows.\n",
        "\n",
        "   Put labels in a vector $\\boldsymbol{y}$, where the $j$-th entry stores whether the $j$-th student was admitted or not.\n",
        "\n",
        "   Put features in a matrix $\\boldsymbol{X}$, where the $j$-th row stores the features associated to $j$-th student, ordered as follows: \n",
        "   * GRE on the first column, \n",
        "   * GPA on the second column, \n",
        "   * whether or not they come from a rank 2 university of the third column, \n",
        "   * whether or not they come from a rank 3 university of the fourth column, \n",
        "   * whether or not they come from a rank 4 university of the fifth column.\n",
        "\n",
        "*Solution*:\n",
        "> Note that in the following we will use the capital letter $\\boldsymbol{X}$ to the denote the matrix of (all) features, while the lowercase letter $\\boldsymbol{x}$ will denote a feature vector (a single student, or a single row of the matrix)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "korean-waters",
      "metadata": {
        "id": "korean-waters"
      },
      "outputs": [],
      "source": [
        "y = data[[\"admit\"]].to_numpy().reshape(-1)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "injured-forestry",
      "metadata": {
        "id": "injured-forestry"
      },
      "outputs": [],
      "source": [
        "X = data[[\"gre\", \"gpa\", \"rank_2\", \"rank_3\", \"rank_4\"]].to_numpy()\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hollow-stopping",
      "metadata": {
        "id": "hollow-stopping"
      },
      "source": [
        "4. Separate the dataset $(\\boldsymbol{X}, \\boldsymbol{y})$, composed of 400 rows, in:\n",
        "   1. a training dataset $(\\boldsymbol{X}_{\\text{train}}, \\boldsymbol{y}_{\\text{train}})$, composed of $m_{\\text{train}} = 350$ randomly selected rows, and\n",
        "   2. a test dataset $(\\boldsymbol{X}_{\\text{test}}, \\boldsymbol{y}_{\\text{test}})$, composed of the remaining $m_{\\text{test}} = 50$ rows.\n",
        " \n",
        "*Solution*:\n",
        "> In order to randomly select rows, we first define a random permutation of the 400 row indices, using [`numpy.random.permutation`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.permutation.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "extensive-flooring",
      "metadata": {
        "id": "extensive-flooring"
      },
      "outputs": [],
      "source": [
        "np.random.seed(23)\n",
        "perm = np.random.permutation(y.shape[0])\n",
        "perm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "contrary-subcommittee",
      "metadata": {
        "id": "contrary-subcommittee"
      },
      "source": [
        "> We then use the first 350 indices in `perm` to define the training set, and the remaining indices to define the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "immediate-transaction",
      "metadata": {
        "id": "immediate-transaction"
      },
      "outputs": [],
      "source": [
        "y_train = y[perm[:350]]\n",
        "X_train = X[perm[:350]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "complicated-devices",
      "metadata": {
        "id": "complicated-devices"
      },
      "outputs": [],
      "source": [
        "y_test = y[perm[350:]]\n",
        "X_test = X[perm[350:]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "subtle-cooler",
      "metadata": {
        "id": "subtle-cooler"
      },
      "source": [
        "5. Implement the evaluation of the empirical risk associated to a logistic regression, as well as its gradient and hessian.\n",
        "\n",
        "*Solution*:\n",
        "> Similarly to Exercise 1.3, we start by implementing the prediction function $\\hat{y}(\\boldsymbol{x}; \\boldsymbol{w}) = \\sigma(\\boldsymbol{s}^T \\boldsymbol{x} + q)$ associated to a logistic regression with $\\boldsymbol{x} \\in \\mathbb{R}^5$, where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function, and the weights (optimization variable) are given by\n",
        "$$\\boldsymbol{w} = \\begin{bmatrix}\\boldsymbol{s}\\\\ q\\end{bmatrix} = \\begin{bmatrix}s^{(0)}\\\\s^{(1)}\\\\ \\dots\\\\s^{(4)}\\\\q\\end{bmatrix} \\in \\mathbb{R}^6.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "modern-summary",
      "metadata": {
        "id": "modern-summary"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z: float) -> float:\n",
        "    \"\"\"Evaluate the sigmoid function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sunrise-supply",
      "metadata": {
        "id": "sunrise-supply"
      },
      "outputs": [],
      "source": [
        "def y_hat(x_j: np.ndarray, w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate the prediction function associated to a logistic regression.\"\"\"\n",
        "    return sigmoid(np.dot(w[0:5], x_j) + w[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "transparent-triangle",
      "metadata": {
        "id": "transparent-triangle"
      },
      "source": [
        "> The logistic loss and its derivatives are:\n",
        "$$\\ell(\\boldsymbol{x}, y; \\boldsymbol{w}) = - y \\log \\sigma(\\boldsymbol{s}^T \\boldsymbol{x} + q) - (1 - y) \\log (1 - \\sigma(\\boldsymbol{s}^T \\boldsymbol{x} + q)).$$\n",
        "$$\\nabla_\\boldsymbol{w} \\ell(\\boldsymbol{x}, y; \\boldsymbol{w}) = (- y + \\sigma(\\boldsymbol{s}^T \\boldsymbol{x} + q)) \\begin{bmatrix}\\boldsymbol{x}\\\\1\\end{bmatrix}.$$\n",
        "$$\\nabla_\\boldsymbol{w}^2 \\ell(\\boldsymbol{x}, y; \\boldsymbol{w}) = \\sigma(\\boldsymbol{s}^T \\boldsymbol{x} + q) (1 - \\sigma(\\boldsymbol{s}^T \\boldsymbol{x} + q)) \\begin{bmatrix}\\boldsymbol{x} \\boldsymbol{x}^T & \\boldsymbol{x}\\\\\\boldsymbol{x}^T & 1\\end{bmatrix}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "weighted-transition",
      "metadata": {
        "id": "weighted-transition"
      },
      "outputs": [],
      "source": [
        "def logistic_loss(x_j: np.ndarray, y_j: float, w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate the logistic loss.\"\"\"\n",
        "    return - y_j * np.log(y_hat(x_j, w)) - (1 - y_j) * np.log(1 - y_hat(x_j, w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sharp-sheriff",
      "metadata": {
        "id": "sharp-sheriff"
      },
      "outputs": [],
      "source": [
        "def grad_logistic_loss(x_j: np.ndarray, y_j: float, w: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Evaluate the gradient of the logistic loss.\"\"\"\n",
        "    vec = np.zeros((6, ))\n",
        "    vec[0:5] = x_j\n",
        "    vec[5] = 1\n",
        "    return (y_hat(x_j, w) - y_j) * vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "heard-newcastle",
      "metadata": {
        "id": "heard-newcastle"
      },
      "outputs": [],
      "source": [
        "def hessian_logistic_loss(x_j: np.ndarray, y_j: float, w: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Evaluate the hessian of the logistic loss.\"\"\"\n",
        "    mat = np.zeros((6, 6))\n",
        "    mat[0:5, 0:5] = np.outer(x_j, x_j)\n",
        "    mat[0:5, 5] = mat[5, 0:5] = x_j\n",
        "    mat[5, 5] = 1\n",
        "    return y_hat(x_j, w) * (1 - y_hat(x_j, w)) * mat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "critical-certificate",
      "metadata": {
        "id": "critical-certificate"
      },
      "source": [
        "> The empirical risk can finally be obtained summing the loss functions corresponding to every row $\\boldsymbol{x}_j$ and label $y_j$ of the *training* dataset. In the implementation below, note how the `for` loop over `X_train` will be carried out row by row, which is actually what we need here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "quantitative-railway",
      "metadata": {
        "id": "quantitative-railway"
      },
      "outputs": [],
      "source": [
        "def f_ex_2_3(w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate the empirical risk.\"\"\"\n",
        "    m = X_train.shape[0]\n",
        "    return 1 / m * sum(logistic_loss(x_j, y_j, w) for (x_j, y_j) in zip(X_train, y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "running-spanish",
      "metadata": {
        "id": "running-spanish"
      },
      "outputs": [],
      "source": [
        "def grad_f_ex_2_3(w: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Evaluate the gradient of the empirical risk.\"\"\"\n",
        "    m = X_train.shape[0]\n",
        "    return 1 / m * sum(grad_logistic_loss(x_j, y_j, w) for (x_j, y_j) in zip(X_train, y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "under-wrong",
      "metadata": {
        "id": "under-wrong"
      },
      "outputs": [],
      "source": [
        "def hessian_f_ex_2_3(w: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Evaluate the hessian of the empirical risk.\"\"\"\n",
        "    m = X_train.shape[0]\n",
        "    return 1 / m * sum(hessian_logistic_loss(x_j, y_j, w) for (x_j, y_j) in zip(X_train, y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "atmospheric-elder",
      "metadata": {
        "id": "atmospheric-elder"
      },
      "source": [
        "> We test the implemented functions on a point $\\boldsymbol{w}_0 = \\boldsymbol{0}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oriented-satellite",
      "metadata": {
        "id": "oriented-satellite"
      },
      "outputs": [],
      "source": [
        "w_0 = np.zeros((6, ))\n",
        "w_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "entertaining-singing",
      "metadata": {
        "id": "entertaining-singing"
      },
      "outputs": [],
      "source": [
        "f_ex_2_3(w_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "residential-attraction",
      "metadata": {
        "id": "residential-attraction"
      },
      "outputs": [],
      "source": [
        "grad_f_ex_2_3(w_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "special-carbon",
      "metadata": {
        "id": "special-carbon"
      },
      "outputs": [],
      "source": [
        "hessian_f_ex_2_3(w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dramatic-establishment",
      "metadata": {
        "id": "dramatic-establishment"
      },
      "source": [
        "6. Implement a Python function for the minimization of a function $f$ using one of the following methods:\n",
        "   * gradient descent, or\n",
        "   * Newton method, or\n",
        "   * BFGS method,\n",
        "\n",
        "   with backtracking line search. Such function should:\n",
        "   * take as input the method to use (`Newton` or `gradient` or `BFGS`), the function $f$, its gradient $\\nabla f$, its hessian $\\nabla^2 f$, the constants $\\alpha$, $c_1$ and $c_2$ of the backtracking algorithm, the tolerance $\\varepsilon$ and $K_{\\max}$ for the stopping criterion and the initial condition $\\boldsymbol{w}_{0}$;\n",
        "   * return as outputs the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$, the corresponding function values $\\{f(\\boldsymbol{w}_k)\\}_k$ and gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$.\n",
        "\n",
        "   Use the following stopping criteria:\n",
        "   * the main stopping criterion is based on the norm of the gradient. Continue the iterations while such norm is above $\\varepsilon$;\n",
        "   * allow a maximum number of iterations $K_{\\max}$ of iterations, after which the method should terminate with a warning.\n",
        " \n",
        "*Solution*:\n",
        "> The implementation is very similar to the one in Exercise 2.2, with the additional cases associated to BFGS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "classified-turtle",
      "metadata": {
        "id": "classified-turtle"
      },
      "outputs": [],
      "source": [
        "def optimization_with_backtracking_line_search_ex_2_3(\n",
        "    method: str, f: typing.Callable, grad_f: typing.Callable, hess_f: typing.Callable, alpha: float,\n",
        "    c_1: float, c_2: float, epsilon: float, maxit: int, w_0: np.ndarray\n",
        ") -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run one of gradient descent, Newton or BFGS method with backtracking line search.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    method : str\n",
        "        optimization method to be used, either \"gradient\", \"Newton\" or \"BFGS\".\n",
        "    f, grad_f, hess_f : Python function\n",
        "        callable evaluating the cost function, its gradient and its hessian, respectively.\n",
        "    alpha : float\n",
        "        initial step length.\n",
        "    c_1, c_2 : float\n",
        "        constants of the backtracking algorithm.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the norm of the gradient of the cost.\n",
        "    maxit : int\n",
        "        maximum number of allowed iterations.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    \"\"\"\n",
        "    assert method in (\"gradient\", \"Newton\", \"BFGS\")\n",
        "\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_f = [f(w_0)]\n",
        "    all_grad_f = [grad_f(w_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Print convergence history\n",
        "    print(\"Iteration\", 0)\n",
        "    print(\"\\t Cost function\", all_f[0])\n",
        "    print(\"\\t Norm of gradient of cost function\", np.linalg.norm(all_grad_f[0]))\n",
        "\n",
        "    # Prepare initial approximation of inverse of the hessian (BFGS only)\n",
        "    if method == \"BFGS\":\n",
        "        inv_hess_f_k = np.linalg.inv(hess_f(w_0))\n",
        "        I = np.eye(w_0.shape[0])\n",
        "\n",
        "    # Use the norm of the gradient as stopping criterion.\n",
        "    while np.linalg.norm(all_grad_f[k]) > epsilon:\n",
        "        w_k = all_w[k]\n",
        "        f_k = all_f[k]\n",
        "        grad_f_k = all_grad_f[k]\n",
        "\n",
        "        # Define the search direction p_k\n",
        "        if method == \"gradient\":\n",
        "            p_k = - grad_f_k\n",
        "            p_k_dot_g_k = - np.linalg.norm(grad_f_k)**2\n",
        "        elif method == \"Newton\":\n",
        "            hess_f_k = hess_f(w_k)\n",
        "            p_k = - np.linalg.solve(hess_f_k, grad_f_k)\n",
        "            p_k_dot_g_k = np.dot(p_k, grad_f_k)\n",
        "        elif method == \"BFGS\":\n",
        "            p_k = - np.dot(inv_hess_f_k, grad_f_k)\n",
        "            p_k_dot_g_k = np.dot(p_k, grad_f_k)\n",
        "\n",
        "        # Carry out a backtracking line search\n",
        "        alpha_k = alpha\n",
        "        while np.isnan(f(w_k + alpha_k * p_k)) or f(w_k + alpha_k * p_k) > f_k + c_1 * alpha_k * p_k_dot_g_k:\n",
        "            alpha_k = c_2 * alpha_k\n",
        "\n",
        "        # Compute w_{k+1}\n",
        "        w_k_plus_1 = w_k + alpha_k * p_k\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f(w_k_plus_1))\n",
        "        all_grad_f.append(grad_f(w_k_plus_1))\n",
        "\n",
        "        # Update approximation of inverse of the hessian (BFGS only)\n",
        "        if method == \"BFGS\":\n",
        "            y_k = all_grad_f[k + 1] - all_grad_f[k]\n",
        "            s_k = all_w[k + 1] - all_w[k]\n",
        "            rho_k = 1 / np.dot(y_k, s_k)\n",
        "            inv_hess_f_k = (\n",
        "                np.dot(np.dot(I - rho_k * np.outer(s_k, y_k), inv_hess_f_k), I - rho_k * np.outer(y_k, s_k))\n",
        "                + rho_k * np.outer(s_k, s_k)\n",
        "            )\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "        # Print convergence history\n",
        "        print(\"Iteration\", k)\n",
        "        print(\"\\t Step length\", alpha_k)\n",
        "        print(\"\\t Cost function\", all_f[k])\n",
        "        print(\"\\t Norm of gradient of cost function\", np.linalg.norm(all_grad_f[k]))\n",
        "\n",
        "        # Bail out if exceeded allowed number of iterations\n",
        "        if k >= maxit:\n",
        "            print(\"WARNING: \" + method + \" method exceeded number of allowed iterations\")\n",
        "            break\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_f), np.array(all_grad_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "compound-acoustic",
      "metadata": {
        "id": "compound-acoustic"
      },
      "source": [
        "7. Choose $c_1 = 0.1$, $c_2 = 0.7$, $\\varepsilon = 10^{-5}$, $\\boldsymbol{w}_0 = \\boldsymbol{0}$. Run gradient, Newton and BFGS methods, and compare their convergence rates.\n",
        "\n",
        "*Solution*:\n",
        "> Note that the text does not say which value of $\\alpha$ to choose. In a typical machine learning scenario, it is up to us to decide the hyperparameters, using our experience on the problem and any insights the theory might offer.\n",
        ">\n",
        "> The step length for gradient method suggested by the theory is $\\alpha = \\frac{1}{L}$. In order to compute the smoothness constant $L$, we have to slightly generalize the formula we have seen in Lecture 0. Indeed, the formula we have seen in the lecture had been derived for the scalar case $x \\in \\mathbb{R}$, while now we have a vector of features $\\boldsymbol{x} \\in \\mathbb{R}^5$. You may follow the same steps we had there, or use directly the expression of the hessian of the empirical risk to see that\n",
        "$$L = \\frac{1}{4 m_{\\text{train}}} \\lambda_{\\max}(\\boldsymbol{X}_{\\text{train}}^T \\boldsymbol{X}_{\\text{train}}).$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "liked-identifier",
      "metadata": {
        "id": "liked-identifier"
      },
      "outputs": [],
      "source": [
        "eigs, _ = np.linalg.eig(np.dot(X_train.T, X_train))\n",
        "L = np.max(eigs) / (4 * X_train.shape[0])\n",
        "print(\"L =\", L)\n",
        "print(\"1 / L =\", 1 / L)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "engaged-dance",
      "metadata": {
        "id": "engaged-dance"
      },
      "source": [
        "> The suggested value of $\\alpha$ is $O(10^{-5})$, which seems like a very small step length. Since we have a backtracking implementation, we may try using a slighlty larger (say, $10^{-4}$) first guess for the step length. If such value is too large the backtracking algorithm will decrease it appropriately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "excellent-fraud",
      "metadata": {
        "id": "excellent-fraud"
      },
      "outputs": [],
      "source": [
        "all_w_gradient, all_f_gradient, all_grad_f_gradient = optimization_with_backtracking_line_search_ex_2_3(\n",
        "    \"gradient\", f_ex_2_3, grad_f_ex_2_3, hessian_f_ex_2_3, 1e-4, 0.1, 0.7, 1e-5, 200, w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acknowledged-drunk",
      "metadata": {
        "id": "acknowledged-drunk"
      },
      "source": [
        "> Notice how:\n",
        "> * the code stopped due to reaching the maximum value of 200 iterations, with gradient norms which are considerably above the required tolerance of $\\varepsilon = 10^{-5}$;\n",
        "> * our larger step length $\\alpha = 10^{-4}$ was always rejected. Actual values of chosen step lengths are between $1 \\cdot 10^{-5}$ and $4 \\cdot 10^{-5}$, which are of the same order of magnitude as the suggested value of $1/L$;\n",
        "> * after a reasonable large decrease of the cost function between iterations 0 and 4, the improvements from iterations 4 to 200 are negligible. The gradient method is showing a very slow sublinear convergence. Indeed, since logistic regression is convex (but not strongly convex), in general we cannot expect a linear convergence. From Lecture 1 we know that the number of iterations required to reach the tolerance $\\varepsilon$ on the gradient norm scales as\n",
        "$$ O\\left(\\frac{L}{\\varepsilon^2}\\right).$$\n",
        "Since $L \\approx 10^{5}$ and $\\varepsilon = 10^{-5}$, the theory warns us that it might take\n",
        "$$ O(10^{15}) $$\n",
        "iterations to convergence to the desired tolerance. No practical application can wait for such a large number of iterations.\n",
        ">\n",
        "> Therefore, we try to use alternative methods, such as Newton and BFGS. We use $\\alpha = 1$ for both, as convergence results suggest to start backtracking from a unit step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "level-array",
      "metadata": {
        "id": "level-array"
      },
      "outputs": [],
      "source": [
        "all_w_newton, all_f_newton, all_grad_f_newton = optimization_with_backtracking_line_search_ex_2_3(\n",
        "    \"Newton\", f_ex_2_3, grad_f_ex_2_3, hessian_f_ex_2_3, 1, 0.1, 0.7, 1e-5, 200, w_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "romantic-stewart",
      "metadata": {
        "id": "romantic-stewart"
      },
      "outputs": [],
      "source": [
        "all_w_bfgs, all_f_bfgs, all_grad_f_bfgs = optimization_with_backtracking_line_search_ex_2_3(\n",
        "    \"BFGS\", f_ex_2_3, grad_f_ex_2_3, hessian_f_ex_2_3, 1, 0.1, 0.7, 1e-5, 200, w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nonprofit-india",
      "metadata": {
        "id": "nonprofit-india"
      },
      "source": [
        "> Both methods have converged very fast, in less than 10 iterations. BFGS uses only a handful of iterations more than Newton, but does not require the evaluation of the hessian matrix (for simplicity we used it to initialize $\\boldsymbol{H}_0$, but in actual implementations this initialization would be done differently) and does not require to solve any linear system, so it might be preferable when dealing with a number of features which is very large (much more than 5 features we have here).\n",
        ">\n",
        "> This can be confirmed by the following plot. Uncomment the line which restricts the range of the horizontal axis for a clearer comparison between Newton and BFGS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "civic-profession",
      "metadata": {
        "id": "civic-profession"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "all_methods = [\"Gradient\", \"Newton\", \"BFGS\"]\n",
        "all_norm_grad_f_method = [\n",
        "    np.linalg.norm(all_grad_f_gradient, axis=1),\n",
        "    np.linalg.norm(all_grad_f_newton, axis=1),\n",
        "    np.linalg.norm(all_grad_f_bfgs, axis=1)\n",
        "]\n",
        "for method_index in range(3):\n",
        "    fig.add_scatter(\n",
        "        x=np.arange(all_norm_grad_f_method[method_index].shape[0]), y=all_norm_grad_f_method[method_index],\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[method_index], size=10),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[method_index], width=2),\n",
        "        mode=\"lines+markers\", name=all_methods[method_index] + \" method\"\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=\"Logistic regression - error on the function value - different methods\",\n",
        "    width=768, height=768, autosize=False\n",
        ")\n",
        "# fig.update_xaxes(range=[0, 10])\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hidden-blackjack",
      "metadata": {
        "id": "hidden-blackjack"
      },
      "source": [
        "> The final weights provided by Newton and BFGS are comparable. (Note: the values obtained here are slightly different from the ones reported on the UCLA website because the ones on the website have been computed using the whole dataset $(\\boldsymbol{X}, \\boldsymbol{y})$, and not just a subset $(\\boldsymbol{X}_{\\text{train}}, \\boldsymbol{y}_{\\text{train}})$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "passive-latex",
      "metadata": {
        "id": "passive-latex"
      },
      "outputs": [],
      "source": [
        "print(all_w_newton[-1])\n",
        "print(all_w_bfgs[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "covered-night",
      "metadata": {
        "id": "covered-night"
      },
      "source": [
        "> We may try to interpret these weights to draw some conclusions on how the features affect the prediction:\n",
        "> * the third, fourth and fifth weights are multiplied by the $r_2$, $r_3$ and $r_4$ dummy variables, and they have a quite clear statistical interpretation. Since $r_2 = r_3 = r_4 = 0$ means that a student comes from a rank 1 university, we will interpret these coefficients using the rank 1 institution as baseline. The third, fourth and fifth weights are negative, meaning that not being from a rank 1 university negatively affects the odds of a student to be admitted (reasonable!). Furthermore, since the coefficients are decreasing when moving from the third to the fifth, the lower the rank of the home institution, the worse are the odds of being admitted. The weight associated to $r_3$ is roughly twice the one associated to $r_2$, while the coefficient associated to $r_4$ is roughly 2.3 times the one to $r_2$;\n",
        "> * the first and second weights are associated to the GRE and GPA respectively. The first weight is O(100) times smaller than the second one. Does this mean that the GPA has a large influence on the odds of getting admitted, while the GRE has negigible influence? Are you sure of this conclusion?\n",
        "\n",
        "8. Using the optimal weights, assess the accuracy of the prediction on the test dataset.\n",
        "\n",
        "*Solution*:\n",
        "> Since this is a classification task (expect labels are admit/don't admit), given a feature vector $\\boldsymbol{x}$ in the test set $\\boldsymbol{X}_{\\text{test}}$, we assign the label ''admit'' if\n",
        "> $\\hat{y}(\\boldsymbol{x}; \\boldsymbol{w}) = \\sigma(\\boldsymbol{s}^T \\boldsymbol{x} + q) > 0.5$\n",
        "> where $\\boldsymbol{w}$ are the optimal weights computed (e.g.) by the Newton method.\n",
        ">\n",
        "> We then compare the predicted values and the actual values (stored in $\\boldsymbol{y}_{\\text{test}}$) by means of the following *table of confusion*\n",
        "> <table>\n",
        "    <tr>\n",
        "        <th></th>\n",
        "        <th>$\\hat{y} \\leq 0.5$</th>\n",
        "        <th>$\\hat{y} > 0.5$</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>$y = 0$</td>\n",
        "        <td><i># of true negative</i></td>\n",
        "        <td># of false positive</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>$y = 1$</td>\n",
        "        <td># of false negative</td>\n",
        "        <td><i># of true positive</i></td>\n",
        "    </tr>\n",
        "> </table>\n",
        "> and the corresponding accuracy index\n",
        "$$\\text{accuracy} = \\frac{\\text{# of true positive} + \\text{# of true negative}}{\\text{# of true positive} + \\text{# of false negative} + \\text{# of false positive} + \\text{# of true negative}}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bacterial-choice",
      "metadata": {
        "id": "bacterial-choice"
      },
      "outputs": [],
      "source": [
        "confusion = np.zeros((2, 2))\n",
        "for (x_j, y_j) in zip(X_test, y_test):\n",
        "    y_hat_j = y_hat(x_j, all_w_newton[-1])\n",
        "    confusion[int(y_j > 0.5), int(y_hat_j > 0.5)] += 1\n",
        "confusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "crucial-permission",
      "metadata": {
        "id": "crucial-permission"
      },
      "outputs": [],
      "source": [
        "accuracy = (confusion[0, 0] + confusion[1, 1]) / (\n",
        "    confusion[0, 0] + confusion[0, 1] + confusion[1, 0] + confusion[1, 1])\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "million-hardware",
      "metadata": {
        "id": "million-hardware"
      },
      "source": [
        "> The trained model handles very well the true negative cases (i.e., students who were not admitted would have indeed been predicted as not admitted), but also has several false negative (i.e., students who were admitted, but the model would have predicted as not admitted). The resulting accuracy on the test set is $80\\%$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "corporate-damages",
      "metadata": {
        "id": "corporate-damages"
      },
      "source": [
        "9. GRE scores are between 220 and 990, while GPA values are between 0 and 4. Repeat the optimization after having normalized the GRE and GPA columns to be between 0 and 1. How does this normalization process affect the convergence of the optimization method and the interpretation of the optimal weights?\n",
        "\n",
        "*Solution*:\n",
        "> We normalize the first two columns of $\\boldsymbol{X}_{\\text{train}}$ and $\\boldsymbol{X}_{\\text{test}}$. Note that we are modifying the matrices `X_train` and `X_test` *in place*, not creating a new copy. Be careful: do not execute the cell below twice (you would be applying the scaling process twice!), and do not re-run cells at point 5 (if you need to do so, make sure to reload the original matrix `X_train` and `X_test` re-running the corresponding cell at point 2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "given-slave",
      "metadata": {
        "id": "given-slave"
      },
      "outputs": [],
      "source": [
        "X_train[:, 0] -= 220\n",
        "X_train[:, 0] /= (990 - 220)\n",
        "X_train[:, 1] /= 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "valid-humanity",
      "metadata": {
        "id": "valid-humanity"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "emotional-current",
      "metadata": {
        "id": "emotional-current"
      },
      "outputs": [],
      "source": [
        "X_test[:, 0] -= 220\n",
        "X_test[:, 0] /= (990 - 220)\n",
        "X_test[:, 1] /= 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "constant-ceramic",
      "metadata": {
        "id": "constant-ceramic"
      },
      "source": [
        "> Since the matrix $\\boldsymbol{X}_{\\text{train}}$ has changed, we need to recompute the smoothness constant $L$ of the empirical risk function. Note how after normalization the smoothness constant is $O(0.1)$, which is considerably lower than the $O(10^{5})$ we had computed without normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pointed-passing",
      "metadata": {
        "id": "pointed-passing"
      },
      "outputs": [],
      "source": [
        "eigs, _ = np.linalg.eig(np.dot(X_train.T, X_train))\n",
        "L = np.max(eigs) / (4 * X_train.shape[0])\n",
        "print(\"L =\", L)\n",
        "print(\"1 / L =\", 1 / L)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "perfect-notebook",
      "metadata": {
        "id": "perfect-notebook"
      },
      "source": [
        "> Having a different value of $L$ also means that the suggested value of step length $\\alpha$ is now bigger when applying the gradient method. We try backtracking from an initial step length of 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lightweight-familiar",
      "metadata": {
        "id": "lightweight-familiar"
      },
      "outputs": [],
      "source": [
        "all_w_gradient, all_f_gradient, all_grad_f_gradient = optimization_with_backtracking_line_search_ex_2_3(\n",
        "    \"gradient\", f_ex_2_3, grad_f_ex_2_3, hessian_f_ex_2_3, 10, 0.1, 0.7, 1e-5, 200, w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "careful-nightmare",
      "metadata": {
        "id": "careful-nightmare"
      },
      "source": [
        "> Execution of the gradient method stopped having exceeded the maximum number of iterations. However, the results at the final iteration are considerably better than the ones we have obtained at step 5 (non normalized dataset). Compare for instance the cost and the norm of the gradient at the final iteration.\n",
        "> This is a concrete machine learning problem that (as we already discussed in Exercise 2.1) shows how first order methods are sensitive to different scales (orders of magnitude) in the optimization variables. This is (part of) the reason why many machine learning algorithms suggest to always normalize/standardize the data!\n",
        ">\n",
        "> Are second order methods affected by variable scaling too?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stock-guyana",
      "metadata": {
        "id": "stock-guyana"
      },
      "outputs": [],
      "source": [
        "all_w_newton, all_f_newton, all_grad_f_newton = optimization_with_backtracking_line_search_ex_2_3(\n",
        "    \"Newton\", f_ex_2_3, grad_f_ex_2_3, hessian_f_ex_2_3, 1, 0.1, 0.7, 1e-5, 200, w_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gorgeous-singapore",
      "metadata": {
        "id": "gorgeous-singapore"
      },
      "outputs": [],
      "source": [
        "all_w_bfgs, all_f_bfgs, all_grad_f_bfgs = optimization_with_backtracking_line_search_ex_2_3(\n",
        "    \"BFGS\", f_ex_2_3, grad_f_ex_2_3, hessian_f_ex_2_3, 1, 0.1, 0.7, 1e-5, 200, w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "spread-measure",
      "metadata": {
        "id": "spread-measure"
      },
      "source": [
        "> Second order methods are either not affected by scaling at all (Newton method) or only very mildly affected by it (BFGS). This is surely another advantage of second order methods compared to first order ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ancient-cabinet",
      "metadata": {
        "id": "ancient-cabinet"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "all_methods = [\"Gradient\", \"Newton\", \"BFGS\"]\n",
        "all_norm_grad_f_method = [\n",
        "    np.linalg.norm(all_grad_f_gradient, axis=1),\n",
        "    np.linalg.norm(all_grad_f_newton, axis=1),\n",
        "    np.linalg.norm(all_grad_f_bfgs, axis=1)\n",
        "]\n",
        "for method_index in range(3):\n",
        "    fig.add_scatter(\n",
        "        x=np.arange(all_norm_grad_f_method[method_index].shape[0]), y=all_norm_grad_f_method[method_index],\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[method_index], size=10),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[method_index], width=2),\n",
        "        mode=\"lines+markers\", name=all_methods[method_index] + \" method\"\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=\"Logistic regression - error on the function value - different methods\",\n",
        "    width=768, height=768, autosize=False\n",
        ")\n",
        "# fig.update_xaxes(range=[0, 10])\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fresh-slovakia",
      "metadata": {
        "id": "fresh-slovakia"
      },
      "source": [
        "> We finally print the optimal weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "handed-engineering",
      "metadata": {
        "id": "handed-engineering"
      },
      "outputs": [],
      "source": [
        "print(all_w_newton[-1])\n",
        "print(all_w_bfgs[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "continuing-balloon",
      "metadata": {
        "id": "continuing-balloon"
      },
      "source": [
        "> Notice how only the first two weights have changed, as we had only scaled the first two variables. Without normalization, it is not fair to compare different weights and conclude whether \"one feature is more important than the other\", because an increase of 1 GRE point (out of 990) is not comparable to an increase of 1 GPA point (out of 4). It is only possible to carry out a fair comparison on normalized features. Based on the first two values of the weights vector, we conclude that the normalized GPA has an influence which is roughly the double of the one of the normalized GRE on the odds of admission to the graduate program.\n",
        ">\n",
        "> Normalization (standardization) has had two beneficial effects:\n",
        "> * from the optimization point of view, it has improved the convergence of the first order method;\n",
        "> * from the statistics/machine learning point of view, it has improved the interpretability of the resulting model.\n",
        "\n",
        "10. Using the optimal weights, assess the accuracy of the prediction on the normalized test dataset.\n",
        "\n",
        "*Solution*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "judicial-amplifier",
      "metadata": {
        "id": "judicial-amplifier"
      },
      "outputs": [],
      "source": [
        "confusion = np.zeros((2, 2))\n",
        "for (x_j, y_j) in zip(X_test, y_test):\n",
        "    y_hat_j = y_hat(x_j, all_w_newton[-1])\n",
        "    confusion[int(y_j > 0.5), int(y_hat_j > 0.5)] += 1\n",
        "confusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "grand-bracelet",
      "metadata": {
        "id": "grand-bracelet"
      },
      "outputs": [],
      "source": [
        "accuracy = (confusion[0, 0] + confusion[1, 1]) / (\n",
        "    confusion[0, 0] + confusion[0, 1] + confusion[1, 0] + confusion[1, 1])\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "municipal-purpose",
      "metadata": {
        "id": "municipal-purpose"
      },
      "source": [
        "> In this case normalization did not make any difference in terms of accuracy. However, in practical cases normalization often helps in obtaining faster convergence of the weights to the optimum (as it happened for us for the gradient method) and, especially when the number of iterations is limited due to comptational resources, normalization often helps in obtaining better models."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}