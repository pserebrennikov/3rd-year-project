{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pserebrennikov/3rd-year-project/blob/master/4_regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tropical-settlement",
      "metadata": {
        "id": "tropical-settlement"
      },
      "source": [
        "# Tutorial 4 - Regularization techniques\n",
        "### Course on Optimization for Machine Learning - Dr. F. Ballarin\n",
        "### Master Degree in Data Analytics for Business, Catholic University of the Sacred Heart, Milano"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sexual-salvation",
      "metadata": {
        "id": "sexual-salvation"
      },
      "source": [
        "In this notebook we discuss regularization techniques for regression problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe5c5fcf",
      "metadata": {
        "id": "fe5c5fcf"
      },
      "outputs": [],
      "source": [
        "import typing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sustained-canada",
      "metadata": {
        "id": "sustained-canada"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import plotly.colors\n",
        "import plotly.graph_objects as go\n",
        "import plotly.subplots"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "reflected-eight",
      "metadata": {
        "id": "reflected-eight"
      },
      "source": [
        "## Exercise 4.1\n",
        "\n",
        "Consider a scalar feature $x$ in the domain domain $I = [-1, 1]$ and the observation $y$ defined as\n",
        "$$y = g(x) + \\xi,$$\n",
        "where the (deterministic) function $g$ is defined as\n",
        "$$g(x) = \\cos(1.5 \\pi x)$$\n",
        "and $\\xi$ is a white Gaussian noise with zero mean and variance $\\sigma^2 = 0.01$.\n",
        "\n",
        "1. Generate a training dataset $\\boldsymbol{x}_{\\text{train}} \\in \\mathbb{R}^{200}$ and a test dataset $\\boldsymbol{x}_{\\text{test}} \\in \\mathbb{R}^{50}$ using features drawn from a uniform distribution on $I$. Generate the corresponding train and test datasets ($\\boldsymbol{y}_{\\text{train}} \\in \\mathbb{R}^{200}$ and $\\boldsymbol{y}_{\\text{test}} \\in \\mathbb{R}^{50}$, respectively) for the observation.\n",
        "\n",
        "*Solution*:\n",
        "> We first define the deterministic function $g(x)$ in a Python function `g(x)`. Note that calling such Python function for a vector of features (e.g., all rows in a dataset) gives a vector of the elementwise evaluation of $g$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shaped-chancellor",
      "metadata": {
        "id": "shaped-chancellor"
      },
      "outputs": [],
      "source": [
        "def g(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Evaluate g(x).\"\"\"\n",
        "    return np.cos(1.5 * np.pi * x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "confirmed-motivation",
      "metadata": {
        "id": "confirmed-motivation"
      },
      "source": [
        "> We set a seed for reproducibility, and generate the required datasets using [`np.random.uniform`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html) and [`np.random.normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sonic-legend",
      "metadata": {
        "id": "sonic-legend"
      },
      "outputs": [],
      "source": [
        "np.random.seed(41)\n",
        "x_train = np.random.uniform(-1, 1, size=30)\n",
        "x_test = np.random.uniform(-1, 1, size=50)\n",
        "y_train = g(x_train) + np.random.normal(0, np.sqrt(0.01), size=30)\n",
        "y_test = g(x_test) + np.random.normal(0, np.sqrt(0.01), size=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "coordinated-isolation",
      "metadata": {
        "id": "coordinated-isolation"
      },
      "source": [
        "> We prepare of plot of the two datasets, on top of the deterministic function $g$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "apparent-transaction",
      "metadata": {
        "id": "apparent-transaction"
      },
      "outputs": [],
      "source": [
        "x_plot = np.linspace(-1, 1, 500)\n",
        "y_plot = g(x_plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bound-article",
      "metadata": {
        "id": "bound-article"
      },
      "outputs": [],
      "source": [
        "fig = plotly.subplots.make_subplots(rows=2, cols=1, vertical_spacing=0.05)\n",
        "fig.add_scatter(\n",
        "    x=x_train, y=y_train,\n",
        "    marker=dict(color=plotly.colors.qualitative.Set1[0], size=6),\n",
        "    mode=\"markers\", name=\"training dataset\",\n",
        "    row=1, col=1\n",
        ")\n",
        "fig.add_scatter(\n",
        "    x=x_test, y=y_test,\n",
        "    marker=dict(color=plotly.colors.qualitative.Set1[1], size=6),\n",
        "    mode=\"markers\", name=\"test dataset\",\n",
        "    row=2, col=1\n",
        ")\n",
        "fig.add_scatter(\n",
        "    x=x_plot, y=y_plot,\n",
        "    line=dict(color=plotly.colors.qualitative.Set1[2], width=2),\n",
        "    mode=\"lines\", name=\"g(x)\",\n",
        "    row=1, col=1\n",
        ")\n",
        "fig.add_scatter(\n",
        "    x=x_plot, y=y_plot,\n",
        "    line=dict(color=plotly.colors.qualitative.Set1[2], width=2),\n",
        "    mode=\"lines\", name=\"g(x)\", showlegend=False,\n",
        "    row=2, col=1\n",
        ")\n",
        "fig.update_layout(\n",
        "    title=\"Training dataset (top) and test dataset (bottom)\",\n",
        "    width=640, height=640, autosize=False\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "interracial-marsh",
      "metadata": {
        "id": "interracial-marsh"
      },
      "source": [
        "2. Implement the evaluation of the empirical risk associated to a polynomial regression via least squares, as well as its gradient.\n",
        "\n",
        "*Solution*:\n",
        "> Similarly to previous exercises on logistic regression, we first start by implementing the prediction function $\\hat{y}(x; \\boldsymbol{w}) = \\sum_{i = 0}^{n - 1} w^{(i)} x^i$ associated to a polynomial regression. Note that the value of $n$, which controlas the maximum polynomial degree $n - 1$, will be specified later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "forward-probe",
      "metadata": {
        "id": "forward-probe"
      },
      "outputs": [],
      "source": [
        "def y_hat(x_j: float, w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate the prediction function associated to a polynomial regression.\"\"\"\n",
        "    return sum(w[i] * x_j**i for i in range(w.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "neural-utilization",
      "metadata": {
        "id": "neural-utilization"
      },
      "source": [
        "> The least squares loss and its derivatives are:\n",
        "$$\\ell(x, y; \\boldsymbol{w}) = \\left(\\sum_{i = 0}^{n - 1} w^{(i)} x^i - y\\right)^2.$$\n",
        "$$\\nabla_\\boldsymbol{w} \\ell(x, y; \\boldsymbol{w}) = 2 \\left(\\sum_{i = 0}^{n - 1} w^{(i)} x^i - y\\right) \\begin{bmatrix}1\\\\x\\\\x^2\\\\\\dots\\\\x^{n-1}\\end{bmatrix}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hungry-vision",
      "metadata": {
        "id": "hungry-vision"
      },
      "outputs": [],
      "source": [
        "def least_squares_loss(x_j: float, y_j: float, w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate the least squares loss.\"\"\"\n",
        "    return (y_hat(x_j, w) - y_j)**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "statewide-disclaimer",
      "metadata": {
        "id": "statewide-disclaimer"
      },
      "outputs": [],
      "source": [
        "def grad_least_squares_loss(x_j: float, y_j: float, w: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Evaluate the gradient of the least squares loss.\"\"\"\n",
        "    powers = np.arange(w.shape[0])\n",
        "    return 2 * (y_hat(x_j, w) - y_j) * x_j**powers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "separate-finding",
      "metadata": {
        "id": "separate-finding"
      },
      "source": [
        "> The empirical risk is the obtained summing over all elements of the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "identified-karen",
      "metadata": {
        "id": "identified-karen"
      },
      "outputs": [],
      "source": [
        "def f_ex_4_1(w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate the empirical risk.\"\"\"\n",
        "    m_train = x_train.shape[0]\n",
        "    return 1 / m_train * sum(least_squares_loss(x_j, y_j, w) for (x_j, y_j) in zip(x_train, y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "numerical-metallic",
      "metadata": {
        "id": "numerical-metallic"
      },
      "outputs": [],
      "source": [
        "def grad_f_ex_4_1(w: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Evaluate the gradient of the empirical risk.\"\"\"\n",
        "    m_train = x_train.shape[0]\n",
        "    return 1 / m_train * sum(grad_least_squares_loss(x_j, y_j, w) for (x_j, y_j) in zip(x_train, y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "alike-effectiveness",
      "metadata": {
        "id": "alike-effectiveness"
      },
      "source": [
        "3. Implement a Python function for the minimization of a function $f$ using BFGS with constant step length. Such function should:\n",
        "   * take as input the function $f$, its gradient $\\nabla f$, the constant step length $\\alpha$, the tolerance $\\varepsilon$ and $K_{\\max}$ for the stopping criterion, the initial conditions $\\boldsymbol{w}_{0}$ and $h_0$;\n",
        "   * return as outputs the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$, the corresponding function values $\\{f(\\boldsymbol{w}_k)\\}_k$ and gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$.\n",
        " \n",
        "   Use the following initialization:\n",
        "   * initial condition $\\boldsymbol{w}_{0}$ for the optimization variable, and\n",
        "   * define the initial approximation of the inverse of the hessian matrix as $\\boldsymbol{H}_0 = h_0 \\boldsymbol{I}$, where $h_0$ is the last input argument.\n",
        " \n",
        "   Use the following stopping criteria:\n",
        "   * the main stopping criterion is based on the norm of the gradient. Continue the iterations while such norm is above $\\varepsilon$;\n",
        "   * allow a maximum number of iterations $K_{\\max}$ of iterations, after which the method should terminate with a warning.\n",
        " \n",
        "*Solution*:\n",
        "> We have already implemented BFGS in Tutorial 2. The main difference is that here (as typically done in machine learning) we require an initialization for $\\boldsymbol{H}_0$, while in Tutorial 2 we assumed it was possible to evaluate the hessian matrix exactly as the initial iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "noted-people",
      "metadata": {
        "id": "noted-people"
      },
      "outputs": [],
      "source": [
        "def bfgs(\n",
        "    f: typing.Callable, grad_f: typing.Callable, alpha: float, epsilon: float, maxit: int, w_0: np.ndarray,\n",
        "    h_0: float\n",
        ") -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run BFGS method with constant step length.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    f, grad_f : Python function\n",
        "        callable evaluating the cost function and its gradient, respectively.\n",
        "    alpha : float\n",
        "        constant step length.\n",
        "    c_1, c_2 : float\n",
        "        constants of the backtracking algorithm.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the norm of the gradient of the cost.\n",
        "    maxit : int\n",
        "        maximum number of allowed iterations.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "    h_0 : float\n",
        "        diagonal scaling factor that defines the initial approximation of the inverse of the hessian matrix.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    \"\"\"\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_f = [f(w_0)]\n",
        "    all_grad_f = [grad_f(w_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Prepare initial approximation of inverse of the hessian: take the identity matrix\n",
        "    I = np.eye(w_0.shape[0])\n",
        "    inv_hess_f_k = h_0 * I\n",
        "\n",
        "    # Use the norm of the gradient as stopping criterion.\n",
        "    while np.linalg.norm(all_grad_f[k]) > epsilon:\n",
        "        w_k = all_w[k]\n",
        "        grad_f_k = all_grad_f[k]\n",
        "\n",
        "        # Compute w_{k+1}\n",
        "        w_k_plus_1 = w_k - alpha * np.dot(inv_hess_f_k, grad_f_k)\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f(w_k_plus_1))\n",
        "        all_grad_f.append(grad_f(w_k_plus_1))\n",
        "\n",
        "        # Update approximation of inverse of the hessian\n",
        "        y_k = all_grad_f[k + 1] - all_grad_f[k]\n",
        "        s_k = all_w[k + 1] - all_w[k]\n",
        "        rho_k = 1 / np.dot(y_k, s_k)\n",
        "        inv_hess_f_k = (\n",
        "            np.dot(np.dot(I - rho_k * np.outer(s_k, y_k), inv_hess_f_k), I - rho_k * np.outer(y_k, s_k))\n",
        "            + rho_k * np.outer(s_k, s_k)\n",
        "        )\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "        # Bail out if exceeded allowed number of iterations\n",
        "        if k >= maxit:\n",
        "            print(\"WARNING: BFGS method exceeded number of allowed iterations\")\n",
        "            break\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_f), np.array(all_grad_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "close-nation",
      "metadata": {
        "id": "close-nation"
      },
      "source": [
        "4. Choose $\\varepsilon = 10^{-8}$, $\\boldsymbol{w}_0 = \\boldsymbol{0}$ and $h_0 = 1$. Obtain the best fit polynomial for $n$ between 1 and 25 by running BFGS. Comment on the quality of the fit when $n$ increases.\n",
        "\n",
        "*Solution*:\n",
        "> We run the `bfgs` function for all required values of $n$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dramatic-dryer",
      "metadata": {
        "id": "dramatic-dryer"
      },
      "outputs": [],
      "source": [
        "all_w = dict()\n",
        "all_f = dict()\n",
        "all_grad_f = dict()\n",
        "for n in range(1, 26):\n",
        "    all_w[n], all_f[n], all_grad_f[n] = bfgs(f_ex_4_1, grad_f_ex_4_1, 1, 1e-8, 1000, np.zeros(n), 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "changed-cocktail",
      "metadata": {
        "id": "changed-cocktail"
      },
      "source": [
        "> We next create the following plot to get a graphical intuition of the concepts of underfitting and overfitting. The plot has two rows, corresponding to the training set (top) and the test set (bottom), and four columns, each column corresponding to a value of $n \\in \\{3, 7, 13, 25\\}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "alleged-liberia",
      "metadata": {
        "id": "alleged-liberia"
      },
      "outputs": [],
      "source": [
        "n_plot = [3, 7, 13, 25]\n",
        "fig = plotly.subplots.make_subplots(rows=2, cols=4, vertical_spacing=0.05, horizontal_spacing=0.05)\n",
        "for (col, n) in enumerate(n_plot):\n",
        "    fig.add_scatter(\n",
        "        x=x_train, y=y_train,\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[0], size=6),\n",
        "        mode=\"markers\", name=\"training dataset\",\n",
        "        row=1, col=col + 1, showlegend=(col == 0)\n",
        "    )\n",
        "    fig.add_scatter(\n",
        "        x=x_test, y=y_test,\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[1], size=6),\n",
        "        mode=\"markers\", name=\"test dataset\",\n",
        "        row=2, col=col + 1, showlegend=(col == 0)\n",
        "    )\n",
        "    fig.add_scatter(\n",
        "        x=x_plot, y=y_hat(x_plot, all_w[n][-1]),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[6], width=2),\n",
        "        mode=\"lines\", name=\"y_hat(x)\",\n",
        "        row=1, col=col + 1, showlegend=(col == 0)\n",
        "    )\n",
        "    fig.add_scatter(\n",
        "        x=x_plot, y=y_hat(x_plot, all_w[n][-1]),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[6], width=2),\n",
        "        mode=\"lines\", name=\"y_hat(x)\",\n",
        "        row=2, col=col + 1, showlegend=False\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=(\n",
        "        \"Training dataset (top) and test dataset (bottom). \"\n",
        "        + \"Increasing n = \" + str(n_plot) + \" (left to right)\"\n",
        "    ), width=2 * 512, height=512, autosize=False\n",
        ")\n",
        "# fig.update_yaxes(range=[-1.2, 1.2])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "english-salem",
      "metadata": {
        "id": "english-salem"
      },
      "source": [
        "> * From the first column we see that the generated model with $n = 3$ is underfitting the data: the prediction curve is not able to describe especially the peak near $x = 0$ (which is underestimated by the prediction), and at least one of the regions with low values (either $x \\approx \\pm 2/3$.\n",
        "> * The second column seems to represent a nice fit. The overall pattern of the data is well reproduced both the training and the test datasets.\n",
        "> * The third and fourth columns show increasing overfitting. Uncomment the code line that restricts the vertical axis to realize that the polynomial is trying to pass through as many data point as possible (especially close to the left and right boundaries of the domain $I$), but this is causing oscillations of larger and larger magnitude when $n$ increases. Since these oscillations try to match the training dataset, but *cannot* match the unseen test dataset, they will lead in general to worse predictions.\n",
        ">\n",
        "> A more precise quantification of whether a model is underfitting or overfitting can be obtained introducing the concept of RMSE (root mean square error), defined as\n",
        "$$\\text{RMSE}(\\boldsymbol{x}, \\boldsymbol{y}) = \\sqrt{\\frac{1}{m} \\sum_{j = 1}^{m} (\\hat{y}(x_j) - y_j)^2}.$$\n",
        "> Upon computing the RMSE on both train and test datasets, the following cases may occur:\n",
        "> * $\\text{RMSE}(\\boldsymbol{x}_{\\text{train}}, \\boldsymbol{y}_{\\text{train}}) \\gg 0$ and $\\text{RMSE}(\\boldsymbol{x}_{\\text{test}}, \\boldsymbol{y}_{\\text{test}}) \\gg 0$: this is a case of underfitting, where the model is either not trained properly or is not rich enough to describe the phenomenon that generated the data. In this exercise, underfitting happens when $n < 5$\n",
        "> * $\\text{RMSE}(\\boldsymbol{x}_{\\text{train}}, \\boldsymbol{y}_{\\text{train}}) \\approx 0$ and $\\text{RMSE}(\\boldsymbol{x}_{\\text{test}}, \\boldsymbol{y}_{\\text{test}}) \\approx 0$: this is the desired case, in which a model correctly describes the data both in the training and in the test sets. In this exercise, this corresponds to $5 \\leq n \\leq 8$\n",
        "> * $\\text{RMSE}(\\boldsymbol{x}_{\\text{train}}, \\boldsymbol{y}_{\\text{train}}) \\approx 0$ and $\\text{RMSE}(\\boldsymbol{x}_{\\text{test}}, \\boldsymbol{y}_{\\text{test}}) \\gg 0$: this is the case of overfitting, in which the model has memorized the training data very well, but has poor generalization capabilities when provided with unseen test data. In this exercise, this corresponds to $n > 8$.\n",
        "> * (the remaining case, $\\text{RMSE}(\\boldsymbol{x}_{\\text{train}}, \\boldsymbol{y}_{\\text{train}}) \\gg 0$ and $\\text{RMSE}(\\boldsymbol{x}_{\\text{test}}, \\boldsymbol{y}_{\\text{test}}) \\approx 0$ never occurs in practice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "according-swaziland",
      "metadata": {
        "id": "according-swaziland"
      },
      "outputs": [],
      "source": [
        "RMSE_train = np.array([\n",
        "    np.sqrt(1 / x_train.shape[0] * np.sum((y_hat(x_train, all_w[n][-1]) - y_train)**2))\n",
        "    for n in range(1, 26)\n",
        "])\n",
        "RMSE_test = np.array([\n",
        "    np.sqrt(1 / x_test.shape[0] * np.sum((y_hat(x_test, all_w[n][-1]) - y_test)**2))\n",
        "    for n in range(1, 26)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "secret-devices",
      "metadata": {
        "id": "secret-devices"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "fig.add_scatter(x=np.arange(RMSE_train.shape[0]) + 1, y=RMSE_train, name=\"Training set\")\n",
        "fig.add_scatter(x=np.arange(RMSE_test.shape[0]) + 1, y=RMSE_test, name=\"Test set\")\n",
        "fig.update_layout(title=\"RMSE\")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "alone-restriction",
      "metadata": {
        "id": "alone-restriction"
      },
      "source": [
        "> It is clear that underfitting is quite simple to detect while training the model, while overfitting can be detected only by querying data that are yet to be seen. There are validation techniques available, but often require putting apart some of the training data into a validation dataset. In alternative, is there a way to *change* the optimization problem to prevent overfitting?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "satisfactory-demand",
      "metadata": {
        "id": "satisfactory-demand"
      },
      "source": [
        "## Exercise 4.2\n",
        "\n",
        "The US National Centers for Environmental Information has collected weather data at the Raleigh Durham International Airport. The dataset contains a long list of fields, starting from the date\n",
        "* `date`: the date the following observations were collected,\n",
        "\n",
        "and associated numerical observations:\n",
        "* `temperaturemin`: the minimum temperature (in Fahrenheit),\n",
        "* `temperaturemax`: the maximum temperature (in Fahrenheit),\n",
        "* `precipitation`: the total amount of precipitation (in inches),\n",
        "* `snowfall`: the total amount of snow precipitation (in inches),\n",
        "* `snowdepth`: the maximum depth of snow (in inches),\n",
        "* `avgwindspeed`: the average wind speed (in miles per hour),\n",
        "* `fastest2minwinddir`: the direction of fastest 2-minute wind (in degrees),\n",
        "* `fastest2minwindspeed`: the fastest 2-minute wind speed (in miles per hour),\n",
        "* `fastest5secwinddir`: the direction of fastest 5-second wind (in degrees),\n",
        "* `fastest5secwindspeed`: the fastest 5-second wind speed (in miles per hour),\n",
        "\n",
        "or boolean observations:\n",
        "* `fog`: the text `Present` is used if there has been fog during the day; if blank, it means that no fog was registered during that day,\n",
        "* `fogheavy`: the text `Present` is used if there has been heavy fog during the day; if blank, it means fog was either not registered during that day, or was not classified as heavy,\n",
        "* `mist`: the text `Present` is used if there has been mist during the day; if blank, it means that no mist was registered during that day,\n",
        "* `rain`: the text `Present` is used if there has been rain during the day; if blank, it means that no rain was registered during that day,\n",
        "* `fogground`: the text `Present` is used if there has been fog at ground level during the day; if blank, it means fog was either not registered during that day, or was not at ground level,\n",
        "* `ice`: the text `Present` is used if there have been ice pellets, sleet, snow pellets, or small hail during the day; if blank, it means that no ice was registered during that day,\n",
        "* `glaze`: the text `Present` is used if there have been glaze or rime during the day; if blank, it means that no glaze was registered during that day,\n",
        "* `drizzle`: the text `Present` is used if there has been drizzle during the day; if blank, it means that no drizzle was registered during that day,\n",
        "* `snow`:  the text `Present` is used if there have been snow, snow pellets, snow grains, or ice crystals during the day; if blank, it means that no snow was registered during that day,\n",
        "* `freezingrain`: the text `Present` is used if there has been freezing rain during the day; if blank, it means that either no rain was registered during that day, or it was not classified as freezing rain,\n",
        "* `smokehaze`: the text `Present` is used if there has been smoke during the day; if blank, it means that no smoke was registered during that day,\n",
        "* `thunder`: the text `Present` is used if there have been thunders during the day; if blank, it means that no thunder was registered during that day,\n",
        "* `highwind`: the text `Present` is used if there has been high or damaging wind during the day; if blank, it means that either no wind was registered during that day, or any registered wind was not particularly high,\n",
        "* `hail`: the text `Present` is used if there has been hail during the day; if blank, it means that no hail was registered during that day,\n",
        "* `blowingsnow`: the text `Present` is used if there has been blowing or drifting snow during the day; if blank, it means that either no snow was registered during that day, or any registered snow was not classified as blowing,\n",
        "* `dust`: the text `Present` is used if there have been dust, volcanic ash, blowing dust or blowing sand during the day; if blank, it means that no dust was registered during that day,\n",
        "* `freezingfog`: the text `Present` is used if there has been freezing fog during the day; if blank, it means fog was either not registered during that day, or was not classified as freezing.\n",
        "\n",
        "The goal of this exercise is to predict the minimum temperature of any given day as a function of the remaining columns of the dataset.\n",
        "\n",
        "1. Load the dataset of historical data from [its website](`https://data.townofcary.org/explore/dataset/rdu-weather-history/information/`)\n",
        "\n",
        "*Solution*:\n",
        "> We use `pandas` to load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "essential-bibliography",
      "metadata": {
        "id": "essential-bibliography"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e923c59",
      "metadata": {
        "id": "8e923c59"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "polyphonic-facility",
      "metadata": {
        "id": "polyphonic-facility"
      },
      "outputs": [],
      "source": [
        "if os.path.isfile(\"data/rdu-weather-history.csv\"):\n",
        "    csv_path = \"data/rdu-weather-history.csv\"\n",
        "else:\n",
        "    # csv_path = \"https://data.townofcary.org/explore/dataset/rdu-weather-history/download/?format=csv\"\n",
        "    csv_path = (\n",
        "        \"https://dmf.unicatt.it/~fball/public/optimization_for_machine_learning\"\n",
        "        + \"/rdu-weather-history.csv\"\n",
        "    )\n",
        "df = pd.read_csv(csv_path, parse_dates=[\"date\"], sep=\";\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dental-georgia",
      "metadata": {
        "id": "dental-georgia"
      },
      "source": [
        "> We see that the dataset has thousands of rows and 28 columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "assumed-hierarchy",
      "metadata": {
        "id": "assumed-hierarchy"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "perceived-passenger",
      "metadata": {
        "id": "perceived-passenger"
      },
      "source": [
        "> The boolean columns contain either the string `Present`, or `NaN`. We replace them, respectively, with 1 and 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "southern-eight",
      "metadata": {
        "id": "southern-eight"
      },
      "outputs": [],
      "source": [
        "bool_cols = [\n",
        "    \"fog\", \"fogheavy\", \"mist\", \"rain\", \"fogground\", \"ice\", \"glaze\", \"drizzle\", \"snow\", \"freezingrain\",\n",
        "    \"smokehaze\", \"thunder\", \"highwind\", \"hail\", \"blowingsnow\", \"dust\", \"freezingfog\"\n",
        "]\n",
        "for bool_col in bool_cols:\n",
        "    df[bool_col].fillna(0.0, inplace=True)\n",
        "    df[bool_col].replace(\"Present\", 1.0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "growing-filling",
      "metadata": {
        "id": "growing-filling"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "indie-garlic",
      "metadata": {
        "id": "indie-garlic"
      },
      "source": [
        "> However, there are also columns that should contain real numbers but which have missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ideal-remove",
      "metadata": {
        "id": "ideal-remove"
      },
      "outputs": [],
      "source": [
        "df.columns[df.isna().any()].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "anonymous-annual",
      "metadata": {
        "id": "anonymous-annual"
      },
      "source": [
        "> Printing the number of non-missing entries, it seems that the columns `fastest5secwinddir` and `fastest5secwindspeed` are not available at all in the dataset, so we drop them. Furthermore, only very few rows have missing values for `snowfall` and `snowdepth`, which we assume to be zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pursuant-grave",
      "metadata": {
        "id": "pursuant-grave"
      },
      "outputs": [],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "crude-mountain",
      "metadata": {
        "id": "crude-mountain"
      },
      "outputs": [],
      "source": [
        "df.drop(\"fastest5secwinddir\", axis=1, inplace=True)\n",
        "df.drop(\"fastest5secwindspeed\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "honest-newport",
      "metadata": {
        "id": "honest-newport"
      },
      "outputs": [],
      "source": [
        "missing_cols = [\"snowfall\", \"snowdepth\"]\n",
        "for missing_col in missing_cols:\n",
        "    df[missing_col].fillna(0.0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "freelance-description",
      "metadata": {
        "id": "freelance-description"
      },
      "outputs": [],
      "source": [
        "assert len(df.columns[df.isna().any()].tolist()) == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "advised-hypothesis",
      "metadata": {
        "id": "advised-hypothesis"
      },
      "source": [
        "> The main statistics are reported below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "incredible-acting",
      "metadata": {
        "id": "incredible-acting"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "collectible-reference",
      "metadata": {
        "id": "collectible-reference"
      },
      "source": [
        "2. In order to transform the date field in a categorical field, add a new column which stores the season (winter, spring, summer, fall) associated to the date of the row, and drop the date column from the dataset.\n",
        "\n",
        "*Solution*:\n",
        "> A simple way to determine the season from a date is to combine the month and the day, in that order, into a single number (e.g., March 12 into 0312 = 312). We then assign a category from 0 to 3, corresponding to each season. The name of the season (at least in the northern hemisphere) is provided as comment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "induced-blackjack",
      "metadata": {
        "id": "induced-blackjack"
      },
      "outputs": [],
      "source": [
        "season_col = []\n",
        "for date in df[\"date\"]:\n",
        "    if int(date.strftime(\"%m%d\")) >= 1221 or int(date.strftime(\"%m%d\")) <= 319:\n",
        "        season = 0  # winter\n",
        "    elif int(date.strftime(\"%m%d\")) >= 320 and int(date.strftime(\"%m%d\")) <= 620:\n",
        "        season = 1  # spring\n",
        "    elif int(date.strftime(\"%m%d\")) >= 621 and int(date.strftime(\"%m%d\")) <= 921:\n",
        "        season = 2  # summer\n",
        "    elif int(date.strftime(\"%m%d\")) >= 922 and int(date.strftime(\"%m%d\")) <= 1220:\n",
        "        season = 3  # fall\n",
        "    season_col.append(season)\n",
        "\n",
        "df[\"season\"] = season_col\n",
        "df.drop(\"date\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "optional-testimony",
      "metadata": {
        "id": "optional-testimony"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "distant-franchise",
      "metadata": {
        "id": "distant-franchise"
      },
      "source": [
        "> We then replace the categorical column with dummy variables, as we already did in tutorial 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "resistant-register",
      "metadata": {
        "id": "resistant-register"
      },
      "outputs": [],
      "source": [
        "dummy_seasons = pd.get_dummies(df[\"season\"], prefix=\"season\")\n",
        "dummy_seasons.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "committed-concrete",
      "metadata": {
        "id": "committed-concrete"
      },
      "source": [
        "> We exclude the first dummy, and collect the remaining column in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "laughing-institute",
      "metadata": {
        "id": "laughing-institute"
      },
      "outputs": [],
      "source": [
        "dummy_seasons.drop(\"season_0\", axis=1, inplace=True)\n",
        "df.drop(\"season\", axis=1, inplace=True)\n",
        "df = pd.concat([df, dummy_seasons], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "worst-warrant",
      "metadata": {
        "id": "worst-warrant"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "joined-champagne",
      "metadata": {
        "id": "joined-champagne"
      },
      "source": [
        "3. After normalizing the dataset, define the `temperaturemin` column as the observation $\\boldsymbol{y}$, and the remaining columns as features $\\boldsymbol{X}$.\n",
        "\n",
        "   Separate the dataset $(\\boldsymbol{X}, \\boldsymbol{y})$, composed of $m$ rows, in:\n",
        "    1. a training dataset $(\\boldsymbol{X}_{\\text{train}}, \\boldsymbol{y}_{\\text{train}})$, composed of $m_{\\text{train}} = 0.8 m$ randomly selected rows, and\n",
        "    2. a test dataset $(\\boldsymbol{X}_{\\text{test}}, \\boldsymbol{y}_{\\text{test}})$, composed of the remaining $m_{\\text{test}} = 0.2 m$ rows.\n",
        "    \n",
        "*Solution*:\n",
        "> Normalization can be obtained in `pandas` as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hollywood-affair",
      "metadata": {
        "id": "hollywood-affair"
      },
      "outputs": [],
      "source": [
        "df = (df - df.min()) / (df.max() - df.min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "naughty-billy",
      "metadata": {
        "id": "naughty-billy"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pediatric-advancement",
      "metadata": {
        "id": "pediatric-advancement"
      },
      "source": [
        "> We then split the dataset as requested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "convinced-builder",
      "metadata": {
        "id": "convinced-builder"
      },
      "outputs": [],
      "source": [
        "y = df[[\"temperaturemin\"]].to_numpy().reshape(-1)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prescription-tracker",
      "metadata": {
        "id": "prescription-tracker"
      },
      "outputs": [],
      "source": [
        "X = df.drop(\"temperaturemin\", axis=1).to_numpy()\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "accomplished-device",
      "metadata": {
        "id": "accomplished-device"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "incredible-lyric",
      "metadata": {
        "id": "incredible-lyric"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42 + 300)\n",
        "perm = np.random.permutation(y.shape[0])\n",
        "perm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bright-description",
      "metadata": {
        "id": "bright-description"
      },
      "outputs": [],
      "source": [
        "m_train = int(0.8 * perm.shape[0])\n",
        "y_train = y[perm[:m_train]]\n",
        "X_train = X[perm[:m_train]]\n",
        "y_test = y[perm[m_train:]]\n",
        "X_test = X[perm[m_train:]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "standing-sheet",
      "metadata": {
        "id": "standing-sheet"
      },
      "source": [
        "4. Implement the evaluation of the empirical risk associated to a linear regression via least squares with multiple features, as well as its gradient.\n",
        "\n",
        "*Solution*:\n",
        "> This is very similar to the previous exercise, except that now we have a vector of features. Since here we have 28 features, than $n = 29$ (28 features + 1 bias) The prediction function is $\\hat{y}(\\boldsymbol{x}; \\boldsymbol{w}) = \\sum_{i = 1}^{n - 1} w^{(i)} x^{(i)} + w^{(n)}$, where the first $n - 1$ weights are associated to the corresponding feature, while the last weight is a bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "demanding-paint",
      "metadata": {
        "id": "demanding-paint"
      },
      "outputs": [],
      "source": [
        "def y_hat(x_j: np.ndarray, w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate the prediction function associated to a linear regression.\"\"\"\n",
        "    return np.dot(w[:-1], x_j) + w[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "little-dealer",
      "metadata": {
        "id": "little-dealer"
      },
      "source": [
        "> The least squares loss and its derivatives are:\n",
        "$$\\ell(x, y; \\boldsymbol{w}) = \\left(\\sum_{i = 1}^{n - 1} w^{(i)} x^{(i)} + w^{(n)} - y\\right)^2.$$\n",
        "$$\\nabla_\\boldsymbol{w} \\ell(x, y; \\boldsymbol{w}) = 2 \\left(\\sum_{i = 1}^{n - 1} w^{(i)} x^{(i)} + w^{(n)} - y\\right) \\begin{bmatrix}\\boldsymbol{x}\\\\1\\end{bmatrix}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "collected-teddy",
      "metadata": {
        "id": "collected-teddy"
      },
      "outputs": [],
      "source": [
        "def least_squares_loss(x_j: np.ndarray, y_j: float, w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate the least squares loss.\"\"\"\n",
        "    return (y_hat(x_j, w) - y_j)**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "destroyed-vienna",
      "metadata": {
        "id": "destroyed-vienna"
      },
      "outputs": [],
      "source": [
        "def grad_least_squares_loss(x_j: np.ndarray, y_j: float, w: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Evaluate the gradient of the least squares loss.\"\"\"\n",
        "    vec = np.zeros(w.shape[0])\n",
        "    vec[:-1] = x_j\n",
        "    vec[-1] = 1\n",
        "    return 2 * (y_hat(x_j, w) - y_j) * vec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "meaningful-nothing",
      "metadata": {
        "id": "meaningful-nothing"
      },
      "source": [
        "> The empirical risk is the obtained summing over all elements of the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "foreign-prior",
      "metadata": {
        "id": "foreign-prior"
      },
      "outputs": [],
      "source": [
        "def f_ex_4_2(w: np.ndarray, addend: int = None) -> float:\n",
        "    \"\"\"\n",
        "    Evaluate the empirical risk (if addend is None).\n",
        "\n",
        "    For use within a stochastic method, the optional parameter addend may take an integer value.\n",
        "    In such case, the loss associated to the addend-th element is computed instead.\n",
        "    \"\"\"\n",
        "    if addend is None:\n",
        "        m_train = X_train.shape[0]\n",
        "        return 1 / m_train * sum(least_squares_loss(x_j, y_j, w) for (x_j, y_j) in zip(X_train, y_train))\n",
        "    else:\n",
        "        return least_squares_loss(X_train[addend], y_train[addend], w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "threaded-personal",
      "metadata": {
        "id": "threaded-personal"
      },
      "outputs": [],
      "source": [
        "def grad_f_ex_4_2(w: np.ndarray, addend: int = None) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Evaluate the gradient of the empirical risk (if addend is None).\n",
        "\n",
        "    For use within a stochastic method, the optional parameter addend may take an integer value.\n",
        "    In such case, the gradient loss associated to the addend-th element is computed instead.\n",
        "    \"\"\"\n",
        "    if addend is None:\n",
        "        m_train = X_train.shape[0]\n",
        "        return 1 / m_train * sum(grad_least_squares_loss(x_j, y_j, w) for (x_j, y_j) in zip(X_train, y_train))\n",
        "    else:\n",
        "        return grad_least_squares_loss(X_train[addend], y_train[addend], w)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "confidential-gallery",
      "metadata": {
        "id": "confidential-gallery"
      },
      "source": [
        "5. Train the linear regression model using a mini-batch stochastic gradient descent, with mini-batch size equal to $10\\%, 20\\%, 50\\%$ or $100\\%$ of the training set cardinality.\n",
        "\n",
        "*Solution*:\n",
        "> We first copy the implementation of the minibatch stochastic gradient from the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "conservative-leader",
      "metadata": {
        "id": "conservative-leader"
      },
      "outputs": [],
      "source": [
        "def mini_batch_stochastic_gradient(\n",
        "    m: int, m_b: int, f: typing.Callable, grad_f: typing.Callable, alpha: float, epsilon: float,\n",
        "    maxit: int, w_0: np.ndarray\n",
        ") -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run the mini-batch stochastic gradient method with constant step length.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    m : int\n",
        "        number of addends in the expression of the cost function.\n",
        "    m_b : int\n",
        "        size of the mini-batch.\n",
        "    f, grad_f : Python function\n",
        "        callable evaluating the cost function and its gradient, respectively.\n",
        "    alpha : float\n",
        "        constant step length.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the norm of the gradient of the cost.\n",
        "    maxit : int\n",
        "        maximum number of allowed iterations.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    \"\"\"\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_f = [f(w_0)]\n",
        "    all_grad_f = [grad_f(w_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Use the norm of the gradient as stopping criterion.\n",
        "    while np.linalg.norm(all_grad_f[k]) > epsilon:\n",
        "        w_k = all_w[k]\n",
        "\n",
        "        # Draw random indices\n",
        "        J_k = np.random.choice(m, size=m_b, replace=False)\n",
        "\n",
        "        # Compute the update direction\n",
        "        g_k = - 1 / m_b * sum([grad_f(w_k, addend=j) for j in J_k])\n",
        "\n",
        "        # Compute w_{k + 1}\n",
        "        w_k_plus_1 = w_k + alpha * g_k\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f(w_k_plus_1))\n",
        "        all_grad_f.append(grad_f(w_k_plus_1))\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "        # Bail out if exceeded allowed number of iterations\n",
        "        if k >= maxit:\n",
        "            print(\"WARNING: stochastic gradient method exceeded number of allowed iterations\")\n",
        "            break\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_f), np.array(all_grad_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ruled-testimony",
      "metadata": {
        "id": "ruled-testimony"
      },
      "source": [
        "> We compute the smoothness constant $L$ in order to determine an appropriate step size $1/L$, defined for a linear regression problem as\n",
        "$$L = 2 \\frac{\\lambda_{\\max}(\\boldsymbol{X}_{\\text{train}}^T \\boldsymbol{X}_{\\text{train}})}{m_{\\text{train}}}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "coral-present",
      "metadata": {
        "id": "coral-present"
      },
      "outputs": [],
      "source": [
        "eigs, _ = np.linalg.eig(np.dot(X_train.T, X_train))\n",
        "L = 2 * np.max(eigs) / m_train\n",
        "print(\"L =\", L)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gorgeous-regulation",
      "metadata": {
        "id": "gorgeous-regulation"
      },
      "source": [
        "> We then train the model for different mini-batch sizes. We print the total number of iterations $K$, but also the number of epochs, defined as\n",
        "$$E = K \\frac{m_b}{m}$$\n",
        "and the elapsed time. Notice how training with a small mini-batch size takes a very similar number of iterations than training with the whole gradient. However, since the dataset is composed of thousands of rows, a mini-batch iterations is far less expensive than one interation of the gradient method, thus mini-batch methods (even though stochastic) may even be faster than the deterministic gradient method! This is the first example we see that shows how efficient stochastic methods are on real datasets, which is part of the reason why they are so successful in machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unlimited-browser",
      "metadata": {
        "id": "unlimited-browser"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "settled-substance",
      "metadata": {
        "id": "settled-substance"
      },
      "outputs": [],
      "source": [
        "all_w = dict()\n",
        "all_f = dict()\n",
        "all_grad_f = dict()\n",
        "\n",
        "np.random.seed(42 + 500)\n",
        "\n",
        "all_perc = [10, 20, 50, 100]\n",
        "for perc in all_perc:\n",
        "    m_b = int(perc / 100 * m_train)\n",
        "    start = time.time()\n",
        "    all_w[perc], all_f[perc], all_grad_f[perc] = mini_batch_stochastic_gradient(\n",
        "        m_train, m_b, f_ex_4_2, grad_f_ex_4_2, 1 / L, 1e-2, 200, np.zeros(X.shape[1] + 1))\n",
        "    end = time.time()\n",
        "    K = all_w[perc].shape[0]\n",
        "    E = int(np.ceil(K * m_b / m_train))\n",
        "    print(\n",
        "        \"m_b =\", m_b, \"out of\", m_train, \"converged in\", K, \"iterations,\",\n",
        "        E, \"epochs and\", end - start, \"seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "distinguished-blame",
      "metadata": {
        "id": "distinguished-blame"
      },
      "source": [
        "6. Evaluate the accuracy of the prediction, and determine which are the most relevant features while carrying out the prediction task.\n",
        "\n",
        "*Solution*:\n",
        "> We evaluate the root mean squared error (RMSE) for training and test set, for all configurations of mini-batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "environmental-conspiracy",
      "metadata": {
        "id": "environmental-conspiracy"
      },
      "outputs": [],
      "source": [
        "for perc in all_perc:\n",
        "    RMSE_train = np.sqrt(\n",
        "        1 / X_train.shape[0]\n",
        "        * sum((y_hat(x_j, all_w[perc][-1]) - y_j)**2 for (x_j, y_j) in zip(X_train, y_train)))\n",
        "    RMSE_test = np.sqrt(\n",
        "        1 / X_test.shape[0]\n",
        "        * sum((y_hat(x_j, all_w[perc][-1]) - y_j)**2 for (x_j, y_j) in zip(X_test, y_test)))\n",
        "    m_b = int(perc / 100 * m_train)\n",
        "    print(\"m_b =\", m_b, \"out of\", m_train, \"RMSE_train =\", RMSE_train, \"RMSE_test =\", RMSE_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "graduate-portuguese",
      "metadata": {
        "id": "graduate-portuguese"
      },
      "source": [
        "> We notice that in all cases the RMSE is of the of the order of $10\\%$. For our goals this prediction is reasonably accurate. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "collective-realtor",
      "metadata": {
        "id": "collective-realtor"
      },
      "source": [
        "> In order to determine which are the most relevant features, we look at the largest entries among the weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "proof-transparency",
      "metadata": {
        "id": "proof-transparency"
      },
      "outputs": [],
      "source": [
        "all_w[100][-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "apart-constant",
      "metadata": {
        "id": "apart-constant"
      },
      "source": [
        "> We notice that there are a few coefficients which are of $O(10^{-1})$, and many that are of lower order of magnitude. In particular"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "liked-detector",
      "metadata": {
        "id": "liked-detector"
      },
      "outputs": [],
      "source": [
        "important_features = np.argwhere(np.abs(all_w[100][-1]) > 0.1)\n",
        "important_features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "introductory-reply",
      "metadata": {
        "id": "introductory-reply"
      },
      "source": [
        "> To get the corresponding feature names, we query the `pandas` dataframe. Notice how we should drop the last entry from `important_features`, because it is the coefficient associated to the bias (and not to a feature)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "irish-contract",
      "metadata": {
        "id": "irish-contract"
      },
      "outputs": [],
      "source": [
        "[df.drop(\"temperaturemin\", axis=1).columns[f][0] for f in important_features[:-1]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "linear-fiber",
      "metadata": {
        "id": "linear-fiber"
      },
      "source": [
        "> We could have probably guessed that these were the important features just by looking at the Pearson correlation coefficient in the dataset. Notice how all the features selected as important are at the very top of the list of such correlation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "departmental-ethnic",
      "metadata": {
        "id": "departmental-ethnic"
      },
      "outputs": [],
      "source": [
        "correlation = df.corr()[\"temperaturemin\"]\n",
        "correlation.sort_values(ascending=False, key=abs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "warming-single",
      "metadata": {
        "id": "warming-single"
      },
      "source": [
        "> Is there a way to change the optimization problem used during training so that it automatically discards features (i.e., sets a zero weight in front of them) which are not relevant in the prediction of the output?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "related-novel",
      "metadata": {
        "id": "related-novel"
      },
      "source": [
        "## Exercise 4.3 (continuation of Exercise 4.1)\n",
        "\n",
        "Consider a scalar feature $x$ in the domain domain $I = [-1, 1]$ and the observation $y$ defined as\n",
        "$$y = g(x) + \\xi,$$\n",
        "where the (deterministic) function $g$ is defined as\n",
        "$$g(x) = \\cos(1.5 \\pi x)$$\n",
        "and $\\xi$ is a white Gaussian noise with zero mean and variance $\\sigma^2 = 0.01$.\n",
        "\n",
        "5. Re-generate the same training dataset $\\boldsymbol{x}_{\\text{train}} \\in \\mathbb{R}^{200}, \\boldsymbol{y}_{\\text{train}} \\in \\mathbb{R}^{200}$ and test dataset $\\boldsymbol{x}_{\\text{test}} \\in \\mathbb{R}^{50}, \\boldsymbol{y}_{\\text{test}} \\in \\mathbb{R}^{50}$ that was used in Exercise 4.1.\n",
        "\n",
        "*Solution*:\n",
        "> We copy the code from Exercise 4.1. Notice that, in order to generate the *same* dataset, it is important to set the same seed that we used in Exercise 4.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pleased-meter",
      "metadata": {
        "id": "pleased-meter"
      },
      "outputs": [],
      "source": [
        "def g(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Evaluate g(x).\"\"\"\n",
        "    return np.cos(1.5 * np.pi * x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "human-price",
      "metadata": {
        "id": "human-price"
      },
      "outputs": [],
      "source": [
        "np.random.seed(41)\n",
        "x_train = np.random.uniform(-1, 1, size=30)\n",
        "x_test = np.random.uniform(-1, 1, size=50)\n",
        "y_train = g(x_train) + np.random.normal(0, np.sqrt(0.01), size=30)\n",
        "y_test = g(x_test) + np.random.normal(0, np.sqrt(0.01), size=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vanilla-spice",
      "metadata": {
        "id": "vanilla-spice"
      },
      "source": [
        "6. Implement the evaluation of the empirical risk associated to a polynomial regression via least squares with ridge regression, as well as its gradient.\n",
        "\n",
        "*Solution*:\n",
        "> The prediction function is the same as Exercise 4.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vulnerable-activation",
      "metadata": {
        "id": "vulnerable-activation"
      },
      "outputs": [],
      "source": [
        "def y_hat(x_j: float, w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate the prediction function associated to a polynomial regression.\"\"\"\n",
        "    return sum(w[i] * x_j**i for i in range(w.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "worthy-election",
      "metadata": {
        "id": "worthy-election"
      },
      "source": [
        "> The least squares loss with $\\ell^2$ regularization and its derivatives are:\n",
        "$$\\ell_{\\text{reg}}(x, y; \\boldsymbol{w}) = \\left(\\sum_{i = 0}^{n - 1} w^{(i)} x^i - y\\right)^2 + \\lambda \\sum_{i = 0}^{n - 1} \\left[w^{(i)}\\right]^2.$$\n",
        "$$\\nabla_\\boldsymbol{w} \\ell_{\\text{reg}}(x, y; \\boldsymbol{w}) = 2 \\left(\\sum_{i = 0}^{n - 1} w^{(i)} x^i - y\\right) \\begin{bmatrix}1\\\\x\\\\x^2\\\\\\dots\\\\x^{n-1}\\end{bmatrix} + 2 \\lambda \\boldsymbol{w}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "chronic-curtis",
      "metadata": {
        "id": "chronic-curtis"
      },
      "outputs": [],
      "source": [
        "def least_squares_loss_reg(x_j: float, y_j: float, w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate the least squares loss with ridge penalization term.\"\"\"\n",
        "    return (y_hat(x_j, w) - y_j)**2 + lambda_ * np.linalg.norm(w)**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lucky-equity",
      "metadata": {
        "id": "lucky-equity"
      },
      "outputs": [],
      "source": [
        "def grad_least_squares_loss_reg(x_j: float, y_j: float, w: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Evaluate the gradient of the least squares loss with ridge penalization term.\"\"\"\n",
        "    powers = np.arange(w.shape[0])\n",
        "    return 2 * (y_hat(x_j, w) - y_j) * x_j**powers + 2 * lambda_ * w"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "material-penny",
      "metadata": {
        "id": "material-penny"
      },
      "source": [
        "> The empirical risk is then definied by summing the regularized loss over the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "authorized-kansas",
      "metadata": {
        "id": "authorized-kansas"
      },
      "outputs": [],
      "source": [
        "def f_ex_4_3(w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate the empirical risk.\"\"\"\n",
        "    m_train = x_train.shape[0]\n",
        "    return 1 / m_train * sum(least_squares_loss_reg(x_j, y_j, w) for (x_j, y_j) in zip(x_train, y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "legislative-seeking",
      "metadata": {
        "id": "legislative-seeking"
      },
      "outputs": [],
      "source": [
        "def grad_f_ex_4_3(w: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Evaluate the gradient of the empirical risk.\"\"\"\n",
        "    m_train = x_train.shape[0]\n",
        "    return 1 / m_train * sum(grad_least_squares_loss_reg(x_j, y_j, w) for (x_j, y_j) in zip(x_train, y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "creative-turtle",
      "metadata": {
        "id": "creative-turtle"
      },
      "source": [
        "7. Choose $\\varepsilon = 10^{-8}$, $\\boldsymbol{w}_0 = \\boldsymbol{0}$ and $h_0 = 1$. Obtain the best fit polynomial for $n$ between 1 and 25 by running BFGS, and $\\lambda \\in \\{10^{-1}, 10^{-5}, 10^{-9}\\}$. Comment on the quality of the fit when $n$ increases.\n",
        "\n",
        "*Solution*:\n",
        "> We first copy the BFGS implementation we used in Exercise 4.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "outdoor-tonight",
      "metadata": {
        "id": "outdoor-tonight"
      },
      "outputs": [],
      "source": [
        "def bfgs(\n",
        "    f: typing.Callable, grad_f: typing.Callable, alpha: float, epsilon: float, maxit: int, w_0: np.ndarray,\n",
        "    h_0: float\n",
        ") -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run BFGS method with constant step length.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    f, grad_f : Python function\n",
        "        callable evaluating the cost function and its gradient, respectively.\n",
        "    alpha : float\n",
        "        constant step length.\n",
        "    c_1, c_2 : float\n",
        "        constants of the backtracking algorithm.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the norm of the gradient of the cost.\n",
        "    maxit : int\n",
        "        maximum number of allowed iterations.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "    h_0 : float\n",
        "        diagonal scaling factor that defines the initial approximation of the inverse of the hessian matrix.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    \"\"\"\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_f = [f(w_0)]\n",
        "    all_grad_f = [grad_f(w_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Prepare initial approximation of inverse of the hessian: take the identity matrix\n",
        "    I = np.eye(w_0.shape[0])\n",
        "    inv_hess_f_k = h_0 * I\n",
        "\n",
        "    # Use the norm of the gradient as stopping criterion.\n",
        "    while np.linalg.norm(all_grad_f[k]) > epsilon:\n",
        "        w_k = all_w[k]\n",
        "        grad_f_k = all_grad_f[k]\n",
        "\n",
        "        # Compute w_{k+1}\n",
        "        w_k_plus_1 = w_k - alpha * np.dot(inv_hess_f_k, grad_f_k)\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f(w_k_plus_1))\n",
        "        all_grad_f.append(grad_f(w_k_plus_1))\n",
        "\n",
        "        # Update approximation of inverse of the hessian\n",
        "        y_k = all_grad_f[k + 1] - all_grad_f[k]\n",
        "        s_k = all_w[k + 1] - all_w[k]\n",
        "        rho_k = 1 / np.dot(y_k, s_k)\n",
        "        inv_hess_f_k = (\n",
        "            np.dot(np.dot(I - rho_k * np.outer(s_k, y_k), inv_hess_f_k), I - rho_k * np.outer(y_k, s_k))\n",
        "            + rho_k * np.outer(s_k, s_k)\n",
        "        )\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "        # Bail out if exceeded allowed number of iterations\n",
        "        if k >= maxit:\n",
        "            print(\"WARNING: BFGS method exceeded number of allowed iterations\")\n",
        "            break\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_f), np.array(all_grad_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abstract-leonard",
      "metadata": {
        "id": "abstract-leonard"
      },
      "source": [
        "> We run the training for all required values of $n$ and $\\lambda$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "considered-black",
      "metadata": {
        "id": "considered-black"
      },
      "outputs": [],
      "source": [
        "all_lambdas = [1e-1, 1e-5, 1e-9]\n",
        "\n",
        "all_w = {lambda_: dict() for lambda_ in all_lambdas}\n",
        "all_f = {lambda_: dict() for lambda_ in all_lambdas}\n",
        "all_grad_f = {lambda_: dict() for lambda_ in all_lambdas}\n",
        "\n",
        "for lambda_ in all_lambdas:\n",
        "    # Note that the variable lambda_ will then be used in the functions least_squares_loss_reg\n",
        "    # and grad_least_squares_loss_reg, which are called in f_ex_4_3 and grad_f_ex_4_3\n",
        "    for n in range(1, 26):\n",
        "        all_w[lambda_][n], all_f[lambda_][n], all_grad_f[lambda_][n] = bfgs(\n",
        "            f_ex_4_3, grad_f_ex_4_3, 1, 1e-8, 1000, np.zeros(n), 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "retired-distributor",
      "metadata": {
        "id": "retired-distributor"
      },
      "source": [
        "> We then create three plots, one for each value of $\\lambda$. Each plot has two rows, corresponding to the training set (top) and the test set (bottom), and four columns, each column corresponding to a value of $n \\in \\{3, 7, 13, 25\\}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wrapped-strategy",
      "metadata": {
        "id": "wrapped-strategy"
      },
      "outputs": [],
      "source": [
        "x_plot = np.linspace(-1, 1, 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "convenient-weather",
      "metadata": {
        "id": "convenient-weather"
      },
      "outputs": [],
      "source": [
        "n_plot = [3, 7, 13, 25]\n",
        "for lambda_ in all_lambdas:\n",
        "    fig = plotly.subplots.make_subplots(rows=2, cols=4, vertical_spacing=0.05, horizontal_spacing=0.05)\n",
        "    for (col, n) in enumerate(n_plot):\n",
        "        fig.add_scatter(\n",
        "            x=x_train, y=y_train,\n",
        "            marker=dict(color=plotly.colors.qualitative.Set1[0], size=6),\n",
        "            mode=\"markers\", name=\"training dataset\",\n",
        "            row=1, col=col + 1, showlegend=(col == 0)\n",
        "        )\n",
        "        fig.add_scatter(\n",
        "            x=x_test, y=y_test,\n",
        "            marker=dict(color=plotly.colors.qualitative.Set1[1], size=6),\n",
        "            mode=\"markers\", name=\"test dataset\",\n",
        "            row=2, col=col + 1, showlegend=(col == 0)\n",
        "        )\n",
        "        fig.add_scatter(\n",
        "            x=x_plot, y=y_hat(x_plot, all_w[lambda_][n][-1]),\n",
        "            line=dict(color=plotly.colors.qualitative.Set1[6], width=2),\n",
        "            mode=\"lines\", name=\"y_hat(x)\",\n",
        "            row=1, col=col + 1, showlegend=(col == 0)\n",
        "        )\n",
        "        fig.add_scatter(\n",
        "            x=x_plot, y=y_hat(x_plot, all_w[lambda_][n][-1]),\n",
        "            line=dict(color=plotly.colors.qualitative.Set1[6], width=2),\n",
        "            mode=\"lines\", name=\"y_hat(x)\",\n",
        "            row=2, col=col + 1, showlegend=False\n",
        "        )\n",
        "    fig.update_layout(\n",
        "        title=(\n",
        "            \"lambda = \" + str(lambda_) + \". Training dataset (top) and test dataset (bottom). \"\n",
        "            + \"Increasing n = \" + str(n_plot) + \" (left to right)\"\n",
        "        ), width=2 * 512, height=512, autosize=False\n",
        "    )\n",
        "    # fig.update_yaxes(range=[-1.2, 1.2])\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "published-pierre",
      "metadata": {
        "id": "published-pierre"
      },
      "source": [
        "> * From the first plot we see that the regularization coefficient $\\lambda = 10^{-1}$ is too large. The regularization term is dominating the overall empirical risk, and this prevents a good fitting to the data.\n",
        "> * From the third plot we see that the regularization coefficient $\\lambda = 10^{-9}$ is too small. Indeed, the prediction for large values of $n$ is still oscillating, especially close to the boundaries of the interval $I$.\n",
        "> * From the second plot we realize that the choice $\\lambda = 10^{-5}$ is the best one among the proposed values for the regularization coefficient. Indeed, the fitting of the training dataset and the prediction on the test dataset look as good as the unregularized case in the second column ($n = 7$), and oscillations do not affect too much the prediction curve in the last two columns of the plot (especially the third column, $n = 13$).\n",
        ">\n",
        "> To get a further qualitative comparison between the proposed values of $\\lambda$, we prepare three RMSE plots as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "retained-carbon",
      "metadata": {
        "id": "retained-carbon"
      },
      "outputs": [],
      "source": [
        "RMSE_train = {\n",
        "    lambda_: np.array([\n",
        "        np.sqrt(1 / x_train.shape[0] * np.sum((y_hat(x_train, all_w[lambda_][n][-1]) - y_train)**2))\n",
        "        for n in range(1, 26)\n",
        "    ]) for lambda_ in all_lambdas\n",
        "}\n",
        "RMSE_test = {\n",
        "    lambda_: np.array([\n",
        "        np.sqrt(1 / x_test.shape[0] * np.sum((y_hat(x_test, all_w[lambda_][n][-1]) - y_test)**2))\n",
        "        for n in range(1, 26)\n",
        "    ]) for lambda_ in all_lambdas\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "breathing-lindsay",
      "metadata": {
        "id": "breathing-lindsay"
      },
      "outputs": [],
      "source": [
        "for lambda_ in all_lambdas:\n",
        "    fig = go.Figure()\n",
        "    fig.add_scatter(x=np.arange(RMSE_train[lambda_].shape[0]) + 1, y=RMSE_train[lambda_], name=\"Training set\")\n",
        "    fig.add_scatter(x=np.arange(RMSE_test[lambda_].shape[0]) + 1, y=RMSE_test[lambda_], name=\"Test set\")\n",
        "    fig.update_layout(title=\"lambda = \" + str(lambda_) + \". RMSE\")\n",
        "    fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tutorial-advance",
      "metadata": {
        "id": "tutorial-advance"
      },
      "source": [
        "> * From the first plot we confirm that the regularization coefficient $\\lambda = 10^{-1}$ is too large. Indeed, RMSE for both training and test sets are considerably larger than the best value we had obtained in the unregularized case.\n",
        "> * From the third plot we confirm that the regularization coefficient $\\lambda = 10^{-9}$ is too small. Indeed, RMSE for both training and test sets looks very close to the curve we had obtained in the unregularized case, meaning that the regularization has a very minor effect on the choice of the weights.\n",
        "> * From the second plot we confirm that the choice $\\lambda = 10^{-5}$ is the best one among the proposed values. Indeed, the best value of the RMSE on the test dataset is comparable to the one we had obtained in the unregularized case, and the increase of RMSE on the test dataset when $n$ increases is very moderate compared to the steep increase detected in the unregularized case.\n",
        ">\n",
        "> In order to better understand how ridge regularization is allowing to improve the results, we print for $n$ above 9 a comparison of the weights between the best regularized case $\\lambda = 10^{-5}$ (first row of the print) and the case $\\lambda = 10^{-9}$ (second row of the print), which is almost an unregularized regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "genuine-massage",
      "metadata": {
        "id": "genuine-massage"
      },
      "outputs": [],
      "source": [
        "for n in range(10, 26):\n",
        "    print(\"n =\", n)\n",
        "    weights_comparison = np.vstack([all_w[lambda_][n][-1] for lambda_ in all_lambdas[1:]])\n",
        "    with np.printoptions(precision=2, suppress=True):\n",
        "        print(weights_comparison)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lesser-procurement",
      "metadata": {
        "id": "lesser-procurement"
      },
      "source": [
        "> We notice that, for each fixed $n$, the regularization has the effect of (considerably, in some cases) decreasing the values of the weights $w^{(i)}$ associated to indices where $i$ is big (i.e., high monomial degree $x^i$). Notice however that, even tough decreased, such values are still non-zero, so ridge regression *did not* operate a feature selection. We will see next an exercise on feature selection.\n",
        ">\n",
        "> *Bonus remark*: notice how, in the regularized case, the weights in even positions (numbering positions from zero as Python does, so positions 0, 2, 4, ...) are typically larger in absolute value then those in odd positions (positions 1, 3, 5, ...). Looking at the expression (or the plot) of the function $g$, can you give a mathematical explanation of why this happens?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "excess-intervention",
      "metadata": {
        "id": "excess-intervention"
      },
      "source": [
        "## Exercise 4.4 (continuation of Exercise 4.2)\n",
        "\n",
        "The US National Centers for Environmental Information has collected weather data at the Raleigh Durham International Airport, that we are using to prediction the lowest temperature in any given day.\n",
        "\n",
        "7. Re-generate the same training dataset $\\boldsymbol{X}_{\\text{train}}, \\boldsymbol{y}_{\\text{train}}$ and test dataset $\\boldsymbol{x}_{\\text{test}}, \\boldsymbol{y}_{\\text{test}}$ that was used in Exercise 4.2.\n",
        "\n",
        "*Solution*:\n",
        "> We re-import the data using `pandas`, and carry out the same preprocessing operations that we were required to do in Exercise 4.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "monthly-crime",
      "metadata": {
        "id": "monthly-crime"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6693e1dc",
      "metadata": {
        "id": "6693e1dc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scientific-palace",
      "metadata": {
        "id": "scientific-palace"
      },
      "outputs": [],
      "source": [
        "if os.path.isfile(\"data/rdu-weather-history.csv\"):\n",
        "    csv_path = \"data/rdu-weather-history.csv\"\n",
        "else:\n",
        "    # csv_path = \"https://data.townofcary.org/explore/dataset/rdu-weather-history/download/?format=csv\"\n",
        "    csv_path = (\n",
        "        \"https://dmf.unicatt.it/~fball/public/optimization_for_machine_learning\"\n",
        "        + \"/rdu-weather-history.csv\"\n",
        "    )\n",
        "df = pd.read_csv(csv_path, parse_dates=[\"date\"], sep=\";\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "israeli-disclosure",
      "metadata": {
        "id": "israeli-disclosure"
      },
      "outputs": [],
      "source": [
        "bool_cols = [\n",
        "    \"fog\", \"fogheavy\", \"mist\", \"rain\", \"fogground\", \"ice\", \"glaze\", \"drizzle\", \"snow\", \"freezingrain\",\n",
        "    \"smokehaze\", \"thunder\", \"highwind\", \"hail\", \"blowingsnow\", \"dust\", \"freezingfog\"\n",
        "]\n",
        "for bool_col in bool_cols:\n",
        "    df[bool_col].fillna(0.0, inplace=True)\n",
        "    df[bool_col].replace(\"Present\", 1.0, inplace=True)\n",
        "df.drop(\"fastest5secwinddir\", axis=1, inplace=True)\n",
        "df.drop(\"fastest5secwindspeed\", axis=1, inplace=True)\n",
        "missing_cols = [\"snowfall\", \"snowdepth\"]\n",
        "for missing_col in missing_cols:\n",
        "    df[missing_col].fillna(0.0, inplace=True)\n",
        "assert len(df.columns[df.isna().any()].tolist()) == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "historic-marketplace",
      "metadata": {
        "id": "historic-marketplace"
      },
      "outputs": [],
      "source": [
        "season_col = []\n",
        "for date in df[\"date\"]:\n",
        "    if int(date.strftime(\"%m%d\")) >= 1221 or int(date.strftime(\"%m%d\")) <= 319:\n",
        "        season = 0  # winter\n",
        "    elif int(date.strftime(\"%m%d\")) >= 320 and int(date.strftime(\"%m%d\")) <= 620:\n",
        "        season = 1  # spring\n",
        "    elif int(date.strftime(\"%m%d\")) >= 621 and int(date.strftime(\"%m%d\")) <= 921:\n",
        "        season = 2  # summer\n",
        "    elif int(date.strftime(\"%m%d\")) >= 922 and int(date.strftime(\"%m%d\")) <= 1220:\n",
        "        season = 3  # fall\n",
        "    season_col.append(season)\n",
        "\n",
        "df[\"season\"] = season_col\n",
        "df.drop(\"date\", axis=1, inplace=True)\n",
        "dummy_seasons = pd.get_dummies(df[\"season\"], prefix=\"season\")\n",
        "dummy_seasons.drop(\"season_0\", axis=1, inplace=True)\n",
        "df.drop(\"season\", axis=1, inplace=True)\n",
        "df = pd.concat([df, dummy_seasons], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "suffering-military",
      "metadata": {
        "id": "suffering-military"
      },
      "outputs": [],
      "source": [
        "df = (df - df.min()) / (df.max() - df.min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bottom-globe",
      "metadata": {
        "id": "bottom-globe"
      },
      "outputs": [],
      "source": [
        "y = df[[\"temperaturemin\"]].to_numpy().reshape(-1)\n",
        "X = df.drop(\"temperaturemin\", axis=1).to_numpy()\n",
        "np.random.seed(42 + 300)\n",
        "perm = np.random.permutation(y.shape[0])\n",
        "m_train = int(0.8 * perm.shape[0])\n",
        "y_train = y[perm[:m_train]]\n",
        "X_train = X[perm[:m_train]]\n",
        "y_test = y[perm[m_train:]]\n",
        "X_test = X[perm[m_train:]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "crazy-vancouver",
      "metadata": {
        "id": "crazy-vancouver"
      },
      "source": [
        "8. Implement the ISTA algorithm with mini-batch stochastic gradient computation and with constant step length in a Python function. Such function should:\n",
        "   * take as input the number $m$ of addends, the size $m_b$ of a mini-batch, the (unregularized) function $s$, its gradient $\\nabla s$, the regularization coefficient $\\lambda$, the value $\\alpha$ of the step length, the tolerance $\\varepsilon$ for the stopping criterion, maximum number $K_{\\max}$ of allowed iterations, and the initial condition $\\boldsymbol{w}_{0}$;\n",
        "   * return as outputs the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$, the corresponding function values $\\{s(\\boldsymbol{w}_k)\\}_k$ and gradients $\\{\\nabla s(\\boldsymbol{w}_k)\\}_k$.\n",
        "\n",
        "   Use the stopping criterion based on the norm of the gradient (of the unregularized function $s$).\n",
        " \n",
        "*Solution*:\n",
        "> We start defining the soft thresholding function. Since we are interested in applying it to a vector $\\boldsymbol{w}$ (and not a scalar component $w^{(i)}$), we implement it using operators `<` and `>` applied elementwise, rather than `if` cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceramic-graham",
      "metadata": {
        "id": "ceramic-graham"
      },
      "outputs": [],
      "source": [
        "def soft_thresholding(w: np.ndarray, eta: float) -> np.ndarray:\n",
        "    \"\"\"Apply the soft thresholding function elementwise.\"\"\"\n",
        "    return (w + eta) * (w < - eta) + (w - eta) * (w > eta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deadly-underground",
      "metadata": {
        "id": "deadly-underground"
      },
      "source": [
        "> Check the elementwise application on a simple case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "electoral-chancellor",
      "metadata": {
        "id": "electoral-chancellor"
      },
      "outputs": [],
      "source": [
        "soft_thresholding(np.array([-10, -5, 5, 10]), eta=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "assigned-opinion",
      "metadata": {
        "id": "assigned-opinion"
      },
      "source": [
        "> We next implement the mini-batch version of ISTA. It is enough to copy the implementation of `mini_batch_stochastic_gradient` and only change the line that computes $\\boldsymbol{w}_{k+1}$ in order to apply the soft thresholding operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "advisory-savings",
      "metadata": {
        "id": "advisory-savings"
      },
      "outputs": [],
      "source": [
        "def mini_batch_ista(\n",
        "    m: int, m_b: int, s: typing.Callable, grad_s: typing.Callable, lambda_: float, alpha: float, epsilon: float,\n",
        "    maxit: int, w_0: np.ndarray\n",
        ") -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run the mini-batch stochastic gradient method with constant step length.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    m : int\n",
        "        number of addends in the expression of the (unregularized) cost function.\n",
        "    m_b : int\n",
        "        size of the mini-batch.\n",
        "    s, grad_s : Python function\n",
        "        callable evaluating the (unregularized) cost function and its gradient, respectively.\n",
        "    lambda_ : float\n",
        "        regularization coefficient.\n",
        "    alpha : float\n",
        "        constant step length.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the norm of the gradient of the cost.\n",
        "    maxit : int\n",
        "        maximum number of allowed iterations.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    \"\"\"\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_s = [s(w_0)]\n",
        "    all_grad_s = [grad_s(w_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Use the norm of the gradient as stopping criterion.\n",
        "    while np.linalg.norm(all_grad_s[k]) > epsilon:\n",
        "        w_k = all_w[k]\n",
        "\n",
        "        # Draw random indices\n",
        "        J_k = np.random.choice(m, size=m_b, replace=False)\n",
        "\n",
        "        # Compute the update direction\n",
        "        g_k = - 1 / m_b * sum([grad_s(w_k, addend=j) for j in J_k])\n",
        "\n",
        "        # Compute w_{k + 1}\n",
        "        w_k_plus_1 = soft_thresholding(w_k + alpha * g_k, alpha * lambda_)\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_s.append(s(w_k_plus_1))\n",
        "        all_grad_s.append(grad_s(w_k_plus_1))\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "        # Bail out if exceeded allowed number of iterations\n",
        "        if k >= maxit:\n",
        "            print(\"WARNING: ISTA method exceeded number of allowed iterations\")\n",
        "            break\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_s), np.array(all_grad_s)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hearing-defense",
      "metadata": {
        "id": "hearing-defense"
      },
      "source": [
        "9. Implement the evaluation of the empirical risk associated to a linear regression via least squares with multiple features, as well as its gradient.\n",
        "\n",
        "*Solution*:\n",
        "> We copy here the functions we have implemented in Exercise 4.2 as they are. Note that (in contrast to the changes we had to do when copying from Exercise 4.1 to Exercise 4.3) here there are no changes, because the regularization term $\\lambda \\left\\|\\boldsymbol{w}\\right\\|_1$ is handled separately by the ISTA algorithm!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "grave-registration",
      "metadata": {
        "id": "grave-registration"
      },
      "outputs": [],
      "source": [
        "def y_hat(x_j: np.ndarray, w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate the prediction function associated to a linear regression.\"\"\"\n",
        "    return np.dot(w[:-1], x_j) + w[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "authentic-knock",
      "metadata": {
        "id": "authentic-knock"
      },
      "outputs": [],
      "source": [
        "def least_squares_loss(x_j: np.ndarray, y_j: float, w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate the least squares loss.\"\"\"\n",
        "    return (y_hat(x_j, w) - y_j)**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tribal-easter",
      "metadata": {
        "id": "tribal-easter"
      },
      "outputs": [],
      "source": [
        "def grad_least_squares_loss(x_j: np.ndarray, y_j: float, w: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Evaluate the gradient of the least squares loss.\"\"\"\n",
        "    vec = np.zeros(w.shape[0])\n",
        "    vec[:-1] = x_j\n",
        "    vec[-1] = 1\n",
        "    return 2 * (y_hat(x_j, w) - y_j) * vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vietnamese-recorder",
      "metadata": {
        "id": "vietnamese-recorder"
      },
      "outputs": [],
      "source": [
        "def s_ex_4_4(w: np.ndarray, addend: int = None) -> float:\n",
        "    \"\"\"\n",
        "    Evaluate the empirical risk (if addend is None).\n",
        "\n",
        "    For use within a stochastic method, the optional parameter addend may take an integer value.\n",
        "    In such case, the loss associated to the addend-th element is computed instead.\n",
        "    \"\"\"\n",
        "    if addend is None:\n",
        "        m_train = X_train.shape[0]\n",
        "        return 1 / m_train * sum(least_squares_loss(x_j, y_j, w) for (x_j, y_j) in zip(X_train, y_train))\n",
        "    else:\n",
        "        return least_squares_loss(X_train[addend], y_train[addend], w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "connected-bowling",
      "metadata": {
        "id": "connected-bowling"
      },
      "outputs": [],
      "source": [
        "def grad_s_ex_4_4(w: np.ndarray, addend: int = None) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Evaluate the gradient of the empirical risk (if addend is None).\n",
        "\n",
        "    For use within a stochastic method, the optional parameter addend may take an integer value.\n",
        "    In such case, the gradient loss associated to the addend-th element is computed instead.\n",
        "    \"\"\"\n",
        "    if addend is None:\n",
        "        m_train = X_train.shape[0]\n",
        "        return 1 / m_train * sum(grad_least_squares_loss(x_j, y_j, w) for (x_j, y_j) in zip(X_train, y_train))\n",
        "    else:\n",
        "        return grad_least_squares_loss(X_train[addend], y_train[addend], w)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "elegant-dressing",
      "metadata": {
        "id": "elegant-dressing"
      },
      "source": [
        "10. Train the linear regression model using a mini-batch ISTA algorithm, varying:\n",
        "    * mini-batch size, equal to $10\\%, 20\\%, 50\\%$ or $100\\%$ of the training set cardinality, and\n",
        "    * regularization coefficient, equal to $10^{-1}$, $10^{-3}$ or $10^{-5}$.\n",
        " \n",
        "*Solution*:\n",
        "> We recompute the value of the smoothness constant $L$, to be used in the evaluation of the step size $\\alpha = 1 / L$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spoken-calendar",
      "metadata": {
        "id": "spoken-calendar"
      },
      "outputs": [],
      "source": [
        "eigs, _ = np.linalg.eig(np.dot(X_train.T, X_train))\n",
        "L = 2 * np.max(eigs) / m_train\n",
        "print(\"L =\", L)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "utility-disco",
      "metadata": {
        "id": "utility-disco"
      },
      "source": [
        "> We next train the regression model for all requested values of $m_b$ and $\\lambda$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "loving-imagination",
      "metadata": {
        "id": "loving-imagination"
      },
      "outputs": [],
      "source": [
        "all_lambdas = [1e-1, 1e-3, 1e-5]\n",
        "all_perc = [10, 20, 50, 100]\n",
        "\n",
        "all_w = {lambda_: dict() for lambda_ in all_lambdas}\n",
        "all_s = {lambda_: dict() for lambda_ in all_lambdas}\n",
        "all_grad_s = {lambda_: dict() for lambda_ in all_lambdas}\n",
        "\n",
        "np.random.seed(44 + 1000)\n",
        "\n",
        "for lambda_ in all_lambdas:\n",
        "    print(\"lambda =\", lambda_)\n",
        "    for perc in all_perc:\n",
        "        m_b = int(perc / 100 * m_train)\n",
        "        all_w[lambda_][perc], all_s[lambda_][perc], all_grad_s[lambda_][perc] = mini_batch_ista(\n",
        "            m_train, m_b, s_ex_4_4, grad_s_ex_4_4, lambda_, 1 / L, 1e-2, 200, np.zeros(X.shape[1] + 1))\n",
        "        K = all_w[lambda_][perc].shape[0]\n",
        "        E = int(np.ceil(K * m_b / m_train))\n",
        "        print(\"\\t m_b =\", m_b, \"out of\", m_train, \"converged in\", K, \"iterations,\", E, \"epochs\")\n",
        "        non_zero = np.count_nonzero(all_w[lambda_][perc][-1]) / all_w[lambda_][perc][-1].shape[0]\n",
        "        print(\"\\t percentage of non zero coefficients:\", non_zero)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cutting-allah",
      "metadata": {
        "id": "cutting-allah"
      },
      "source": [
        "> * When $\\lambda = 10^{-1}$ the optimization did not converge in the required number of iterations. We also see that the feature selection is very aggressive, often discarding more than $90\\%$ of the available features.\n",
        "> * When $\\lambda = 10^{-3}$ we do get convergence to models that often discard around $40\\%$ of the available features.\n",
        "> * When $\\lambda = 10^{-5}$ no feature selection is happening, therefore we expect to obtain a model which is very similar to the unregularized one."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "freelance-arcade",
      "metadata": {
        "id": "freelance-arcade"
      },
      "source": [
        "11. Evaluate the accuracy of the prediction, and determine which are the most relevant features while carrying out the prediction task. Are they the same as in the unregularized case?\n",
        "\n",
        "*Solution*:\n",
        "> We compute the RMSE on the training and test sets for all values of $m_b$ and $\\lambda$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "republican-dakota",
      "metadata": {
        "id": "republican-dakota"
      },
      "outputs": [],
      "source": [
        "for lambda_ in all_lambdas:\n",
        "    print(\"lambda =\", lambda_)\n",
        "    for perc in all_perc:\n",
        "        RMSE_train = np.sqrt(\n",
        "            1 / X_train.shape[0]\n",
        "            * sum((y_hat(x_j, all_w[lambda_][perc][-1]) - y_j)**2 for (x_j, y_j) in zip(X_train, y_train)))\n",
        "        RMSE_test = np.sqrt(\n",
        "            1 / X_test.shape[0]\n",
        "            * sum((y_hat(x_j, all_w[lambda_][perc][-1]) - y_j)**2 for (x_j, y_j) in zip(X_test, y_test)))\n",
        "        m_b = int(perc / 100 * m_train)\n",
        "        print(\"\\t m_b =\", m_b, \"out of\", m_train, \"RMSE_train =\", RMSE_train, \"RMSE_test =\", RMSE_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nervous-kruger",
      "metadata": {
        "id": "nervous-kruger"
      },
      "source": [
        "> * The case $\\lambda = 10^{-1}$ is giving the worst RMSE performance. We are thus led to believe that, by being too aggressive with feature selection, we have pruned some features that were actually important!\n",
        "> * The cases $\\lambda = 10^{-3}$ and $\\lambda = 10^{-5}$ have very similar RMSE results. If interpretability of the prediction model is important, then we suggest to use the model resulting from $\\lambda = 10^{-3}$ because it is easier to read, having discarded around $40\\%$ of the available features.\n",
        ">\n",
        "> Finally, we conclude listing which are the important features of each model (i.e., following what we did in Exercise 4.2, the features with weight greater than 0.1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dressed-plenty",
      "metadata": {
        "id": "dressed-plenty"
      },
      "outputs": [],
      "source": [
        "for lambda_ in all_lambdas:\n",
        "    print(\"lambda =\", lambda_)\n",
        "    for perc in all_perc:\n",
        "        important_features = np.argwhere(np.abs(all_w[lambda_][perc][-1]) > 0.1)\n",
        "        names = [df.drop(\"temperaturemin\", axis=1).columns[f][0] for f in important_features[:-1]]\n",
        "        m_b = int(perc / 100 * m_train)\n",
        "        print(\"\\t m_b =\", m_b, \"out of\", m_train, \"important features:\", names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "elect-contract",
      "metadata": {
        "id": "elect-contract"
      },
      "source": [
        "> * The models trained with $\\lambda = 10^{-3}$ and $\\lambda = 10^{-5}$ agree with the unregularized training in determining which features have been detected as important.\n",
        "> * In contrast, the model trained with $\\lambda = 10^{-1}$ has discarded (or classified as unimportant) the two seasonal features. This confirms our intuition that with such a large value of $\\lambda$ important features were pruned, leading to a worse predictive model."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}