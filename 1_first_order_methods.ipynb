{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pserebrennikov/3rd-year-project/blob/master/1_first_order_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fifteen-kinase",
      "metadata": {
        "id": "fifteen-kinase"
      },
      "source": [
        "# Tutorial 1 - First order methods\n",
        "### Course on Optimization for Machine Learning - Dr. F. Ballarin\n",
        "### Master Degree in Data Analytics for Business, Catholic University of the Sacred Heart, Milano"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "equal-sigma",
      "metadata": {
        "id": "equal-sigma"
      },
      "source": [
        "In this notebook we implement first order methods using the [`numpy` library](https://numpy.org/). Furthermore, we rely on [`plotly`](https://plotly.com/) for the visualization and creation of interactive plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ae0ce43",
      "metadata": {
        "id": "4ae0ce43"
      },
      "outputs": [],
      "source": [
        "import typing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "specialized-dancing",
      "metadata": {
        "id": "specialized-dancing"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import plotly.colors\n",
        "import plotly.graph_objects as go\n",
        "import plotly.subplots"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "southeast-biodiversity",
      "metadata": {
        "id": "southeast-biodiversity"
      },
      "source": [
        "## Exercise 1.1\n",
        "Let $\\boldsymbol{w} \\in \\mathbb{R}^2$. Consider the following quadratic function, known as *Booth function* and defined as\n",
        "$$f(\\boldsymbol{w}) = (w^{(0)} + 2 w^{(1)} - 7)^2 + (2 w^{(0)} + w^{(1)} - 5)^2.$$\n",
        "Note that inside notebooks we will denote $\\boldsymbol{w} = (w^{(0)}, w^{(1)})$ rather than $\\boldsymbol{w} = (w^{(1)}, w^{(2)})$, because Python starts counting from 0!\n",
        "\n",
        "1. Draw a surface plot and a contour plot of the function $f$ on the square domain $[-10, 10]^2$.\n",
        "\n",
        "*Solution*:\n",
        "> First of all we define the domain for each component of $\\boldsymbol{w}$. Note that in this case they correspond to the same interval, but in general (e.g. a rectangular domain) they might be different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "elegant-provincial",
      "metadata": {
        "id": "elegant-provincial"
      },
      "outputs": [],
      "source": [
        "domain_component_0 = [-10, 10]\n",
        "domain_component_1 = [-10, 10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "painted-property",
      "metadata": {
        "id": "painted-property"
      },
      "source": [
        "> In order to prepare a plot, we define a uniform subdivision in each coordinate direction, made up of 100 points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "turkish-smoke",
      "metadata": {
        "id": "turkish-smoke"
      },
      "outputs": [],
      "source": [
        "w_component_0 = np.linspace(domain_component_0[0], domain_component_0[1], 100)\n",
        "w_component_1 = np.linspace(domain_component_1[0], domain_component_1[1], 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "domestic-export",
      "metadata": {
        "id": "domestic-export"
      },
      "outputs": [],
      "source": [
        "w_component_0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "spiritual-choir",
      "metadata": {
        "id": "spiritual-choir"
      },
      "source": [
        "> We then evaluate the function $f$ at every $\\boldsymbol{w} = (w^{(0)}, w^{(1)})$ pair by means of a simple `for` loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shaped-marathon",
      "metadata": {
        "id": "shaped-marathon"
      },
      "outputs": [],
      "source": [
        "f_w = np.zeros((len(w_component_0), len(w_component_1)))\n",
        "for i in range(f_w.shape[0]):\n",
        "    for j in range(f_w.shape[1]):\n",
        "        f_w[j, i] = (\n",
        "            (w_component_0[i] + 2 * w_component_1[j] - 7)**2\n",
        "            + (2 * w_component_0[i] + w_component_1[j] - 5)**2\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "senior-rebecca",
      "metadata": {
        "id": "senior-rebecca"
      },
      "source": [
        "> We show a surface plot using `plotly`. Note that you can interact in multiple ways with the plot, Try to hover your mouse on the surface, and also to change the options on the top-right of the figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bigger-memorabilia",
      "metadata": {
        "id": "bigger-memorabilia"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Surface(x=w_component_0, y=w_component_1, z=f_w)])\n",
        "fig.update_layout(title=\"Booth function - surface plot\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stuffed-replacement",
      "metadata": {
        "id": "stuffed-replacement"
      },
      "source": [
        "> Note how `plotly` shows contour lines when hovering the mouse on the surface. Still, we can prepare a countour plot using similar instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "entitled-narrative",
      "metadata": {
        "id": "entitled-narrative"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Contour(x=w_component_0, y=w_component_1, z=f_w)])\n",
        "fig.update_layout(title=\"Booth function - contour plot\", width=512, height=512, autosize=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bridal-orchestra",
      "metadata": {
        "id": "bridal-orchestra"
      },
      "source": [
        "> One can even combine surface and contour plots in the same figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fixed-parcel",
      "metadata": {
        "id": "fixed-parcel"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Surface(x=w_component_0, y=w_component_1, z=f_w)])\n",
        "fig.update_traces(contours_z=dict(show=True, project_z=True, usecolormap=True))\n",
        "fig.update_layout(title=\"Booth function - surface and countour plots in the same figure\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fallen-daily",
      "metadata": {
        "id": "fallen-daily"
      },
      "source": [
        "*Futher suggestions*:\n",
        "> * if you have a programming background or some experience with `numpy`, you might have heard about *vectorization* as a better (i.e., more efficient) evaluation of a function at several points. Feel free to change the nested for loop that we used to evaluate $f(\\boldsymbol{w})$ with a vectorized version. You may want to use [`numpy.meshgrid`](https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html). While devising algorithms that extract the best performance out of the computing architectures and libraries is certainly beneficial to achieve in machine learning applications, in this course we are more interested in using Python and its libraries are tools to understand optimization algorithms;\n",
        "> * if you have had courses in Python before, you might have used [`matplotlib`](https://matplotlib.org/) to create your plots. `matplotlib` is arguably the most widely used plotting library in Python. In this course I have chosen to show you `plotly` because it offers simpler ways to interact with the generated plots compared to `matplotlib`. I believe that this is especially important for data science students, e.g. when carrying out preprocessing tasks on the dataset (e.g., you have a scatterplot and you would like to easily determine the IDs of points which are outliers, so that you can treat them appropriately)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "variable-alexander",
      "metadata": {
        "id": "variable-alexander"
      },
      "source": [
        "2. Compute the gradient $\\nabla f$ and determine the global minimum of the function $f$.\n",
        "\n",
        "*Solution*:\n",
        "> Recall that $$f(\\boldsymbol{w}) = (w^{(0)} + 2 w^{(1)} - 7)^2 + (2 w^{(0)} + w^{(1)} - 5)^2.$$\n",
        "> By taking the partial derivatives of $f$ we see that the gradient of $\\nabla f$ is\n",
        "\\begin{equation*}\n",
        "\\nabla f(\\boldsymbol{w}) = \\begin{bmatrix}\n",
        "2 \\cdot (w^{(0)} + 2 w^{(1)} - 7) + 2 \\cdot 2 \\cdot (2 w^{(0)} + w^{(1)} - 5)\\\\\n",
        "2 \\cdot 2 \\cdot (w^{(0)} + 2 w^{(1)} - 7) + 2 \\cdot (2 w^{(0)} + w^{(1)} - 5)\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "10 w^{(0)} + 8 w^{(1)} - 34\\\\\n",
        "8 w^{(0)} + 10 w^{(1)} - 38\\\\\n",
        "\\end{bmatrix}.\n",
        "\\end{equation*}\n",
        "> Since stationary points are such that $\\nabla f(\\boldsymbol{w}) = \\boldsymbol{0}$, we have to solve\n",
        "\\begin{equation*}\n",
        "\\begin{cases}\n",
        "10 w^{(0)} + 8 w^{(1)} - 34 = 0\\\\\n",
        "8 w^{(0)} + 10 w^{(1)} - 38 = 0\\\\\n",
        "\\end{cases}.\n",
        "\\end{equation*}\n",
        "This is a linear system that you can easily solve on paper to obtain $\\boldsymbol{w}^* = (1, 3)$.\n",
        ">\n",
        "> Is $\\boldsymbol{w}^*$ a local minimum? We can proceed in at least a couple of ways:\n",
        "> * *graphical way*: we have just plotted $f$ in the square domain $[-10, 10]^2$, and we notice that $\\boldsymbol{w}^*$ is inside that domain. If we hover our mouse on the surface plot we can easily locate $\\boldsymbol{w}^*$ on the plot and confirm that it is a minimum.\n",
        "> * *mathematical way*: we can make use of the second order optimality conditions. They requires us to compute the hessian of $f$\n",
        "\\begin{equation*}\n",
        "\\nabla^2 f(\\boldsymbol{w}) =\n",
        "\\begin{bmatrix}\n",
        "10 & 8\\\\\n",
        "8 & 10\\\\\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "and check if it is positive definite. In order to check the positive definiteness of such matrix we use the equivalent definition in terms of its eigenvalues, which can be computed by means of the [`numpy.linalg.eig`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "single-devil",
      "metadata": {
        "id": "single-devil"
      },
      "outputs": [],
      "source": [
        "hessian_f = np.array([[10, 8], [8, 10]])\n",
        "hessian_f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imported-friendship",
      "metadata": {
        "id": "imported-friendship"
      },
      "outputs": [],
      "source": [
        "eigs, _ = np.linalg.eig(hessian_f)\n",
        "eigs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "composite-entertainment",
      "metadata": {
        "id": "composite-entertainment"
      },
      "outputs": [],
      "source": [
        "assert (eigs > 0).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nonprofit-chapel",
      "metadata": {
        "id": "nonprofit-chapel"
      },
      "source": [
        "> Since all the eigenvalues of the hessian matrix (evaluated at $\\boldsymbol{w}^*$) are strictly positive we conclude that $\\boldsymbol{w}^*$ is local minimum.\n",
        ">\n",
        "> Is $\\boldsymbol{w}^*$ a global minimum? We have just shown that it is a local minimum; furthermore, there are not other possible minima (i.e., the equation $\\nabla f(\\boldsymbol{w}) = \\boldsymbol{0}$) has only one solution), so $\\boldsymbol{w}^*$ is actually a global minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sealed-latex",
      "metadata": {
        "id": "sealed-latex"
      },
      "source": [
        "3. Implement the gradient descent method with constant step length in a Python function. Use the stopping criterion based on the error of the cost. Such function should:\n",
        "   * take as input the value $\\alpha$ of the step length, the tolerance $\\varepsilon$ for the stopping criterion, and the initial condition $\\boldsymbol{w}_{0}$;\n",
        "   * return as outputs the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$, the corresponding function values $\\{f(\\boldsymbol{w}_k)\\}_k$ and gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$.\n",
        "\n",
        "*Solution*:\n",
        "> We first prepare two functions for the evaluation of $f$ and $\\nabla f$. Note that the inputs of both functions i the point $\\boldsymbol{w}$ stored as a [`numpy array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html), so we can access its components $w^{(0)}$ and $w^{(1)}$ using the code `w[0]` and `w[1]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "warming-organic",
      "metadata": {
        "id": "warming-organic"
      },
      "outputs": [],
      "source": [
        "def f_ex_1_1(w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate f(w).\"\"\"\n",
        "    return (w[0] + 2 * w[1] - 7)**2 + (2 * w[0] + w[1] - 5)**2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "restricted-costa",
      "metadata": {
        "id": "restricted-costa"
      },
      "source": [
        "> Furthermore, in preparing the function for $\\nabla f$ we return the vector as a `numpy array` as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "educational-model",
      "metadata": {
        "id": "educational-model"
      },
      "outputs": [],
      "source": [
        "def grad_f_ex_1_1(w: np.ndarray) -> np.ndarray:\n",
        "    r\"\"\"Evaluate \\nabla f(w).\"\"\"\n",
        "    return np.array([10 * w[0] + 8 * w[1] - 34, 8 * w[0] + 10 * w[1] - 38])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "olive-contamination",
      "metadata": {
        "id": "olive-contamination"
      },
      "source": [
        "> We can do a simple check to verify that we have not made any mistake when copying the formule. We know that $\\boldsymbol{w}^* = (1, 3)$ is such that $f(\\boldsymbol{w}^*) = 0$ (by substitution in the expression of $f$) and that it is a minimum (so $\\nabla f(\\boldsymbol{w}^*) = \\boldsymbol{0}$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "taken-western",
      "metadata": {
        "id": "taken-western"
      },
      "outputs": [],
      "source": [
        "w_star = np.array([1, 3])\n",
        "w_star"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "apparent-imagination",
      "metadata": {
        "id": "apparent-imagination"
      },
      "outputs": [],
      "source": [
        "f_ex_1_1(w_star)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "regulation-turning",
      "metadata": {
        "id": "regulation-turning"
      },
      "outputs": [],
      "source": [
        "grad_f_ex_1_1(w_star)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ahead-brunswick",
      "metadata": {
        "id": "ahead-brunswick"
      },
      "source": [
        "> We next implement the gradient descent method with constant step length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "distinct-parts",
      "metadata": {
        "id": "distinct-parts"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_ex_1_1(alpha: float, epsilon: float, w_0: np.ndarray) -> typing.Tuple[\n",
        "        np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run the gradient descent method with constant step length.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : float\n",
        "        constant step length.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the cost.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    \"\"\"\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_f = [f_ex_1_1(w_0)]\n",
        "    all_grad_f = [grad_f_ex_1_1(w_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Use the error on the cost to determine when the while loop should stop.\n",
        "    # Since the optimal cost function is zero and f >= 0 everywhere, the error is actually the evaluation of f.\n",
        "    while all_f[k] > epsilon:\n",
        "        w_k = all_w[k]\n",
        "        grad_f_k = all_grad_f[k]\n",
        "        w_k_plus_1 = w_k - alpha * grad_f_k\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f_ex_1_1(w_k_plus_1))\n",
        "        all_grad_f.append(grad_f_ex_1_1(w_k_plus_1))\n",
        "\n",
        "        # Bail out if the descent condition is not satisfied\n",
        "        if all_f[k + 1] >= all_f[k]:\n",
        "            print(\"WARNING: descent conditions is not satisfied\")\n",
        "            break\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_f), np.array(all_grad_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "macro-carnival",
      "metadata": {
        "id": "macro-carnival"
      },
      "source": [
        "4. Choose $\\alpha = 10^{-2}$, $\\varepsilon = 10^{-5}$ and $\\boldsymbol{w}_0 = (-8, -8)$. Visualize:\n",
        "   * the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$ on a surface plot of $f$;\n",
        "   * the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$ on a contour plot of $f$;\n",
        "   * a semilogarithimic plot of error in the function value $\\{f(\\boldsymbol{w}_k) - f(\\boldsymbol{w}^*)\\}_k$ versus the iteration counter $k$;\n",
        "   * a semilogarithimic plot of the norm of the gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$ versus the iteration counter $k$.\n",
        " \n",
        "*Solution*:\n",
        "> First of all we query our implementation `gradient_descent_ex_1_1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "incorrect-millennium",
      "metadata": {
        "id": "incorrect-millennium"
      },
      "outputs": [],
      "source": [
        "all_w, all_f, all_grad_f = gradient_descent_ex_1_1(1e-2, 1e-5, np.array([-8.0, -8.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "contained-charter",
      "metadata": {
        "id": "contained-charter"
      },
      "source": [
        "> To have an intuition of the how the iterative process went we may have a look at the outputs of the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "banner-surface",
      "metadata": {
        "id": "banner-surface"
      },
      "outputs": [],
      "source": [
        "assert all_w.shape[0] == all_f.shape[0] == all_grad_f.shape[0]\n",
        "all_w.shape[0]  # number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "engaged-manufacturer",
      "metadata": {
        "id": "engaged-manufacturer"
      },
      "outputs": [],
      "source": [
        "all_w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "silver-rescue",
      "metadata": {
        "id": "silver-rescue"
      },
      "outputs": [],
      "source": [
        "all_f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "informative-error",
      "metadata": {
        "id": "informative-error"
      },
      "outputs": [],
      "source": [
        "all_grad_f"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "missing-nelson",
      "metadata": {
        "id": "missing-nelson"
      },
      "source": [
        "> We first prepare the image containing the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$ on a surface plot of $f$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "southwest-appeal",
      "metadata": {
        "id": "southwest-appeal"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(\n",
        "    data=[go.Surface(x=w_component_0, y=w_component_1, z=f_w, opacity=0.5)]\n",
        ")\n",
        "fig.add_scatter3d(\n",
        "    x=all_w[:, 0], y=all_w[:, 1], z=all_f,\n",
        "    marker=dict(color=\"black\", size=4),\n",
        "    line=dict(color=\"black\", width=2)\n",
        ")\n",
        "fig.update_layout(title=\"Booth function - optimization variable iterations over surface plot\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "familiar-angle",
      "metadata": {
        "id": "familiar-angle"
      },
      "source": [
        "> Similarly, the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$ can be plotted on a contour plot of $f$. Note how, the gradient method proceeds in a direction orthogonal to the contour lines, as expected (because from calculus courses we know that the gradient is orthogonal to the contour lines)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thirty-kruger",
      "metadata": {
        "id": "thirty-kruger"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Contour(x=w_component_0, y=w_component_1, z=f_w, opacity=0.5)])\n",
        "fig.add_scatter(\n",
        "    x=all_w[:, 0], y=all_w[:, 1],\n",
        "    marker=dict(color=\"black\", size=10),\n",
        "    line=dict(color=\"black\", width=2),\n",
        "    mode=\"lines+markers\"\n",
        ")\n",
        "fig.update_layout(\n",
        "    title=\"Booth function - optimization variable iterations over contour plot\",\n",
        "    width=512, height=512, autosize=False\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pointed-military",
      "metadata": {
        "id": "pointed-military"
      },
      "source": [
        "> The next plot asks to compute the error in the function value $f(\\boldsymbol{w}_k) - f(\\boldsymbol{w}^*)$. However, since in this case $f(\\boldsymbol{w}^*)$ is zero, that is actually the same as plotting $f(\\boldsymbol{w}_k)$. Preparing such plot with a linear vertical axis helps us to realize that there is a very steep decrease of the objective function in the first 10 iterations, as we could have also realized from the plots above. However, we cannot appreciate the effect of subsequent improvements because they are very close to the horizontal axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "invisible-substance",
      "metadata": {
        "id": "invisible-substance"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=go.Scatter(x=np.arange(all_f.shape[0]), y=all_f))\n",
        "fig.update_layout(title=\"Booth function - error in the function value\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "variable-string",
      "metadata": {
        "id": "variable-string"
      },
      "source": [
        "> In these situations (data that are distributed across several orders of magnitude) it is convenient to employ semilogarithmic plots. In this case, we adopt a logarithimic scale on the vertical axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "opposed-dealing",
      "metadata": {
        "id": "opposed-dealing"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=go.Scatter(x=np.arange(all_f.shape[0]), y=all_f))\n",
        "fig.update_layout(title=\"Booth function - error on the function value - semilog plot\")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "moving-german",
      "metadata": {
        "id": "moving-german"
      },
      "source": [
        "> Note how, starting from $k > 25$, the curve in the previous plot resembles very closely a line. We will come back to this point later on the lecture.\n",
        ">\n",
        "> Similarly, we prepare a plot of the norm of the gradients $\\left\\|\\nabla f(\\boldsymbol{w}_k)\\right\\|$. We use a semilog plot as well, since also these norm will vary by several orders of magnitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "russian-empty",
      "metadata": {
        "id": "russian-empty"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=go.Scatter(x=np.arange(all_f.shape[0]), y=np.linalg.norm(all_grad_f, axis=1)))\n",
        "fig.update_layout(title=\"Booth function - violation of first order optimality conditions - semilog plot\")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chicken-efficiency",
      "metadata": {
        "id": "chicken-efficiency"
      },
      "source": [
        "> Note how also for the norm of the gradient the plot resembles a linear profile for $k > 25$. Note however that while the cost function error at the final iteration is of the order of $10^{-5}$, the violation of the first order optimality conditions is of the order of $5 \\cdot 10^{-2}$. We will also come back to this point later on in the lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "respiratory-integral",
      "metadata": {
        "id": "respiratory-integral"
      },
      "source": [
        "5. Now keep $\\alpha = 10^{-2}$ and $\\varepsilon = 10^{-5}$, but change the initial conditions $\\boldsymbol{w}_0$ by trying eight equispaced points on the circle centered at the origin and of radius 8. Does the gradient method always converge to $\\boldsymbol{w}^*$?\n",
        "\n",
        "*Solution*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "laden-cabin",
      "metadata": {
        "id": "laden-cabin"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Contour(x=w_component_0, y=w_component_1, z=f_w, opacity=0.5, showscale=False)])\n",
        "for guess in range(8):\n",
        "    theta = guess * np.pi / 4\n",
        "    w_0_theta = 8 * np.array([np.cos(theta), np.sin(theta)])\n",
        "    all_w_theta, _, _ = gradient_descent_ex_1_1(1e-2, 1e-5, w_0_theta)\n",
        "    fig.add_scatter(\n",
        "        x=all_w_theta[:, 0], y=all_w_theta[:, 1],\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[guess], size=10),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[guess], width=2),\n",
        "        mode=\"lines+markers\", name=\"Theta = \" + str(guess / 4) + \" pi\"\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=\"Booth function - optimization variable iterations over contour plot - different initial points\",\n",
        "    width=612, height=512, autosize=False\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "frequent-optimum",
      "metadata": {
        "id": "frequent-optimum"
      },
      "source": [
        "> We do always have convergence to $\\boldsymbol{w}^*$. We will justify this later on in the lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "higher-intervention",
      "metadata": {
        "id": "higher-intervention"
      },
      "source": [
        "6. Consider now $\\varepsilon = 10^{-5}$, $\\boldsymbol{w}_0 = (-3, -3)$ and let the step length $\\alpha$ vary among the following five possible choices: 1, 1/9, 1/9.4, 1/18 and 1/36. Does the gradient method always converge to $\\boldsymbol{w}^*$?\n",
        "\n",
        "*Solution*:\n",
        "> Let us now consider first $\\alpha = 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spiritual-subscription",
      "metadata": {
        "id": "spiritual-subscription"
      },
      "outputs": [],
      "source": [
        "all_w_alpha_1, all_f_alpha_1, _ = gradient_descent_ex_1_1(1, 1e-5, np.array([-3.0, -3.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "amazing-binding",
      "metadata": {
        "id": "amazing-binding"
      },
      "source": [
        "> The descent condition has been violated at the first iteration: with such a large value of $\\alpha$ the iterative method is diverging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "swiss-temperature",
      "metadata": {
        "id": "swiss-temperature"
      },
      "outputs": [],
      "source": [
        "all_f_alpha_1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "numerous-projection",
      "metadata": {
        "id": "numerous-projection"
      },
      "source": [
        "> Let us now move to $\\alpha = 1/9$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "surprising-simple",
      "metadata": {
        "id": "surprising-simple"
      },
      "outputs": [],
      "source": [
        "all_w_alpha_9, all_f_alpha_9, _ = gradient_descent_ex_1_1(1 / 9, 1e-5, np.array([-3.0, -3.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "champion-singapore",
      "metadata": {
        "id": "champion-singapore"
      },
      "source": [
        "> Also in this case the iterations have been stopped. However, in constrast to $\\alpha=1$, it seems that the cost function is getting stuck at a value around $450$. Such value is not associated to a minimum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "governing-layout",
      "metadata": {
        "id": "governing-layout"
      },
      "outputs": [],
      "source": [
        "all_f_alpha_9"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "micro-tract",
      "metadata": {
        "id": "micro-tract"
      },
      "source": [
        "> The optimization variables seem to oscillate between the point (6, 8) and the point (-4, -2) without making any sensible progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thick-dodge",
      "metadata": {
        "id": "thick-dodge"
      },
      "outputs": [],
      "source": [
        "all_w_alpha_9"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "local-department",
      "metadata": {
        "id": "local-department"
      },
      "source": [
        "> We move now to the case $\\alpha = 1/9.4$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "extensive-structure",
      "metadata": {
        "id": "extensive-structure"
      },
      "outputs": [],
      "source": [
        "all_w_alpha_94, all_f_alpha_94, _ = gradient_descent_ex_1_1(1 / 9.4, 1e-5, np.array([-3.0, -3.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wicked-robin",
      "metadata": {
        "id": "wicked-robin"
      },
      "outputs": [],
      "source": [
        "all_f_alpha_94"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "elementary-filling",
      "metadata": {
        "id": "elementary-filling"
      },
      "source": [
        "> We can query how many iterations were required to reach convergence: around 100 iterations in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "racial-number",
      "metadata": {
        "id": "racial-number"
      },
      "outputs": [],
      "source": [
        "all_f_alpha_94.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "committed-filter",
      "metadata": {
        "id": "committed-filter"
      },
      "source": [
        "> Let us now try $\\alpha = 1/18$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "welsh-large",
      "metadata": {
        "id": "welsh-large"
      },
      "outputs": [],
      "source": [
        "all_w_alpha_18, all_f_alpha_18, _ = gradient_descent_ex_1_1(1 / 18, 1e-5, np.array([-3.0, -3.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ethical-restoration",
      "metadata": {
        "id": "ethical-restoration"
      },
      "outputs": [],
      "source": [
        "all_f_alpha_18"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chief-soundtrack",
      "metadata": {
        "id": "chief-soundtrack"
      },
      "source": [
        "> We can query how many iterations were required to reach convergence: around 50 iterations in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "typical-kitty",
      "metadata": {
        "id": "typical-kitty"
      },
      "outputs": [],
      "source": [
        "all_f_alpha_18.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecological-cradle",
      "metadata": {
        "id": "ecological-cradle"
      },
      "source": [
        "> Finally, we try with $\\alpha = 1/36$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "approximate-coverage",
      "metadata": {
        "id": "approximate-coverage"
      },
      "outputs": [],
      "source": [
        "all_w_alpha_36, all_f_alpha_36, _ = gradient_descent_ex_1_1(1 / 36, 1e-5, np.array([-3.0, -3.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "separated-press",
      "metadata": {
        "id": "separated-press"
      },
      "outputs": [],
      "source": [
        "all_f_alpha_36"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "owned-bracelet",
      "metadata": {
        "id": "owned-bracelet"
      },
      "source": [
        "> We also have convergence for $\\alpha = 1/36$, again with around 100 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "suspected-voice",
      "metadata": {
        "id": "suspected-voice"
      },
      "outputs": [],
      "source": [
        "all_f_alpha_36.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "another-claim",
      "metadata": {
        "id": "another-claim"
      },
      "source": [
        "> We prepare a four plots of the optimization variable iteration over the contour plot of $f$, each corresponding to a value of $\\alpha$, discarding the case $\\alpha = 1$ which did not converge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cheap-tractor",
      "metadata": {
        "id": "cheap-tractor"
      },
      "outputs": [],
      "source": [
        "fig = plotly.subplots.make_subplots(rows=2, cols=2)\n",
        "denominators = [9, 9.4, 18, 36]\n",
        "rows = [1, 1, 2, 2]\n",
        "cols = [1, 2, 1, 2]\n",
        "all_w_alpha = [all_w_alpha_9, all_w_alpha_94, all_w_alpha_18, all_w_alpha_36]\n",
        "for alpha_index in range(4):\n",
        "    fig.add_contour(\n",
        "        x=w_component_0, y=w_component_1, z=f_w, opacity=0.5, showscale=False,\n",
        "        row=rows[alpha_index], col=cols[alpha_index]\n",
        "    )\n",
        "    fig.add_scatter(\n",
        "        x=all_w_alpha[alpha_index][:, 0], y=all_w_alpha[alpha_index][:, 1],\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[alpha_index], size=10),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[alpha_index], width=2),\n",
        "        mode=\"lines+markers\", name=\"Alpha = 1/\" + str(denominators[alpha_index]),\n",
        "        row=rows[alpha_index], col=cols[alpha_index]\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=\"Booth function - optimization variable iterations over contour plot - different step lengths\",\n",
        "    width=768, height=768, autosize=False\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nuclear-formation",
      "metadata": {
        "id": "nuclear-formation"
      },
      "source": [
        "> These plots allow to have a graphical summary of the behaviors that we encountered when varying $\\alpha$:\n",
        "> * we have not shown the case $\\alpha = 1$, which is the diverging case; \n",
        "> * *top left* corresponds to $\\alpha = 1/9$, which is a case in which oscillation do not reduce, and the iterations do not converge;\n",
        "> * *top right* corresponds to $\\alpha = 1/9.4$, which is a case of slow and oscillatory convergence;\n",
        "> * *bottom left* corresponds to $\\alpha = 1/18$, which is a case is characterized by the optimal step length;\n",
        "> * *bottom right* corresponds to $\\alpha = 1/36$, which is a case of slow and non-oscillatory convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "periodic-geometry",
      "metadata": {
        "id": "periodic-geometry"
      },
      "source": [
        "> We next plot the history of the error on the function value for different step lengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sought-gabriel",
      "metadata": {
        "id": "sought-gabriel"
      },
      "outputs": [],
      "source": [
        "fig = plotly.subplots.make_subplots(rows=2, cols=2)\n",
        "all_f_alpha = [all_f_alpha_9, all_f_alpha_94, all_f_alpha_18, all_f_alpha_36]\n",
        "for alpha_index in range(4):\n",
        "    fig.add_scatter(\n",
        "        x=np.arange(all_f_alpha[alpha_index].shape[0]), y=all_f_alpha[alpha_index],\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[alpha_index], size=10),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[alpha_index], width=2),\n",
        "        mode=\"lines+markers\", name=\"Alpha = 1/\" + str(denominators[alpha_index]),\n",
        "        row=rows[alpha_index], col=cols[alpha_index]\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=\"Booth function - error on the function value - different step lengths\",\n",
        "    width=768, height=768, autosize=False\n",
        ")\n",
        "fig.update_xaxes(range=[0, 110])\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "owned-exhibit",
      "metadata": {
        "id": "owned-exhibit"
      },
      "source": [
        "> Note how the slow converging cases ($\\alpha = 1/9.4$ *top right* and $\\alpha = 1/36$ *bottom right*), even though characterized by a similar number of iterations, have a very different convergence history: the cost function in the *top right* decreases by the same rate throughout the optimization iterations, while the cost function in the *bottom* right* has a very fast decrease during the first few iterations followed by a slower decrease.\n",
        ">\n",
        "> The case $\\alpha = 1/18$ is characterized by the fastest decrease in the very first iteration, and followed by a slow decrease in the subsequent iterations. For a generic function, is there a general way to determine good choices of $\\alpha$ (i.e., exclude values that may cause divergence) and have an idea of how many iterations will be required to get convergence? We will discuss these points later on the lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "certain-stretch",
      "metadata": {
        "id": "certain-stretch"
      },
      "source": [
        "## Exercise 1.2\n",
        "Let $\\boldsymbol{w} \\in \\mathbb{R}^2$. Consider the following function\n",
        "$$f(\\boldsymbol{w}) = e^{w^{(0)} + 3 w^{(1)} - 0.1} + e^{w^{(0)} - 3 w^{(1)} - 0.1} + e^{- w^{(0)} - 0.1}.$$\n",
        "\n",
        "1. Draw a surface plot and a contour plot of the function $f$ on the square domain $[-2, 2]^2$.\n",
        "\n",
        "*Solution*:\n",
        "> As in the previous exercise we define a uniform subdivision in each coordinate direction, made up of 100 points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "musical-animal",
      "metadata": {
        "id": "musical-animal"
      },
      "outputs": [],
      "source": [
        "domain_component_0 = [-2, 2]\n",
        "domain_component_1 = [-2, 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unnecessary-board",
      "metadata": {
        "id": "unnecessary-board"
      },
      "outputs": [],
      "source": [
        "w_component_0 = np.linspace(domain_component_0[0], domain_component_0[1], 100)\n",
        "w_component_1 = np.linspace(domain_component_1[0], domain_component_1[1], 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "technical-graham",
      "metadata": {
        "id": "technical-graham"
      },
      "source": [
        "> We then evaluate the function $f$ at every $\\boldsymbol{w}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "portable-eating",
      "metadata": {
        "id": "portable-eating"
      },
      "outputs": [],
      "source": [
        "def f_ex_1_2(w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate f(w).\"\"\"\n",
        "    return np.exp(w[0] + 3 * w[1] - 0.1) + np.exp(w[0] - 3 * w[1] - 0.1) + np.exp(- w[0] - 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "broadband-premium",
      "metadata": {
        "id": "broadband-premium"
      },
      "outputs": [],
      "source": [
        "f_w = np.zeros((len(w_component_0), len(w_component_1)))\n",
        "for i in range(f_w.shape[0]):\n",
        "    for j in range(f_w.shape[1]):\n",
        "        f_w[j, i] = f_ex_1_2([w_component_0[i], w_component_1[j]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accompanied-limit",
      "metadata": {
        "id": "accompanied-limit"
      },
      "source": [
        "> When preparing a contour plot with `plotly` we notice that the resulting plot is not very informative because most of the figure is associated to a single contour level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sunset-recommendation",
      "metadata": {
        "id": "sunset-recommendation"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Contour(x=w_component_0, y=w_component_1, z=f_w)])\n",
        "fig.update_layout(title=\"Function exercise 1.2 - contour plot\", width=512, height=512, autosize=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "indoor-grade",
      "metadata": {
        "id": "indoor-grade"
      },
      "source": [
        "> This is again due to the fact that the function $f$ spans several order of magnitude. As in the previous exercise, we can use a log scale to improve the visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dangerous-horse",
      "metadata": {
        "id": "dangerous-horse"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Contour(\n",
        "    x=w_component_0, y=w_component_1, z=np.log10(f_w),\n",
        "    hovertemplate=\"x: %{x:.2f}<br>y: %{y:.2f}<br>z: 10^%{z:.2f} = %{customdata}\", customdata=f_w,\n",
        "    colorbar=dict(tickprefix=\"10^\")\n",
        ")])\n",
        "fig.update_layout(title=\"Function exercise 1.2 - contour plot (log scale)\", width=512, height=512, autosize=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "flying-villa",
      "metadata": {
        "id": "flying-villa"
      },
      "source": [
        "2. Compute the gradient $\\nabla f$ and determine the global minimum of the function $f$.\n",
        "\n",
        "*Solution*:\n",
        "> We compute the partial derivatives of $f(\\boldsymbol{w}) = e^{w^{(0)} + 3 w^{(1)} - 0.1} + e^{w^{(0)} - 3 w^{(1)} - 0.1} + e^{- w^{(0)} - 0.1}$ to obtain the gradient\n",
        "\\begin{equation*}\n",
        "\\nabla f(\\boldsymbol{w}) = \\begin{bmatrix}\n",
        "e^{w^{(0)} + 3 w^{(1)} - 0.1} + e^{w^{(0)} - 3 w^{(1)} - 0.1} - e^{- w^{(0)} - 0.1}\\\\\n",
        "3 e^{w^{(0)} + 3 w^{(1)} - 0.1} - 3 e^{w^{(0)} - 3 w^{(1)} - 0.1}\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "Solving the equation $\\nabla f(\\boldsymbol{w}) = 0$ is more complicated than the previous exercise, but we are still in a favorable situation in which, with a bit of algebra, we can solve this problem analytically. Let us start from the second equation\n",
        "$$3 e^{w^{(0)} + 3 w^{(1)} - 0.1} = 3 e^{w^{(0)} - 3 w^{(1)} - 0.1}.$$\n",
        "Dividing by 3 and taking the $\\ln$ on both sides we get\n",
        "$$w^{(0)} + 3 w^{(1)} - 0.1 = w^{(0)} - 3 w^{(1)} - 0.1$$\n",
        "from which we obtain ${w^*}^{(1)}$ = 0. Substituting this value into the first equation gives\n",
        "$$e^{w^{(0)} - 0.1} + e^{w^{(0)} - 0.1} - e^{- w^{(0)} - 0.1} = 0$$\n",
        "which can be equivalently rewritten as\n",
        "$$2 e^{w^{(0)}} e^{- 0.1} - e^{- w^{(0)}}e^{- 0.1} = 0$$\n",
        "which divided by $e^{- 0.1}$ gives\n",
        "$$2 e^{w^{(0)}} - e^{- w^{(0)}} = 0.$$\n",
        "Finally, multiplying both sides by $e^{w^{(0)}}$ we end up with\n",
        "$$2 e^{2 w^{(0)}} - 1 = 0, \\qquad\\text{that is}\\qquad e^{2 w^{(0)}} = \\frac{1}{2},$$\n",
        "from which we obtain ${w^*}^{(0)} = - \\frac{\\ln 2}{2}$. In conclusion, the only stationary point is\n",
        "$$ \\boldsymbol{w}^* = \\left(- \\frac{\\ln 2}{2}, 0\\right).$$\n",
        ">\n",
        "> In order to determine if $\\boldsymbol{w}^*$ is a minimum (and therefore a global minimum, being the only stationary point), we compute the hessian matrix\n",
        "\\begin{equation*}\n",
        "\\nabla^2 f(\\boldsymbol{w}) = \\begin{bmatrix}\n",
        "e^{w^{(0)} + 3 w^{(1)} - 0.1} + e^{w^{(0)} - 3 w^{(1)} - 0.1} + e^{- w^{(0)} - 0.1}\n",
        "&3 e^{w^{(0)} + 3 w^{(1)} - 0.1} - 3 e^{w^{(0)} - 3 w^{(1)} - 0.1}\\\\\n",
        "3 e^{w^{(0)} + 3 w^{(1)} - 0.1} - 3 e^{w^{(0)} - 3 w^{(1)} - 0.1}\n",
        "&9 e^{w^{(0)} + 3 w^{(1)} - 0.1} + 9 e^{w^{(0)} - 3 w^{(1)} - 0.1}\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "> We notice that $\\nabla^2 f(\\boldsymbol{w}^*)$ is of the following form\n",
        "\\begin{equation*}\n",
        "\\nabla^2 f(\\boldsymbol{w}^*) = \\begin{bmatrix}\n",
        "e^{{w^*}^{(0)} + 3 {w^*}^{(1)} - 0.1} + e^{{w^*}^{(0)} - 3 {w^*}^{(1)} - 0.1} + e^{- {w^*}^{(0)} - 0.1}\n",
        "&0\\\\\n",
        "0\n",
        "&9 e^{{w^*}^{(0)} + 3 {w^*}^{(1)} - 0.1} + 9 e^{{w^*}^{(0)} - 3 {w^*}^{(1)} - 0.1}\n",
        "\\end{bmatrix}\n",
        "\\end{equation*}\n",
        "because the off-diagonal terms correspond to the second equation of the first order optimality condition. This is a diagonal matrix, so its eigenvalue are the diagonal elements. Furthermore, since such elements are positive (being the sum of exponential functions), we conclude that $\\nabla^2 f(\\boldsymbol{w}^*)$ is positive definite, and thus $\\boldsymbol{w}^*$ is a global minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "passing-livestock",
      "metadata": {
        "id": "passing-livestock"
      },
      "source": [
        "> We conclude this point by:\n",
        "> * defining $\\boldsymbol{w}^*$, and determining the optimal value $f(\\boldsymbol{w}^*)$,\n",
        "> * implementing a Python function for $\\nabla f$ as well, checking that it is implemented correctly by evaluating $\\nabla f(\\boldsymbol{w}^*)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "literary-edgar",
      "metadata": {
        "id": "literary-edgar"
      },
      "outputs": [],
      "source": [
        "w_star = np.array([- np.log(2) / 2, 0])\n",
        "w_star"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "final-clear",
      "metadata": {
        "id": "final-clear"
      },
      "outputs": [],
      "source": [
        "f_star = f_ex_1_2(w_star)\n",
        "f_star"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "animated-sailing",
      "metadata": {
        "id": "animated-sailing"
      },
      "outputs": [],
      "source": [
        "def grad_f_ex_1_2(w: np.ndarray) -> np.ndarray:\n",
        "    r\"\"\"Evaluate \\nabla f(w).\"\"\"\n",
        "    return np.array([\n",
        "        np.exp(w[0] + 3 * w[1] - 0.1) + np.exp(w[0] - 3 * w[1] - 0.1) - np.exp(- w[0] - 0.1),\n",
        "        3 * np.exp(w[0] + 3 * w[1] - 0.1) - 3 * np.exp(w[0] - 3 * w[1] - 0.1)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vocal-buyer",
      "metadata": {
        "id": "vocal-buyer"
      },
      "outputs": [],
      "source": [
        "grad_f_ex_1_2(w_star)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "preceding-crowd",
      "metadata": {
        "id": "preceding-crowd"
      },
      "source": [
        "3. Implement the gradient descent method with exact line search in a Python function. Use the stopping criterion based on the error of the cost. Such function should:\n",
        "   * take as input the tolerance $\\varepsilon$ for the stopping criterion and the initial condition $\\boldsymbol{w}_{0}$;\n",
        "   * return as outputs the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$, the corresponding function values $\\{f(\\boldsymbol{w}_k)\\}_k$ and gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$, and the values of the step lengths $\\{\\alpha_k\\}_k$.\n",
        " \n",
        "*Solution*:\n",
        "> We will provide a very simple (and inefficient!) implementation of the exact line search. Denote by \n",
        "$$E_k(a) = f(\\boldsymbol{w}_k + a \\boldsymbol{g}_k),$$\n",
        "where $\\boldsymbol{g}_k = - \\nabla f(\\boldsymbol{w}_k)$ is the current descent direction.\n",
        "A very simple (but expensive) way to find the minimum of $E(a)$ is to evaluate it at several points, and then pick the point which has the minimum value. This is not feasible in practical problems (especially in more than one dimension), but it will suffice for the goals of our exercise.\n",
        "> In particular, we will pick 1001 equispaced points in the interval $[0, 1]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "magnetic-satellite",
      "metadata": {
        "id": "magnetic-satellite"
      },
      "outputs": [],
      "source": [
        "alpha_choices = np.linspace(0, 1, 1001, endpoint=True)\n",
        "alpha_choices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "freelance-usage",
      "metadata": {
        "id": "freelance-usage"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_exact_line_search_ex_1_2(epsilon: float, w_0: np.ndarray) -> typing.Tuple[\n",
        "        np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run the gradient descent method with exact line search.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the cost.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    1d numpy array\n",
        "        history of the step lengths selected by the exact line search.\n",
        "    \"\"\"\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_f = [f_ex_1_2(w_0)]\n",
        "    all_grad_f = [grad_f_ex_1_2(w_0)]\n",
        "    all_alpha = []\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Use the error on the cost to determine when the while loop should stop.\n",
        "    # Since we have stored the value of f(w^*) in the variable f_star, we can use it to\n",
        "    # compute f(w_k) - f^*\n",
        "    while all_f[k] - f_star > epsilon:\n",
        "        w_k = all_w[k]\n",
        "        grad_f_k = all_grad_f[k]\n",
        "        g_k = - grad_f_k\n",
        "\n",
        "        # Carry out an exact line search\n",
        "        E_choices = [f_ex_1_2(w_k + a * g_k) for a in alpha_choices]\n",
        "        min_choice = np.argmin(E_choices)\n",
        "        alpha_k = alpha_choices[min_choice]\n",
        "\n",
        "        # Bail out if the descent condition is not satisfied\n",
        "        if alpha_k == 0:\n",
        "            print(\"WARNING: descent conditions is not satisfied\")\n",
        "            break\n",
        "\n",
        "        # Compute w_{k+1}\n",
        "        w_k_plus_1 = w_k + alpha_k * g_k\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f_ex_1_2(w_k_plus_1))\n",
        "        all_grad_f.append(grad_f_ex_1_2(w_k_plus_1))\n",
        "        all_alpha.append(alpha_k)\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_f), np.array(all_grad_f), np.array(all_alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hawaiian-credit",
      "metadata": {
        "id": "hawaiian-credit"
      },
      "source": [
        "4. Choose $\\varepsilon = 10^{-5}$ and $\\boldsymbol{w}_0 = (-1, -1.9)$, and run the gradient descent method with exact line search. Visualize:\n",
        "   * the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$ on a contour plot of $f$;\n",
        "   * a semilogarithimic plot of error in the function value $\\{f(\\boldsymbol{w}_k) - f(\\boldsymbol{w}^*)\\}_k$ versus the iteration counter $k$;\n",
        "   * a video of the functions $E_k(a)$ as $k$ progresses.\n",
        "\n",
        "*Solution*:\n",
        "> We run the function we have implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fewer-inquiry",
      "metadata": {
        "id": "fewer-inquiry"
      },
      "outputs": [],
      "source": [
        "all_w_exact, all_f_exact, all_grad_f_exact, all_alpha_exact = gradient_descent_exact_line_search_ex_1_2(\n",
        "    1e-10, np.array([-1.0, 1.9]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imposed-island",
      "metadata": {
        "id": "imposed-island"
      },
      "source": [
        "> We next create the required contour plot, with overlayed optimization variable iterations. Note how with three iterations we get very close to the minimum from a graphical point of view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "contrary-deployment",
      "metadata": {
        "id": "contrary-deployment"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Contour(\n",
        "    x=w_component_0, y=w_component_1, z=np.log10(f_w),\n",
        "    hovertemplate=\"x: %{x:.2f}<br>y: %{y:.2f}<br>z: 10^%{z:.2f} = %{customdata}\", customdata=f_w,\n",
        "    colorbar=dict(tickprefix=\"10^\")\n",
        ")])\n",
        "fig.add_scatter(\n",
        "    x=all_w_exact[:, 0], y=all_w_exact[:, 1],\n",
        "    marker=dict(color=\"black\", size=10),\n",
        "    line=dict(color=\"black\", width=2),\n",
        "    mode=\"lines+markers\"\n",
        ")\n",
        "fig.update_layout(\n",
        "    title=\"Function exercise 1.2 - optimization variable iterations over contour plot (log scale)\",\n",
        "    width=512, height=512, autosize=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "substantial-imperial",
      "metadata": {
        "id": "substantial-imperial"
      },
      "source": [
        "> The semilog plot of the error on the function value is very similar to the one in the previous exercise. Also in this case we may notice that the the decrease of the function value has a linear trend when the horizontal axis is shown in logarithimic scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beautiful-intelligence",
      "metadata": {
        "id": "beautiful-intelligence"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=go.Scatter(x=np.arange(all_f_exact.shape[0]), y=all_f_exact - f_star))\n",
        "fig.update_layout(title=\"Function exercise 1.2 - error on the function value - semilog plot\")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "greek-patrick",
      "metadata": {
        "id": "greek-patrick"
      },
      "source": [
        "> Finally, we create an animation of the function $E_k(a)$, where each slide of the animation corresponds to a different value of $k$. We can visually confirm that the selected $\\alpha_k$ is the minimum of $E_k(a)$. Notice how while getting closer to the minimum point the range of variation for $E_k(a)$ decreases considerably when varying $a$ in [0, 1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "honey-wrong",
      "metadata": {
        "id": "honey-wrong"
      },
      "outputs": [],
      "source": [
        "K = all_alpha_exact.shape[0]\n",
        "\n",
        "fig = go.Figure()\n",
        "slides = []\n",
        "for k in range(K):\n",
        "    w_k = all_w_exact[k]\n",
        "    grad_f_k = all_grad_f_exact[k]\n",
        "    g_k = - grad_f_k\n",
        "    alpha_k = all_alpha_exact[k]\n",
        "\n",
        "    # Evaluate E_k\n",
        "    E_0 = f_ex_1_2(w_k)\n",
        "    E_alpha_k = f_ex_1_2(w_k + alpha_k * g_k)\n",
        "    E_choices = [f_ex_1_2(w_k + a * g_k) for a in alpha_choices]\n",
        "\n",
        "    # Add line to plot\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=alpha_choices, y=E_choices, visible=False,\n",
        "                   line=dict(color=\"blue\"), name=\"E_k\"))\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[alpha_k], y=[E_alpha_k], visible=False,\n",
        "                   marker=dict(color=\"red\"), name=\"alpha_k\"))\n",
        "\n",
        "    # Add slider tick\n",
        "    slide = {\n",
        "        \"method\": \"update\",\n",
        "        \"args\": [\n",
        "            {\"visible\": [False] * (2 * K)},\n",
        "            {\"title\": \"k = \" + str(k),\n",
        "             \"yaxis.range\": [E_alpha_k - 0.1 * (E_0 - E_alpha_k), E_alpha_k + 2 * (E_0 - E_alpha_k)]}\n",
        "        ]\n",
        "    }\n",
        "    slide[\"args\"][0][\"visible\"][2 * k] = True\n",
        "    slide[\"args\"][0][\"visible\"][2 * k + 1] = True\n",
        "    slides.append(slide)\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"k = 0\", yaxis_range=[2, 10],\n",
        "    sliders=[dict(steps=slides)])\n",
        "fig.data[0].visible = True\n",
        "fig.data[1].visible = True\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "brave-humanitarian",
      "metadata": {
        "id": "brave-humanitarian"
      },
      "source": [
        "5. Implement the gradient descent method with backtracking line search in a Python function. Use the stopping criterion based on the error of the cost. Such function should:\n",
        "   * take as input the constants $\\alpha$, $c_1$ and $c_2$ of the backtracking algorithm, the tolerance $\\varepsilon$ for the stopping criterion and the initial condition $\\boldsymbol{w}_{0}$;\n",
        "   * return as outputs the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$, the corresponding function values $\\{f(\\boldsymbol{w}_k)\\}_k$ and gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$, and the values of the step lengths $\\{\\alpha_k\\}_k$.\n",
        " \n",
        "*Solution*:\n",
        "> We closely follow the previous implementation, and change only the line search procedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rapid-budget",
      "metadata": {
        "id": "rapid-budget"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_backtracking_line_search_ex_1_2(\n",
        "    alpha: float, c_1: float, c_2: float, epsilon: float, w_0: np.ndarray\n",
        ") -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run the gradient descent method with backtracking line search.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : float\n",
        "        initial step length.\n",
        "    c_1, c_2 : float\n",
        "        constants of the backtracking algorithm.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the cost.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    1d numpy array\n",
        "        history of the step lengths selected by the backtracking line search.\n",
        "    \"\"\"\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_f = [f_ex_1_2(w_0)]\n",
        "    all_grad_f = [grad_f_ex_1_2(w_0)]\n",
        "    all_alpha = []\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Use the error on the cost to determine when the while loop should stop.\n",
        "    while all_f[k] - f_star > epsilon:\n",
        "        w_k = all_w[k]\n",
        "        f_k = all_f[k]\n",
        "        grad_f_k = all_grad_f[k]\n",
        "        norm_grad_f_k = np.linalg.norm(grad_f_k)\n",
        "\n",
        "        # Carry out a backtracking line search\n",
        "        alpha_k = alpha\n",
        "        while f_ex_1_2(w_k - alpha_k * grad_f_k) > f_k - c_1 * alpha_k * norm_grad_f_k**2:\n",
        "            alpha_k = c_2 * alpha_k\n",
        "\n",
        "        # Compute w_{k+1}\n",
        "        w_k_plus_1 = w_k - alpha_k * grad_f_k\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f_ex_1_2(w_k_plus_1))\n",
        "        all_grad_f.append(grad_f_ex_1_2(w_k_plus_1))\n",
        "        all_alpha.append(alpha_k)\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_f), np.array(all_grad_f), np.array(all_alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "answering-caribbean",
      "metadata": {
        "id": "answering-caribbean"
      },
      "source": [
        "6. Choose $\\alpha = 1$, $c_1 = 0.1$, $c_2 = 0.7$, $\\varepsilon = 10^{-5}$ and $\\boldsymbol{w}_0 = (-1, -1.9)$, and run the gradient descent method with backtracking line search. Visualize:\n",
        "   * the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$ on a contour plot of $f$;\n",
        "   * a semilogarithimic plot of error in the function value $\\{f(\\boldsymbol{w}_k) - f(\\boldsymbol{w}^*)\\}_k$ versus the iteration counter $k$;\n",
        "   * a video of the functions $E_k(a)$ as $k$ progresses.\n",
        "\n",
        "*Solution*:\n",
        "> We use our implementation as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "negative-prisoner",
      "metadata": {
        "id": "negative-prisoner"
      },
      "outputs": [],
      "source": [
        "all_w_backtracking, all_f_backtracking, all_grad_f_backtracking, all_alpha_backtracking = (\n",
        "    gradient_descent_backtracking_line_search_ex_1_2(\n",
        "        1, 0.1, 0.7, 1e-10, np.array([-1.0, 1.9])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "finished-bangkok",
      "metadata": {
        "id": "finished-bangkok"
      },
      "source": [
        "> We first visually compare the value $\\alpha_k$ selected by backtracking with the minimum of $E_k(a)$. This gives us an idea of how much the step length $\\alpha_k$ selected by the inexact line search is far from the optimal one which would have been selected by the exact line search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "blind-diameter",
      "metadata": {
        "id": "blind-diameter"
      },
      "outputs": [],
      "source": [
        "K = all_alpha_backtracking.shape[0]\n",
        "\n",
        "fig = go.Figure()\n",
        "slides = []\n",
        "for k in range(K):\n",
        "    w_k = all_w_backtracking[k]\n",
        "    grad_f_k = all_grad_f_backtracking[k]\n",
        "    g_k = - grad_f_k\n",
        "    alpha_k = all_alpha_backtracking[k]\n",
        "\n",
        "    # Determine what would have been the optimal step length\n",
        "    E_choices = [f_ex_1_2(w_k + a * g_k) for a in alpha_choices]\n",
        "    min_choice = np.argmin(E_choices)\n",
        "    optimal_alpha_k = alpha_choices[min_choice]\n",
        "\n",
        "    # Evaluate E_k\n",
        "    E_0 = f_ex_1_2(w_k)\n",
        "    E_alpha_k = f_ex_1_2(w_k + alpha_k * g_k)\n",
        "    E_optimal_alpha_k = f_ex_1_2(w_k + optimal_alpha_k * g_k)\n",
        "\n",
        "    # Add line to plot\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=alpha_choices, y=E_choices, visible=False,\n",
        "                   line=dict(color=\"blue\"), name=\"E_k\"))\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[optimal_alpha_k], y=[E_optimal_alpha_k], visible=False,\n",
        "                   marker=dict(color=\"red\"), name=\"optimal alpha_k\"))\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=[alpha_k], y=[E_alpha_k], visible=False,\n",
        "                   marker=dict(color=\"orange\"), name=\"backtracking alpha_k\"))\n",
        "\n",
        "    # Add slider tick\n",
        "    slide = {\n",
        "        \"method\": \"update\",\n",
        "        \"args\": [\n",
        "            {\"visible\": [False] * (3 * K)},\n",
        "            {\"title\": \"k = \" + str(k),\n",
        "             \"yaxis.range\": [\n",
        "                 E_optimal_alpha_k - 0.1 * (E_0 - E_optimal_alpha_k),\n",
        "                 E_optimal_alpha_k + 2 * (E_0 - E_optimal_alpha_k)\n",
        "            ]}\n",
        "        ]\n",
        "    }\n",
        "    slide[\"args\"][0][\"visible\"][3 * k] = True\n",
        "    slide[\"args\"][0][\"visible\"][3 * k + 1] = True\n",
        "    slide[\"args\"][0][\"visible\"][3 * k + 2] = True\n",
        "    slides.append(slide)\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"k = 0\", yaxis_range=[2, 10],\n",
        "    sliders=[dict(steps=slides)])\n",
        "fig.data[0].visible = True\n",
        "fig.data[1].visible = True\n",
        "fig.data[2].visible = True\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "thousand-liberal",
      "metadata": {
        "id": "thousand-liberal"
      },
      "source": [
        "> We may notice that the value $\\alpha_k$ selected by the backtracking consistently overestimates the optimal step length. Therefore, the steps taken by the gradient descent with backtracking are too long, as can see on a contour plot, with overlayed optimization variable iterations comparing the backtracking and the exact iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "meaning-burner",
      "metadata": {
        "id": "meaning-burner"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Contour(\n",
        "    x=w_component_0, y=w_component_1, z=np.log10(f_w),\n",
        "    hovertemplate=\"x: %{x:.2f}<br>y: %{y:.2f}<br>z: 10^%{z:.2f} = %{customdata}\", customdata=f_w,\n",
        "    showscale=False\n",
        ")])\n",
        "fig.add_scatter(\n",
        "    x=all_w_exact[:, 0], y=all_w_exact[:, 1],\n",
        "    marker=dict(color=\"red\", size=10),\n",
        "    line=dict(color=\"red\", width=2),\n",
        "    mode=\"lines+markers\", name=\"Exact line search\"\n",
        ")\n",
        "fig.add_scatter(\n",
        "    x=all_w_backtracking[:, 0], y=all_w_backtracking[:, 1],\n",
        "    marker=dict(color=\"orange\", size=10),\n",
        "    line=dict(color=\"orange\", width=2),\n",
        "    mode=\"lines+markers\", name=\"Backtracking line search\"\n",
        ")\n",
        "fig.update_layout(\n",
        "    title=\"Function exercise 1.2 - optimization variable iterations over contour plot (log scale)\",\n",
        "    width=612, height=512, autosize=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oriental-elements",
      "metadata": {
        "id": "oriental-elements"
      },
      "source": [
        "> This overshooting behavior causes the gradient method with backtracking to take a few more iterations than the one with exact line search to convrge to the same tolerance on the error in the cost function. However, from the next plot we notice that even with backtracking we still have a linear convergence when plotting the error in the cost function in a semilog plot. This indicates that, while the backtracking procedure is certainly inferior to the exact line search, it does not deteriorate the convergence speed of the gradient method. This is a welcome news, because in practice we never use the exact line search (because it is too costly! Count how many function evaluations we were required to spend at each iteration $k$!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "proved-innocent",
      "metadata": {
        "id": "proved-innocent"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "fig.add_scatter(\n",
        "    x=np.arange(all_f_exact.shape[0]), y=all_f_exact - f_star,\n",
        "    line=dict(color=\"red\"), name=\"Exact line search\"\n",
        ")\n",
        "fig.add_scatter(\n",
        "    x=np.arange(all_f_backtracking.shape[0]), y=all_f_backtracking - f_star,\n",
        "    line=dict(color=\"orange\"), name=\"Backtracking line search\"\n",
        ")\n",
        "fig.update_layout(title=\"Function exercise 1.2 - error on the function value - semilog plot\")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "complex-customer",
      "metadata": {
        "id": "complex-customer"
      },
      "source": [
        "## Exercise 1.3\n",
        "In this exercise we consider a binary classification via logistic regression.\n",
        "\n",
        "1. Generate a dataset of $m = 100$ points as follows: 50 points should be uniformly distributed in the interval $[-1, -0.5]$, and 50 points should be uniformly distributed in the interval $[0.5, 1]$. The corresponding label should be 0 for negative points, and 1 positive points.\n",
        "\n",
        "*Solution*:\n",
        "> We use [`numpy.random.uniform`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html) to draw samples from a uniform distribution, and [`numpy.hstack`](https://numpy.org/doc/stable/reference/generated/numpy.hstack.html) to join the two generated datasets.\n",
        "> We already introduce here a few *best practices* that will be helpful when implementing more complex machine learning models:\n",
        "> * in order to ensure reproducibility (between different runs on the same laptop, or across different devices) we set the seed at the beginning of every cell that generates random numbers using [`numpy.random.seed`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.seed.html)\n",
        "> * after stacking, the generated dataset would have the first 50 entries with negative numbers, and the last 50 entries with positive numbers. We then reshuffle the order of the entries with [`numpy.random.shuffle`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.shuffle.html). We will see that this can be important when creating a validation set in more complex machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "relative-friend",
      "metadata": {
        "id": "relative-friend"
      },
      "outputs": [],
      "source": [
        "np.random.seed(13)\n",
        "x = np.hstack((np.random.uniform(-1, -0.5, 50), np.random.uniform(0.5, 1, 50)))\n",
        "np.random.shuffle(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "regulated-california",
      "metadata": {
        "id": "regulated-california"
      },
      "outputs": [],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "developing-fruit",
      "metadata": {
        "id": "developing-fruit"
      },
      "source": [
        "> We then assign label 0 if the corresponding point is negative, label 1 otherwise. This can be easily done by checking the condition `x > 0`, which returns a boolean value. Since the dataset should contain floating point numbers, we convert the boolean value `True` to 1 and `False` to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nominated-friday",
      "metadata": {
        "id": "nominated-friday"
      },
      "outputs": [],
      "source": [
        "y = (x > 0).astype(np.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cognitive-massage",
      "metadata": {
        "id": "cognitive-massage"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "historical-music",
      "metadata": {
        "id": "historical-music"
      },
      "source": [
        "2. Implement the prediction function $\\hat{y}(x; \\boldsymbol{w}) = \\sigma(w^{0} x + w^{1})$ associated to a logistic regression, where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function.\n",
        "\n",
        "*Solution*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "continent-hindu",
      "metadata": {
        "id": "continent-hindu"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z: float) -> float:\n",
        "    \"\"\"Evaluate the sigmoid function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "appreciated-future",
      "metadata": {
        "id": "appreciated-future"
      },
      "outputs": [],
      "source": [
        "def y_hat(x_j: float, w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate the prediction function associated to a logistic regression.\"\"\"\n",
        "    return sigmoid(w[0] * x_j + w[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "moderate-property",
      "metadata": {
        "id": "moderate-property"
      },
      "source": [
        "3. Implement the cross entropy loss $\\ell(x, y; \\boldsymbol{w}) = - y \\log \\hat{y}(x; \\boldsymbol{w}) - (1 - y) \\log (1 - \\hat{y}(x; \\boldsymbol{w}))$, and the corresponding empirical risk function $f$ associated to the dataset.\n",
        "\n",
        "*Solution*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dried-whale",
      "metadata": {
        "id": "dried-whale"
      },
      "outputs": [],
      "source": [
        "def logistic_loss(x_j: float, y_j: float, w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate the logistic loss.\"\"\"\n",
        "    return - y_j * np.log(y_hat(x_j, w)) - (1 - y_j) * np.log(1 - y_hat(x_j, w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "micro-salon",
      "metadata": {
        "id": "micro-salon"
      },
      "outputs": [],
      "source": [
        "def f_ex_1_3(x: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate the empirical risk.\"\"\"\n",
        "    m = x.shape[0]\n",
        "    return 1 / m * sum(logistic_loss(x_j, y_j, w) for (x_j, y_j) in zip(x, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hindu-species",
      "metadata": {
        "id": "hindu-species"
      },
      "source": [
        "4. Initialize $\\boldsymbol{w}$ to $\\boldsymbol{w}_0 = (1, 0)$. Prepare a plot of the corresponding predictions for 500 equispaced points on $[-1, 1]$.\n",
        "\n",
        "*Solution*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "affecting-oxford",
      "metadata": {
        "id": "affecting-oxford"
      },
      "outputs": [],
      "source": [
        "w_0 = np.array([1, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cathedral-apparatus",
      "metadata": {
        "id": "cathedral-apparatus"
      },
      "outputs": [],
      "source": [
        "x_plot = np.linspace(-1, 1, 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "consecutive-reference",
      "metadata": {
        "id": "consecutive-reference"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "fig.add_scatter(x=x, y=y, marker=dict(color=\"red\", size=10), mode=\"markers\", name=\"Data\")\n",
        "y_hat_plot = [y_hat(x_p, w_0) for x_p in x_plot]\n",
        "fig.add_scatter(x=x_plot, y=y_hat_plot, marker=dict(color=\"blue\", size=5), mode=\"markers\", name=\"Prediction\")\n",
        "fig.update_layout(title=\"Logistic regression - dataset (x, y), w = w_0\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "joined-knowing",
      "metadata": {
        "id": "joined-knowing"
      },
      "source": [
        "5. Determine the gradient of the empirical risk $f$.\n",
        "\n",
        "*Solution*:\n",
        "> Recall that $$\\ell(x, y; \\boldsymbol{w}) = - y \\log \\sigma(w^{0} x + w^{1}) - (1 - y) \\log (1 - \\sigma(w^{0} x + w^{1})).$$\n",
        "> When taking the partial derivative w.r.t. $w^{(0)}$ we have\n",
        "$$\\frac{\\partial}{\\partial w^{(0)}} \\ell(x, y; \\boldsymbol{w}) = - y \\frac{1}{\\sigma(w^{0} x + w^{1})} \\sigma'(w^{0} x + w^{1}) x - (1 - y) \\frac{1}{1 - \\sigma(w^{0} x + w^{1})} [- \\sigma'(w^{0} x + w^{1})].$$\n",
        "> Since $\\sigma'(z) = \\sigma(z) (1 - \\sigma(z))$ we end up with\n",
        "$$\\frac{\\partial}{\\partial w^{(0)}} \\ell(x, y; \\boldsymbol{w}) = [- y + \\sigma(w^{0} x + w^{1})] x.$$\n",
        "Similarly\n",
        "$$\\frac{\\partial}{\\partial w^{(1)}} \\ell(x, y; \\boldsymbol{w}) = [- y + \\sigma(w^{0} x + w^{1})].$$\n",
        "Therefore\n",
        "$$\\nabla_\\boldsymbol{w} \\ell(x, y; \\boldsymbol{w}) = (- y + \\sigma(w^{0} x + w^{1})) \\begin{bmatrix}x\\\\1\\end{bmatrix}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "inclusive-leather",
      "metadata": {
        "id": "inclusive-leather"
      },
      "outputs": [],
      "source": [
        "def grad_logistic_loss(x_j: float, y_j: float, w: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Evaluate the gradient of the logistic loss.\"\"\"\n",
        "    return (y_hat(x_j, w) - y_j) * np.array([x_j, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "statutory-recipient",
      "metadata": {
        "id": "statutory-recipient"
      },
      "outputs": [],
      "source": [
        "def grad_f_ex_1_3(x: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Evaluate the gradient of the empirical risk.\"\"\"\n",
        "    m = x.shape[0]\n",
        "    return 1 / m * sum(grad_logistic_loss(x_j, y_j, w) for (x_j, y_j) in zip(x, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "medieval-conservation",
      "metadata": {
        "id": "medieval-conservation"
      },
      "source": [
        "> As the plot above has graphically shown, the choice $\\boldsymbol{w}_0$ is definitely not optimal, since its gradient is not zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "damaged-saudi",
      "metadata": {
        "id": "damaged-saudi"
      },
      "outputs": [],
      "source": [
        "grad_f_ex_1_3(x, y, w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "still-roots",
      "metadata": {
        "id": "still-roots"
      },
      "source": [
        "6. Implement the gradient descent method with constant step size equal to $1/L$, where $L$ is the smoothness constant of the empirical risk.\n",
        "\n",
        "*Solution*:\n",
        "> We have shown in lecture 0 a formula for the smoothness constant of the empirical risk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "seeing-maximum",
      "metadata": {
        "id": "seeing-maximum"
      },
      "outputs": [],
      "source": [
        "def smoothness_constant(x: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate the smoothness constant L of the empirical risk.\"\"\"\n",
        "    m = x.shape[0]\n",
        "    return 0.5 + sum(x**2) / (2 * m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unique-prague",
      "metadata": {
        "id": "unique-prague"
      },
      "outputs": [],
      "source": [
        "smoothness_constant(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "normal-capture",
      "metadata": {
        "id": "normal-capture"
      },
      "source": [
        "> We then implement the gradient method, closely following Exercise 1.1. Note that, compared to that exercise, we change the stopping criterion because we do not know $\\boldsymbol{w}^*$. Here instead we use a stopping criterion based on the norm of the gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prime-zoning",
      "metadata": {
        "id": "prime-zoning"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_ex_1_3(x: np.ndarray, y: np.ndarray, epsilon: float, w_0: np.ndarray) -> typing.Tuple[\n",
        "        np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run the gradient descent method with constant step length.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x, y : np.ndarray\n",
        "        training dataset.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the norm of the gradient of the cost.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    \"\"\"\n",
        "    # Compute the constant step length from the smoothness constant\n",
        "    alpha = 1 / smoothness_constant(x)\n",
        "\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_f = [f_ex_1_3(x, y, w_0)]\n",
        "    all_grad_f = [grad_f_ex_1_3(x, y, w_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # We cannot use here a stopping criterion based on the difference between the function value\n",
        "    # f(w_k) and f(w^*), because we do not know w^*! We use instead a stopping criterion based\n",
        "    # on the norm of the gradient.\n",
        "    # The rest of the implementation is as in Exercise 1.1 (except for changing the function f)\n",
        "    while np.linalg.norm(all_grad_f[k]) > epsilon:\n",
        "        w_k = all_w[k]\n",
        "        grad_f_k = all_grad_f[k]\n",
        "        w_k_plus_1 = w_k - alpha * grad_f_k\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f_ex_1_3(x, y, w_k_plus_1))\n",
        "        all_grad_f.append(grad_f_ex_1_3(x, y, w_k_plus_1))\n",
        "\n",
        "        # Bail out if the descent condition is not satisfied\n",
        "        if all_f[k + 1] >= all_f[k]:\n",
        "            print(\"WARNING: descent conditions is not satisfied\")\n",
        "            break\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "        print(k, np.linalg.norm(all_w[-1]), all_f[-1], np.linalg.norm(all_grad_f[-1]))\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_f), np.array(all_grad_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "possible-reunion",
      "metadata": {
        "id": "possible-reunion"
      },
      "outputs": [],
      "source": [
        "all_w, all_f, all_grad_f = gradient_descent_ex_1_3(x, y, 1e-3, w_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "impossible-preserve",
      "metadata": {
        "id": "impossible-preserve"
      },
      "source": [
        "> The gradient method convergences to the required tolerance after approximately 200 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "therapeutic-helena",
      "metadata": {
        "id": "therapeutic-helena"
      },
      "outputs": [],
      "source": [
        "all_w[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "weighted-prompt",
      "metadata": {
        "id": "weighted-prompt"
      },
      "source": [
        "> Predictions are now more accurate, as the plot below shows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "federal-following",
      "metadata": {
        "id": "federal-following"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "fig.add_scatter(x=x, y=y, marker=dict(color=\"red\", size=10), mode=\"markers\", name=\"Data\")\n",
        "y_hat_plot = [y_hat(x_p, all_w[-1]) for x_p in x_plot]\n",
        "fig.add_scatter(x=x_plot, y=y_hat_plot, marker=dict(color=\"blue\", size=5), mode=\"markers\", name=\"Prediction\")\n",
        "fig.update_layout(title=\"Logistic regression - dataset (x, y), optimal w\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "welsh-craps",
      "metadata": {
        "id": "welsh-craps"
      },
      "source": [
        "> We next plot the progress of the cost function value w.r.t. the iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dense-prize",
      "metadata": {
        "id": "dense-prize"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=go.Scatter(x=np.arange(all_f.shape[0]), y=all_f))\n",
        "fig.update_layout(title=\"Logistic regression - progress of the function value - semilog plot\")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "selective-royal",
      "metadata": {
        "id": "selective-royal"
      },
      "source": [
        "> We finally plot the progress of the norm of the gradient w.r.t. $k$. We notice that the convergence curve is almost horizontal, i.e. the slope of the line is *very low*, which means that very little progress is made at each iteration. Equivalently said, if there was linear convergence the constant $C$ in its definition is very close to 1. (This is actually a case in which the gradient method converges sublinearly, or equivalently said, the constant $C$ is actually equal to $1$, which is not allowed in the definition of linear convergence! This is due to the fact that the logistic regression empirical risk on this dataset is not strongly convex)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "global-grill",
      "metadata": {
        "id": "global-grill"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=go.Scatter(x=np.arange(all_f.shape[0]), y=np.linalg.norm(all_grad_f, axis=1)))\n",
        "fig.update_layout(title=\"Logistic regression - violation of first order optimality conditions - semilog plot\")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "copyrighted-chocolate",
      "metadata": {
        "id": "copyrighted-chocolate"
      },
      "source": [
        "7. Graphically justify why the convergence of the gradient method is so slow.\n",
        "\n",
        "*Solution*:\n",
        "> To get an intuition, we create an animation of the prediction when $\\boldsymbol{w}_k$ changes according to the $k$-th iteration of the gradient descent. We realize that the prediction curve is getting steeper and steeper as $k$ increases. Indeed, a vertical line (characterized by $\\boldsymbol{w}^{0} \\to +\\infty$) would be the best prediction, as it would best reflect that the $y = 0$ when $x < 0$ and $y = 1$ when $x > 0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "democratic-platform",
      "metadata": {
        "id": "democratic-platform"
      },
      "outputs": [],
      "source": [
        "K = all_f.shape[0]\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=x, y=y, marker=dict(color=\"red\", size=10), mode=\"markers\", name=\"Data\"))\n",
        "slides = []\n",
        "for k in range(0, K):\n",
        "    y_hat_plot = [y_hat(x_p, all_w[k]) for x_p in x_plot]\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=x_plot, y=y_hat_plot, marker=dict(color=\"blue\", size=5),\n",
        "                   mode=\"markers\", name=\"Prediction\", visible=False))\n",
        "\n",
        "    # Add slider tick\n",
        "    slide = {\n",
        "        \"method\": \"update\",\n",
        "        \"args\": [\n",
        "            {\"visible\": [False] * (K + 1)},\n",
        "            {\"title\": \"Logistic regression - dataset (x, y), w at iteration \" + str(k)}\n",
        "        ]\n",
        "    }\n",
        "    slide[\"args\"][0][\"visible\"][0] = True\n",
        "    slide[\"args\"][0][\"visible\"][k + 1] = True\n",
        "    slides.append(slide)\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Logistic regression - dataset (x, y), w at iteration 0\",\n",
        "    sliders=[dict(steps=slides)])\n",
        "fig.data[0].visible = True\n",
        "fig.data[1].visible = True\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "convinced-sacramento",
      "metadata": {
        "id": "convinced-sacramento"
      },
      "source": [
        "> This is confirmed by creating a plot of the empirical risk as a function $\\boldsymbol{w}$. Notice how the function is asymptotically zero on the line $(w^{(0)}, 0)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "negative-disco",
      "metadata": {
        "id": "negative-disco"
      },
      "outputs": [],
      "source": [
        "domain_component_0 = [-10, 10]\n",
        "domain_component_1 = [-10, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "annoying-consumption",
      "metadata": {
        "id": "annoying-consumption"
      },
      "outputs": [],
      "source": [
        "w_component_0 = np.linspace(domain_component_0[0], domain_component_0[1], 100)\n",
        "w_component_1 = np.linspace(domain_component_1[0], domain_component_1[1], 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "moral-majority",
      "metadata": {
        "id": "moral-majority"
      },
      "outputs": [],
      "source": [
        "f_w = np.zeros((len(w_component_0), len(w_component_1)))\n",
        "for i in range(f_w.shape[0]):\n",
        "    for j in range(f_w.shape[1]):\n",
        "        f_w[j, i] = f_ex_1_3(x, y, np.array([w_component_0[i], w_component_1[j]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mineral-worse",
      "metadata": {
        "id": "mineral-worse"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Contour(x=w_component_0, y=w_component_1, z=f_w)])\n",
        "fig.update_layout(title=\"Logistic regression - contour plot\", width=512, height=512, autosize=False)\n",
        "fig.add_scatter(\n",
        "    x=all_w[:, 0], y=all_w[:, 1],\n",
        "    marker=dict(color=\"black\", size=10),\n",
        "    line=dict(color=\"black\", width=2),\n",
        "    mode=\"lines+markers\"\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "reduced-custom",
      "metadata": {
        "id": "reduced-custom"
      },
      "source": [
        "> The gradient method enters a region where the function is almost flat, therefore the gradient is almost zero, forcing the iterative method to take very small step"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "collective-click",
      "metadata": {
        "id": "collective-click"
      },
      "source": [
        "## Exercise 1.4\n",
        "\n",
        "Let $\\boldsymbol{w} \\in \\mathbb{R}^2$. Consider the following non-convex function, known as *Ackley function* and defined as\n",
        "$$f(\\boldsymbol{w}) = - 20 \\exp\\left(- 0.2 \\sqrt{\\frac{[w^{(0)}]^2 + [w^{(1)}]^2}{2}}\\right) - \\exp\\left(\\frac{\\cos(2 \\pi w^{(0)}) + \\cos(2 \\pi w^{(1)})}{2}\\right) + 20 + \\exp(1).$$\n",
        "\n",
        "1. Draw a surface plot and a contour plot of the function $f$ in the interval $[-10, 10]$.\n",
        "\n",
        "*Solution*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "strategic-thailand",
      "metadata": {
        "id": "strategic-thailand"
      },
      "outputs": [],
      "source": [
        "domain_component_0 = [-10, 10]\n",
        "domain_component_1 = [-10, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gorgeous-wildlife",
      "metadata": {
        "id": "gorgeous-wildlife"
      },
      "outputs": [],
      "source": [
        "w_component_0 = np.linspace(domain_component_0[0], domain_component_0[1], 200)\n",
        "w_component_1 = np.linspace(domain_component_1[0], domain_component_1[1], 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "political-annual",
      "metadata": {
        "id": "political-annual"
      },
      "outputs": [],
      "source": [
        "def f_ex_1_4(w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate f(w).\"\"\"\n",
        "    return (\n",
        "        - 20 * np.exp(- 0.2 * np.sqrt((w[0]**2 + w[1]**2) / 2))\n",
        "        - np.exp((np.cos(2 * np.pi * w[0]) + np.cos(2 * np.pi * w[1])) / 2)\n",
        "        + 20 + np.exp(1)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "entitled-trunk",
      "metadata": {
        "id": "entitled-trunk"
      },
      "outputs": [],
      "source": [
        "f_w = np.zeros((len(w_component_0), len(w_component_1)))\n",
        "for i in range(f_w.shape[0]):\n",
        "    for j in range(f_w.shape[1]):\n",
        "        f_w[j, i] = f_ex_1_4(np.array([w_component_0[i], w_component_1[j]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "criminal-dover",
      "metadata": {
        "id": "criminal-dover"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Surface(x=w_component_0, y=w_component_1, z=f_w)])\n",
        "fig.update_layout(title=\"Ackley function - surface plot\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "blocked-juice",
      "metadata": {
        "id": "blocked-juice"
      },
      "source": [
        "> We notice that the function has several small \"bumps\" that result in the formation of many local minima (and local maxima). Looking at the expression it can be shown that such points are associated to $(w^{(0)}, w^{(1)})$ given by a pair of integer coordinates. Such points are also visibile looking at a contour plot. There is a global minimum $\\boldsymbol{w}^* = (0, 0)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "waiting-garage",
      "metadata": {
        "id": "waiting-garage"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Contour(x=w_component_0, y=w_component_1, z=f_w)])\n",
        "fig.update_layout(title=\"Ackley function - contour plot\", width=512, height=512, autosize=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scheduled-circle",
      "metadata": {
        "id": "scheduled-circle"
      },
      "source": [
        "2. Implement a gradient method with backtracking line search and a stopping criterion based on the norm of the increment of the optimization variables.\n",
        "\n",
        "*Solution*:\n",
        "> We first need to write the gradient of the function $f$. *Bonus point*: the expression is very complicated to differentiate, but the partial derivatives can be obtained symbolically with `sympy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nearby-blues",
      "metadata": {
        "id": "nearby-blues"
      },
      "outputs": [],
      "source": [
        "def grad_f_ex_1_4(w: np.ndarray) -> np.ndarray:\n",
        "    r\"\"\"Evaluate \\nabla f(w).\"\"\"\n",
        "    return np.array([\n",
        "        2.0 * w[0] * np.exp(- 0.2 * np.sqrt(w[0]**2 / 2 + w[1]**2 / 2)) / np.sqrt(w[0]**2 / 2 + w[1]**2 / 2)\n",
        "        + np.pi * np.exp(np.cos(2 * np.pi * w[0]) / 2 + np.cos(2 * np.pi * w[1]) / 2) * np.sin(2 * np.pi * w[0]),\n",
        "        2.0 * w[1] * np.exp(- 0.2 * np.sqrt(w[0]**2 / 2 + w[1]**2 / 2)) / np.sqrt(w[0]**2 / 2 + w[1]**2 / 2)\n",
        "        + np.pi * np.exp(np.cos(2 * np.pi * w[0]) / 2 + np.cos(2 * np.pi * w[1]) / 2) * np.sin(2 * np.pi * w[1])\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cooperative-shell",
      "metadata": {
        "id": "cooperative-shell"
      },
      "outputs": [],
      "source": [
        "def gradient_descent_backtracking_line_search_ex_1_4(\n",
        "    alpha: float, c_1: float, c_2: float, epsilon: float, w_0: np.ndarray\n",
        ") -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run the gradient descent method with backtracking line search.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : float\n",
        "        initial step length.\n",
        "    c_1, c_2 : float\n",
        "        constants of the backtracking algorithm.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the increment on the optimization variables.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    \"\"\"\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_f = [f_ex_1_4(w_0)]\n",
        "    all_grad_f = [grad_f_ex_1_4(w_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Use the norm of the variable increment as stopping criterion.\n",
        "    variable_increment = 2 * epsilon\n",
        "    while variable_increment > epsilon:\n",
        "        w_k = all_w[k]\n",
        "        f_k = all_f[k]\n",
        "        grad_f_k = all_grad_f[k]\n",
        "        norm_grad_f_k = np.linalg.norm(grad_f_k)\n",
        "\n",
        "        # Carry out a backtracking line search\n",
        "        alpha_k = alpha\n",
        "        while f_ex_1_4(w_k - alpha_k * grad_f_k) > f_k - c_1 * alpha_k * norm_grad_f_k**2:\n",
        "            alpha_k = c_2 * alpha_k\n",
        "\n",
        "        # Compute w_{k+1}\n",
        "        w_k_plus_1 = w_k - alpha_k * grad_f_k\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f_ex_1_4(w_k_plus_1))\n",
        "        all_grad_f.append(grad_f_ex_1_4(w_k_plus_1))\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "        variable_increment = np.linalg.norm(all_w[k] - all_w[k - 1])\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_f), np.array(all_grad_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "better-philip",
      "metadata": {
        "id": "better-philip"
      },
      "source": [
        "3. Apply the gradient descent method with backtracking starting from several initial conditions, obtained by an equispaced subdivision of the domain $[-10, 10]$ in squares of length two. How many times does the gradient descent method convergence to the global minimum $\\boldsymbol{w}^* = (0, 0)$?\n",
        "\n",
        "*Solution*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "neural-simple",
      "metadata": {
        "id": "neural-simple"
      },
      "outputs": [],
      "source": [
        "solutions = dict()\n",
        "for i in range(-10, 11, 2):\n",
        "    for j in range(-10, 11, 2):\n",
        "        all_w, _, _ = gradient_descent_backtracking_line_search_ex_1_4(\n",
        "            1, 0.1, 0.7, 1e-5, np.array([i + 0.5, j + 0.5]))\n",
        "        optimal_w = all_w[-1]\n",
        "        component_0_int = np.round(optimal_w[0], 0)\n",
        "        component_1_int = np.round(optimal_w[1], 0)\n",
        "        assert np.isclose(optimal_w[0] - component_0_int, 0., atol=1e-1)\n",
        "        assert np.isclose(optimal_w[1] - component_1_int, 0., atol=1e-1)\n",
        "        if (component_0_int, component_1_int) not in solutions:\n",
        "            solutions[(component_0_int, component_1_int)] = 0\n",
        "        solutions[(component_0_int, component_1_int)] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pharmaceutical-ethiopia",
      "metadata": {
        "id": "pharmaceutical-ethiopia"
      },
      "source": [
        "> We convert the dictionary into a matrix with three columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "insured-conviction",
      "metadata": {
        "id": "insured-conviction"
      },
      "outputs": [],
      "source": [
        "solutions_keys_np = np.array(list(solutions.keys()))\n",
        "solutions_values_np = np.array(list(solutions.values())).reshape(-1, 1)\n",
        "solutions_np = np.hstack((solutions_keys_np, solutions_values_np))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "whole-hampshire",
      "metadata": {
        "id": "whole-hampshire"
      },
      "outputs": [],
      "source": [
        "solutions_np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "discrete-fusion",
      "metadata": {
        "id": "discrete-fusion"
      },
      "source": [
        "> We prepare a contour plot overlayed with a scatter plot of the minima found by the optimization method. Markers in the scatter plot as larger if a minima has been found several times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "excellent-reception",
      "metadata": {
        "id": "excellent-reception"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Contour(x=w_component_0, y=w_component_1, z=f_w, opacity=0.5)])\n",
        "fig.add_scatter(\n",
        "    x=solutions_np[:, 0], y=solutions_np[:, 1],\n",
        "    marker=dict(size=5 * np.sqrt(solutions_np[:, 2]), color=\"black\"), mode=\"markers\",\n",
        "    hovertemplate=\"(%{x}, %{y}) found %{customdata} times\", customdata=solutions_np[:, 2],\n",
        ")\n",
        "fig.update_layout(title=\"Ackley function - convergence to minima\", width=512, height=512, autosize=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "double-return",
      "metadata": {
        "id": "double-return"
      },
      "source": [
        "> Percentage of runs at which the global minimum has been found:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unavailable-graph",
      "metadata": {
        "id": "unavailable-graph"
      },
      "outputs": [],
      "source": [
        "solutions[(0, 0)] / sum(solutions_np[:, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "historic-junction",
      "metadata": {
        "id": "historic-junction"
      },
      "source": [
        "> Due to the small bumps, the gradient method has got stuck almost 80% of the cases in local minima. Are there extensions of the gradient method that are less susceptible to local minima?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "immediate-eight",
      "metadata": {
        "id": "immediate-eight"
      },
      "source": [
        "## Exercise 1.5 (continuation of Exercise 1.1)\n",
        "Let $\\boldsymbol{w} \\in \\mathbb{R}^2$. Consider the *Booth function*\n",
        "$$f(\\boldsymbol{w}) = (w^{(0)} + 2 w^{(1)} - 7)^2 + (2 w^{(0)} + w^{(1)} - 5)^2.$$\n",
        "\n",
        "7. Implement Nesterov accelerated gradient method with constant step length and constant momentum coefficient in a Python function. Use the stopping criterion based on the error of the cost. Such function should:\n",
        "   * take as input the value $\\alpha$ of the step length, the value $\\beta$ of the momentum coefficient, the tolerance $\\varepsilon$ for the stopping criterion, and the initial condition $\\boldsymbol{w}_{0}$;\n",
        "   * return as outputs the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$, the corresponding function values $\\{f(\\boldsymbol{w}_k)\\}_k$ and gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$.\n",
        " \n",
        "*Solution*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "entitled-course",
      "metadata": {
        "id": "entitled-course"
      },
      "outputs": [],
      "source": [
        "def f_ex_1_5(w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate f(w).\"\"\"\n",
        "    return (w[0] + 2 * w[1] - 7)**2 + (2 * w[0] + w[1] - 5)**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stuck-banking",
      "metadata": {
        "id": "stuck-banking"
      },
      "outputs": [],
      "source": [
        "def grad_f_ex_1_5(w: np.ndarray) -> float:\n",
        "    r\"\"\"Evaluate \\nabla f(w).\"\"\"\n",
        "    return np.array([10 * w[0] + 8 * w[1] - 34, 8 * w[0] + 10 * w[1] - 38])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aquatic-burlington",
      "metadata": {
        "id": "aquatic-burlington"
      },
      "outputs": [],
      "source": [
        "def nesterov_accelerated_gradient_method_ex_1_5(\n",
        "    alpha: float, beta: float, epsilon: float, w_0: np.ndarray\n",
        ") -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run Nesterov accelerated gradient descent method with constant step length and constant momentum coefficient.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : float\n",
        "        constant step length.\n",
        "    beta : float\n",
        "        constant momentum coefficient.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the cost.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    \"\"\"\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_f = [f_ex_1_5(w_0)]\n",
        "    all_grad_f = [grad_f_ex_1_5(w_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Use the error on the cost to determine when the while loop should stop.\n",
        "    while all_f[k] > epsilon:\n",
        "        w_k_minus_1 = all_w[k - 1]\n",
        "        w_k = all_w[k]\n",
        "        z_k = w_k + beta * (w_k - w_k_minus_1)\n",
        "        grad_f_z_k = grad_f_ex_1_5(z_k)\n",
        "        w_k_plus_1 = z_k - alpha * grad_f_z_k\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f_ex_1_5(w_k_plus_1))\n",
        "        all_grad_f.append(grad_f_ex_1_5(w_k_plus_1))\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_f), np.array(all_grad_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hearing-macedonia",
      "metadata": {
        "id": "hearing-macedonia"
      },
      "source": [
        "8. Choose $\\alpha = 1/L = 1/18$, $\\varepsilon = 10^{-5}$ and $\\boldsymbol{w}_0 = (-8, -8)$, and four possible choices of $\\beta$, corresponding to $\\{0, 0.5, 1, 1.5\\}$ multiplied by the momentum coefficient suggested by the convergence result. Run Nesterov method, and visualize a semilogarithimic plot of error in the function value $\\{f(\\boldsymbol{w}_k) - f(\\boldsymbol{w}^*)\\}_k$ versus the iteration counter $k$ (for each value of $\\beta$).\n",
        "\n",
        "*Solution*:\n",
        "> We first run the case $\\beta = 0$. This corresponds to the gradient method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "distant-orientation",
      "metadata": {
        "id": "distant-orientation"
      },
      "outputs": [],
      "source": [
        "all_w_0, all_f_0, all_grad_f_0 = nesterov_accelerated_gradient_method_ex_1_5(\n",
        "    1 / 18, 0, 1e-5, np.array([-8.0, -8.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rocky-rachel",
      "metadata": {
        "id": "rocky-rachel"
      },
      "outputs": [],
      "source": [
        "all_w_0.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "negative-instrument",
      "metadata": {
        "id": "negative-instrument"
      },
      "source": [
        "> We then compute the suggested value of the momentum coefficient, given by $\\frac{\\sqrt{L} - \\sqrt{\\mu}}{\\sqrt{L} + \\sqrt{\\mu}}$, where we have seen in Exercise 1.1 that $\\mu = 2$ and $L = 18$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "european-matthew",
      "metadata": {
        "id": "european-matthew"
      },
      "outputs": [],
      "source": [
        "suggested_beta = (np.sqrt(18) - np.sqrt(2)) / (np.sqrt(18) + np.sqrt(2))\n",
        "suggested_beta"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "noted-consciousness",
      "metadata": {
        "id": "noted-consciousness"
      },
      "source": [
        "> We then run the remaining cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "facial-calendar",
      "metadata": {
        "id": "facial-calendar"
      },
      "outputs": [],
      "source": [
        "all_w_05, all_f_05, all_grad_f_05 = nesterov_accelerated_gradient_method_ex_1_5(\n",
        "    1 / 18, 0.5 * suggested_beta, 1e-5, np.array([-8.0, -8.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mediterranean-collins",
      "metadata": {
        "id": "mediterranean-collins"
      },
      "outputs": [],
      "source": [
        "all_w_05.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "asian-tower",
      "metadata": {
        "id": "asian-tower"
      },
      "outputs": [],
      "source": [
        "all_w_1, all_f_1, all_grad_f_1 = nesterov_accelerated_gradient_method_ex_1_5(\n",
        "    1 / 18, suggested_beta, 1e-5, np.array([-8.0, -8.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "infectious-seating",
      "metadata": {
        "id": "infectious-seating"
      },
      "outputs": [],
      "source": [
        "all_w_1.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "falling-mailman",
      "metadata": {
        "id": "falling-mailman"
      },
      "outputs": [],
      "source": [
        "all_w_15, all_f_15, all_grad_f_15 = nesterov_accelerated_gradient_method_ex_1_5(\n",
        "    1 / 18, 1.5 * suggested_beta, 1e-5, np.array([-8.0, -8.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "affiliated-phenomenon",
      "metadata": {
        "id": "affiliated-phenomenon"
      },
      "outputs": [],
      "source": [
        "all_w_15.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "completed-convertible",
      "metadata": {
        "id": "completed-convertible"
      },
      "source": [
        "> We prepare a plot of the error on the function value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "desperate-kingston",
      "metadata": {
        "id": "desperate-kingston"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "all_f_beta = [all_f_0, all_f_05, all_f_1, all_f_15]\n",
        "beta_factors = [0, 0.5, 1, 1.5]\n",
        "for beta_index in range(4):\n",
        "    fig.add_scatter(\n",
        "        x=np.arange(all_f_beta[beta_index].shape[0]), y=all_f_beta[beta_index],\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[beta_index], size=10),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[beta_index], width=2),\n",
        "        mode=\"lines+markers\", name=\"Factor in front of beta = \" + str(beta_factors[beta_index])\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=\"Booth function - error on the function value - different momentum coefficients\",\n",
        "    width=768, height=768, autosize=False\n",
        ")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "laden-worcester",
      "metadata": {
        "id": "laden-worcester"
      },
      "source": [
        "> From this plot we conclude that:\n",
        "> * $\\beta = 0$ corresponds to the gradient descent method, which is indeed slower than the three other accelerated methods;\n",
        "> * taking $\\beta$ less than the suggested value causes the acceleration to be less effective;\n",
        "> * taking $\\beta$ as suggested by the convergence theory guarantees the best convergence;\n",
        "> * taking $\\beta$ above the suggested values causes a non montone decrease of the cost function, resulting in a method which does not satisfy the descent condition. While non montone decrease is not an issue per se, in this case (and also in several practical cases) it causes a larger number of iterations to be necessary to reach the prescribed tolerance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "appreciated-governor",
      "metadata": {
        "id": "appreciated-governor"
      },
      "source": [
        "9. Since in many practical application the value of $L$ is not known, the actual step length $\\alpha$ may be smaller than the suggested one by the convergence results. Consider for instance $\\alpha = 1/36$, which is half the suggested one. One might try to \"compensate\" for the halved step by approximately doubling the coefficient $\\beta$.\n",
        "So, choose $\\alpha = 1/36$, $\\varepsilon = 10^{-5}$ and $\\boldsymbol{w}_0 = (-8, -8)$, and four possible choices of $\\beta$, corresponding to $\\{0, 1, 1.5, 2\\}$ multiplied by the momentum coefficient suggested by the convergence result. Run Nesterov method, and visualize:\n",
        "   * a semilogarithimic plot of error in the function value $\\{f(\\boldsymbol{w}_k) - f(\\boldsymbol{w}^*)\\}_k$ versus the iteration counter $k$ (for each value of $\\beta$);\n",
        "   * the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$ on a contour plot of $f$.\n",
        " \n",
        "*Solution*:\n",
        "> We run our implementation for all required values of $\\beta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "periodic-meaning",
      "metadata": {
        "id": "periodic-meaning"
      },
      "outputs": [],
      "source": [
        "all_w_0, all_f_0, all_grad_f_0 = nesterov_accelerated_gradient_method_ex_1_5(\n",
        "    1 / 36, 0, 1e-5, np.array([-8.0, -8.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fiscal-annual",
      "metadata": {
        "id": "fiscal-annual"
      },
      "outputs": [],
      "source": [
        "all_w_0.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "antique-parade",
      "metadata": {
        "id": "antique-parade"
      },
      "outputs": [],
      "source": [
        "all_w_1, all_f_1, all_grad_f_1 = nesterov_accelerated_gradient_method_ex_1_5(\n",
        "    1 / 36, suggested_beta, 1e-5, np.array([-8.0, -8.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "local-dylan",
      "metadata": {
        "id": "local-dylan"
      },
      "outputs": [],
      "source": [
        "all_w_1.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "satisfied-economics",
      "metadata": {
        "id": "satisfied-economics"
      },
      "outputs": [],
      "source": [
        "all_w_15, all_f_15, all_grad_f_15 = nesterov_accelerated_gradient_method_ex_1_5(\n",
        "    1 / 36, 1.5 * suggested_beta, 1e-5, np.array([-8.0, -8.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "existing-chuck",
      "metadata": {
        "id": "existing-chuck"
      },
      "outputs": [],
      "source": [
        "all_w_15.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dependent-floor",
      "metadata": {
        "id": "dependent-floor"
      },
      "outputs": [],
      "source": [
        "all_w_2, all_f_2, all_grad_f_2 = nesterov_accelerated_gradient_method_ex_1_5(\n",
        "    1 / 36, 2 * suggested_beta, 1e-5, np.array([-8.0, -8.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "electric-relevance",
      "metadata": {
        "id": "electric-relevance"
      },
      "outputs": [],
      "source": [
        "all_w_2.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "together-employer",
      "metadata": {
        "id": "together-employer"
      },
      "source": [
        "> We prepare a plot of the error on the function value. Following the discussion at the previous point, we suggest to choose a factor of 1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "respective-worth",
      "metadata": {
        "id": "respective-worth"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "all_f_beta = [all_f_0, all_f_1, all_f_15, all_f_2]\n",
        "for beta_index in range(4):\n",
        "    fig.add_scatter(\n",
        "        x=np.arange(all_f_beta[beta_index].shape[0]), y=all_f_beta[beta_index],\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[beta_index], size=10),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[beta_index], width=2),\n",
        "        mode=\"lines+markers\", name=\"Factor in front of beta = \" + str(beta_factors[beta_index])\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=\"Booth function - error on the function value - different momentum coefficients\",\n",
        "    width=768, height=768, autosize=False\n",
        ")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vietnamese-professional",
      "metadata": {
        "id": "vietnamese-professional"
      },
      "source": [
        "> We conclude with the visualization of the iterations over a contour plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spread-victim",
      "metadata": {
        "id": "spread-victim"
      },
      "outputs": [],
      "source": [
        "domain_component_0 = [-10, 10]\n",
        "domain_component_1 = [-10, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cellular-bryan",
      "metadata": {
        "id": "cellular-bryan"
      },
      "outputs": [],
      "source": [
        "w_component_0 = np.linspace(domain_component_0[0], domain_component_0[1], 100)\n",
        "w_component_1 = np.linspace(domain_component_1[0], domain_component_1[1], 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "promising-sharing",
      "metadata": {
        "id": "promising-sharing"
      },
      "outputs": [],
      "source": [
        "f_w = np.zeros((len(w_component_0), len(w_component_1)))\n",
        "for i in range(f_w.shape[0]):\n",
        "    for j in range(f_w.shape[1]):\n",
        "        f_w[j, i] = f_ex_1_5([w_component_0[i], w_component_1[j]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "copyrighted-bridge",
      "metadata": {
        "id": "copyrighted-bridge"
      },
      "outputs": [],
      "source": [
        "fig = plotly.subplots.make_subplots(rows=2, cols=2)\n",
        "rows = [1, 1, 2, 2]\n",
        "cols = [1, 2, 1, 2]\n",
        "all_w_beta = [all_w_0, all_w_1, all_w_15, all_w_2]\n",
        "beta_factors = [0, 1, 1.5, 2]\n",
        "for beta_index in range(4):\n",
        "    fig.add_contour(\n",
        "        x=w_component_0, y=w_component_1, z=f_w, opacity=0.5, showscale=False,\n",
        "        row=rows[beta_index], col=cols[beta_index]\n",
        "    )\n",
        "    fig.add_scatter(\n",
        "        x=all_w_beta[beta_index][:, 0], y=all_w_beta[beta_index][:, 1],\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[beta_index], size=10),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[beta_index], width=2),\n",
        "        mode=\"lines+markers\", name=\"Factor in front of beta = \" + str(beta_factors[beta_index]),\n",
        "        row=rows[beta_index], col=cols[beta_index]\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=\"Booth function - optimization variable iterations over contour plot - different momentum coefficients\",\n",
        "    width=768, height=768, autosize=False\n",
        ")\n",
        "fig.update_xaxes(range=[0, 5])\n",
        "fig.update_yaxes(range=[0, 5])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "partial-processor",
      "metadata": {
        "id": "partial-processor"
      },
      "source": [
        "> For every value of $\\beta$ the first iterations of the accelerated method take considerably larger steps than the gradient method. However, at least for moderate values of $\\beta$ (factors 1 and 1.5) this is beneficial overall for later iterations. In contrast, a too large valu eof $\\beta$ (factor 2) may cause the method to overshoot the minimum point."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "amazing-planner",
      "metadata": {
        "id": "amazing-planner"
      },
      "source": [
        "## Exercise 1.6 (continuation of Exercise 1.4)\n",
        "\n",
        "Let $\\boldsymbol{w} \\in \\mathbb{R}^2$. Consider the *Ackley function*\n",
        "$$f(\\boldsymbol{w}) = - 20 \\exp\\left(- 0.2 \\sqrt{\\frac{[w^{(0)}]^2 + [w^{(1)}]^2}{2}}\\right) - \\exp\\left(\\frac{\\cos(2 \\pi w^{(0)}) + \\cos(2 \\pi w^{(1)})}{2}\\right) + 20 + \\exp(1).$$\n",
        "\n",
        "4. Implement the heavy ball method with backtracking line search and constant momentum coefficient. Use the stopping criterion based on the norm of the increment of the optimization variables. Such function should:\n",
        "   * take as input the constants $\\alpha$, $c_1$ and $c_2$ of the backtracking algorithm, the momentum coefficient $\\beta$, the tolerance $\\varepsilon$ for the stopping criterion, and the initial condition $\\boldsymbol{w}_{0}$;\n",
        "   * return as outputs the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$, the corresponding function values $\\{f(\\boldsymbol{w}_k)\\}_k$ and gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$.\n",
        " \n",
        "*Solution*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eastern-amateur",
      "metadata": {
        "id": "eastern-amateur"
      },
      "outputs": [],
      "source": [
        "def f_ex_1_6(w: np.ndarray) -> float:\n",
        "    \"\"\"Evaluate f(w).\"\"\"\n",
        "    return (\n",
        "        - 20 * np.exp(- 0.2 * np.sqrt((w[0]**2 + w[1]**2) / 2))\n",
        "        - np.exp((np.cos(2 * np.pi * w[0]) + np.cos(2 * np.pi * w[1])) / 2)\n",
        "        + 20 + np.exp(1)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "satellite-george",
      "metadata": {
        "id": "satellite-george"
      },
      "outputs": [],
      "source": [
        "def grad_f_ex_1_6(w: np.ndarray) -> np.ndarray:\n",
        "    r\"\"\"Evaluate \\nabla f(w).\"\"\"\n",
        "    return np.array([\n",
        "        2.0 * w[0] * np.exp(- 0.2 * np.sqrt(w[0]**2 / 2 + w[1]**2 / 2)) / np.sqrt(w[0]**2 / 2 + w[1]**2 / 2)\n",
        "        + np.pi * np.exp(np.cos(2 * np.pi * w[0]) / 2 + np.cos(2 * np.pi * w[1]) / 2) * np.sin(2 * np.pi * w[0]),\n",
        "        2.0 * w[1] * np.exp(- 0.2 * np.sqrt(w[0]**2 / 2 + w[1]**2 / 2)) / np.sqrt(w[0]**2 / 2 + w[1]**2 / 2)\n",
        "        + np.pi * np.exp(np.cos(2 * np.pi * w[0]) / 2 + np.cos(2 * np.pi * w[1]) / 2) * np.sin(2 * np.pi * w[1])\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "social-least",
      "metadata": {
        "id": "social-least"
      },
      "outputs": [],
      "source": [
        "def heavy_ball_backtracking_line_search_ex_1_6(\n",
        "    alpha: float, c_1: float, c_2: float, beta: float, epsilon: float, w_0: np.ndarray\n",
        ") -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run the heavy ball method with backtracking line search and constant momentum.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : float\n",
        "        initial step length.\n",
        "    c_1, c_2 : float\n",
        "        constants of the backtracking algorithm.\n",
        "    beta : float\n",
        "        constant momentum coefficient.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the increment on the optimization variables.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    \"\"\"\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_f = [f_ex_1_6(w_0)]\n",
        "    all_grad_f = [grad_f_ex_1_6(w_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Use the norm of the variable increment as stopping criterion.\n",
        "    variable_increment = 2 * epsilon\n",
        "    while variable_increment > epsilon:\n",
        "        w_k_minus_1 = all_w[k - 1]\n",
        "        w_k = all_w[k]\n",
        "        f_k = all_f[k]\n",
        "        grad_f_k = all_grad_f[k]\n",
        "        norm_grad_f_k = np.linalg.norm(grad_f_k)\n",
        "\n",
        "        # Carry out a backtracking line search\n",
        "        alpha_k = alpha\n",
        "        while f_ex_1_6(w_k - alpha_k * grad_f_k) > f_k - c_1 * alpha_k * norm_grad_f_k**2:\n",
        "            alpha_k = c_2 * alpha_k\n",
        "\n",
        "        # Compute z_k and w_{k+1}\n",
        "        z_k = w_k - alpha_k * grad_f_k\n",
        "        w_k_plus_1 = z_k + beta * (w_k - w_k_minus_1)\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f_ex_1_6(w_k_plus_1))\n",
        "        all_grad_f.append(grad_f_ex_1_6(w_k_plus_1))\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "        variable_increment = np.linalg.norm(all_w[k] - all_w[k - 1])\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_f), np.array(all_grad_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sealed-intranet",
      "metadata": {
        "id": "sealed-intranet"
      },
      "source": [
        "5. Apply the heavy ball method with backtracking starting from several initial conditions, obtained by an equispaced subdivision of the domain $[-10, 10]$ in squares of length two. How many times does the heavy ball method convergence to the global minimum $\\boldsymbol{w}^* = (0, 0)$? Is it better than gradient descent?\n",
        "\n",
        "*Solution*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "advised-hunter",
      "metadata": {
        "id": "advised-hunter"
      },
      "outputs": [],
      "source": [
        "solutions = dict()\n",
        "for i in range(-10, 11, 2):\n",
        "    for j in range(-10, 11, 2):\n",
        "        all_w, _, _ = heavy_ball_backtracking_line_search_ex_1_6(\n",
        "            1, 0.1, 0.7, 0.5, 1e-5, np.array([i + 0.5, j + 0.5]))\n",
        "        optimal_w = all_w[-1]\n",
        "        component_0_int = np.round(optimal_w[0], 0)\n",
        "        component_1_int = np.round(optimal_w[1], 0)\n",
        "        assert np.isclose(optimal_w[0] - component_0_int, 0., atol=1e-1)\n",
        "        assert np.isclose(optimal_w[1] - component_1_int, 0., atol=1e-1)\n",
        "        if (component_0_int, component_1_int) not in solutions:\n",
        "            solutions[(component_0_int, component_1_int)] = 0\n",
        "        solutions[(component_0_int, component_1_int)] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "minus-split",
      "metadata": {
        "id": "minus-split"
      },
      "source": [
        "> We convert the dictionary into a matrix with three columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thirty-soundtrack",
      "metadata": {
        "id": "thirty-soundtrack"
      },
      "outputs": [],
      "source": [
        "solutions_keys_np = np.array(list(solutions.keys()))\n",
        "solutions_values_np = np.array(list(solutions.values())).reshape(-1, 1)\n",
        "solutions_np = np.hstack((solutions_keys_np, solutions_values_np))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hollow-vancouver",
      "metadata": {
        "id": "hollow-vancouver"
      },
      "source": [
        "> We prepare a contour plot overlayed with a scatter plot of the minima found by the optimization method. Markers in the scatter plot as larger if a minima has been found several times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "south-flood",
      "metadata": {
        "id": "south-flood"
      },
      "outputs": [],
      "source": [
        "domain_component_0 = [-10, 10]\n",
        "domain_component_1 = [-10, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "posted-grill",
      "metadata": {
        "id": "posted-grill"
      },
      "outputs": [],
      "source": [
        "w_component_0 = np.linspace(domain_component_0[0], domain_component_0[1], 100)\n",
        "w_component_1 = np.linspace(domain_component_1[0], domain_component_1[1], 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "persistent-basic",
      "metadata": {
        "id": "persistent-basic"
      },
      "outputs": [],
      "source": [
        "f_w = np.zeros((len(w_component_0), len(w_component_1)))\n",
        "for i in range(f_w.shape[0]):\n",
        "    for j in range(f_w.shape[1]):\n",
        "        f_w[j, i] = f_ex_1_6([w_component_0[i], w_component_1[j]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wooden-lincoln",
      "metadata": {
        "id": "wooden-lincoln"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Contour(x=w_component_0, y=w_component_1, z=f_w, opacity=0.5)])\n",
        "fig.add_scatter(\n",
        "    x=solutions_np[:, 0], y=solutions_np[:, 1],\n",
        "    marker=dict(size=5 * np.sqrt(solutions_np[:, 2]), color=\"black\"), mode=\"markers\",\n",
        "    hovertemplate=\"(%{x}, %{y}) found %{customdata} times\", customdata=solutions_np[:, 2],\n",
        ")\n",
        "fig.update_layout(title=\"Ackley function - convergence to minima\", width=512, height=512, autosize=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "plastic-knife",
      "metadata": {
        "id": "plastic-knife"
      },
      "source": [
        "> Percentage of runs at which the global minimum has been found:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "visible-organization",
      "metadata": {
        "id": "visible-organization"
      },
      "outputs": [],
      "source": [
        "solutions[(0, 0)] / sum(solutions_np[:, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "geological-penalty",
      "metadata": {
        "id": "geological-penalty"
      },
      "source": [
        "> The heavy ball method converged to the global minimum in more than $60\\%$ cases, compared to roughly $20\\%$ of the gradient method."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}