{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pserebrennikov/3rd-year-project/blob/master/3_stochastic_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "occupied-haiti",
      "metadata": {
        "id": "occupied-haiti"
      },
      "source": [
        "# Tutorial 3 - Stochastic methods\n",
        "### Course on Optimization for Machine Learning - Dr. F. Ballarin\n",
        "### Master Degree in Data Analytics for Business, Catholic University of the Sacred Heart, Milano"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "graduate-channel",
      "metadata": {
        "id": "graduate-channel"
      },
      "source": [
        "In this notebook we implement the stochastic version of first order methods we have seen in the first lecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecd0fa4a",
      "metadata": {
        "id": "ecd0fa4a"
      },
      "outputs": [],
      "source": [
        "import typing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "collective-threshold",
      "metadata": {
        "id": "collective-threshold"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import plotly.colors\n",
        "import plotly.graph_objects as go\n",
        "import plotly.subplots"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dated-aircraft",
      "metadata": {
        "id": "dated-aircraft"
      },
      "source": [
        "## Exercise 3.1 (continuation of Exercises 1.1, 1.5 and 2.1)\n",
        "Let $\\boldsymbol{w} \\in \\mathbb{R}^2$. Consider the *Booth function*\n",
        "$$f(\\boldsymbol{w}) = (w^{(0)} + 2 w^{(1)} - 7)^2 + (2 w^{(0)} + w^{(1)} - 5)^2$$\n",
        "as the sum of two functions\n",
        "$$f_0(\\boldsymbol{w}) = (w^{(0)} + 2 w^{(1)} - 7)^2, \\qquad f_1(\\boldsymbol{w}) = (2 w^{(0)} + w^{(1)} - 5)^2.$$\n",
        "(Note that the numbering of the addends starts from 0 for consistency with Python numbering)\n",
        "\n",
        "13. Draw a contour plot of the functions $f$, $f_0$ and $f_1$ on the square domain $[-10, 10]^2$.\n",
        "\n",
        "*Solution*:\n",
        "> As in previous exercises we start defining an equispaced sampling of the domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "formed-mailing",
      "metadata": {
        "id": "formed-mailing"
      },
      "outputs": [],
      "source": [
        "domain_component_0 = [-10, 10]\n",
        "domain_component_1 = [-10, 10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "anticipated-mystery",
      "metadata": {
        "id": "anticipated-mystery"
      },
      "outputs": [],
      "source": [
        "w_component_0 = np.linspace(domain_component_0[0], domain_component_0[1], 100)\n",
        "w_component_1 = np.linspace(domain_component_1[0], domain_component_1[1], 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "charged-paragraph",
      "metadata": {
        "id": "charged-paragraph"
      },
      "source": [
        "> We then define $f$, $f_0$ and $f_1$ using a single Python function. In addition to the usual `w` argument, this function takes a second optional argument:\n",
        "> * when the second argument is not provided, i.e. `f_ex_3_1(w)` is called, we return the evaluation of $f$, as we are used to from previous exercises\n",
        "> * when the second argument is either 0 or 1, we return the evaluation of either $f_0$ or $f_1$, respectively, at the point `w`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "identical-animation",
      "metadata": {
        "id": "identical-animation"
      },
      "outputs": [],
      "source": [
        "def f_ex_3_1(w: np.ndarray, addend: int = None) -> float:\n",
        "    r\"\"\"Evaluate f(w) if addend is None, or f_{addend}(w) if addend is an integer.\"\"\"\n",
        "    if addend == 0:\n",
        "        return (w[0] + 2 * w[1] - 7)**2\n",
        "    elif addend == 1:\n",
        "        return (2 * w[0] + w[1] - 5)**2\n",
        "    elif addend is None:\n",
        "        return (w[0] + 2 * w[1] - 7)**2 + (2 * w[0] + w[1] - 5)**2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "partial-proposal",
      "metadata": {
        "id": "partial-proposal"
      },
      "source": [
        "> In preparation of the contour plot we store the evaluation of $f$, $f_0$ and $f_1$ in three different matrices. Note the three different calls to `f_ex_3_1`:\n",
        "> * we do not provide the second argument when evaluating $f$,\n",
        "> * when interested in evaluating $f_0$ or $f_1$, we may either provide directly the corresponding number 0 or 1 as the second argument, or prepend it with the `addend=` syntax if we want to highlight that the value is associated to the `addend` input variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hazardous-graph",
      "metadata": {
        "id": "hazardous-graph"
      },
      "outputs": [],
      "source": [
        "f_w = np.zeros((len(w_component_0), len(w_component_1)))\n",
        "f0_w = np.zeros((len(w_component_0), len(w_component_1)))\n",
        "f1_w = np.zeros((len(w_component_0), len(w_component_1)))\n",
        "for i in range(f_w.shape[0]):\n",
        "    for j in range(f_w.shape[1]):\n",
        "        f_w[j, i] = f_ex_3_1([w_component_0[i], w_component_1[j]])\n",
        "        f0_w[j, i] = f_ex_3_1([w_component_0[i], w_component_1[j]], addend=0)\n",
        "        f1_w[j, i] = f_ex_3_1([w_component_0[i], w_component_1[j]], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "modern-secondary",
      "metadata": {
        "id": "modern-secondary"
      },
      "source": [
        "> We then prepare a contour plot. We prepare three subplots aligned vertically, that will contain (from top to bottom) $f$, $f_0$ and $f_1$. Notice that, with `coloraxis=\"coloraxis\"`, we are instructing `plotly` to share the colorbar among the three subplots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "surprising-resource",
      "metadata": {
        "id": "surprising-resource"
      },
      "outputs": [],
      "source": [
        "fig = plotly.subplots.make_subplots(rows=3, cols=1, vertical_spacing=0.05)\n",
        "fig.add_contour(x=w_component_0, y=w_component_1, z=f_w, row=1, col=1, coloraxis=\"coloraxis\")\n",
        "fig.add_contour(x=w_component_0, y=w_component_1, z=f0_w, row=2, col=1, coloraxis=\"coloraxis\")\n",
        "fig.add_contour(x=w_component_0, y=w_component_1, z=f1_w, row=3, col=1, coloraxis=\"coloraxis\")\n",
        "fig.update_layout(title=\"Booth function - contour plot\", width=512, height=2.5 * 512, autosize=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "isolated-medium",
      "metadata": {
        "id": "isolated-medium"
      },
      "source": [
        "14. Implement the stochastic gradient method with constant step length in a Python function. Such function should:\n",
        "    * take as input the number $m$ of addends, the function $f$, its gradient $\\nabla f$, the value $\\alpha$ of the step length, the tolerance $\\varepsilon$ for the stopping criterion, maximum number $K_{\\max}$ of allowed iterations, and the initial condition $\\boldsymbol{w}_{0}$;\n",
        "    * return as outputs the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$, the corresponding function values $\\{f(\\boldsymbol{w}_k)\\}_k$ and gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$.\n",
        "\n",
        "    Use the stopping criterion based on the norm of the gradient. (Note that the such stopping criterion is not realistic in big data applications, because it requires the gradient evaluation of the whole sum!)\n",
        "\n",
        "*Solution*:\n",
        "> Before starting the implementation of the stochastic gradient method we still have to implement the gradient of the functions $f$, $f_0$ and $f_1$. We follow a design similar to the one used for function evaluation, by adding an optional `addend` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "speaking-brisbane",
      "metadata": {
        "id": "speaking-brisbane"
      },
      "outputs": [],
      "source": [
        "def grad_f_ex_3_1(w: np.ndarray, addend: int = None) -> np.ndarray:\n",
        "    r\"\"\"Evaluate \\nabla f(w) if addend is None, or \\nabla f_{addend}(w) if addend is an integer.\"\"\"\n",
        "    if addend == 0:\n",
        "        return (w[0] + 2 * w[1] - 7) * np.array([2, 4])\n",
        "    elif addend == 1:\n",
        "        return (2 * w[0] + w[1] - 5) * np.array([4, 2])\n",
        "    elif addend is None:\n",
        "        return np.array([10 * w[0] + 8 * w[1] - 34, 8 * w[0] + 10 * w[1] - 38])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sweet-mumbai",
      "metadata": {
        "id": "sweet-mumbai"
      },
      "source": [
        "> We test the implemented functions on the optimal point $\\boldsymbol{w}^*$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prompt-drawing",
      "metadata": {
        "id": "prompt-drawing"
      },
      "outputs": [],
      "source": [
        "w_star = np.array([1, 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "technical-keeping",
      "metadata": {
        "id": "technical-keeping"
      },
      "outputs": [],
      "source": [
        "f_ex_3_1(w_star)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "angry-listing",
      "metadata": {
        "id": "angry-listing"
      },
      "outputs": [],
      "source": [
        "grad_f_ex_3_1(w_star)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "outside-ontario",
      "metadata": {
        "id": "outside-ontario"
      },
      "source": [
        "> Notice that $\\boldsymbol{w}^*$ is a not only a minimum of $f$, but also of $f_0$ and $f_1$. This is a particularly favorable situation for the Booth function, but this will not be true in general! (see next Exercise 3.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bronze-accommodation",
      "metadata": {
        "id": "bronze-accommodation"
      },
      "outputs": [],
      "source": [
        "f_ex_3_1(w_star, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "continental-pitch",
      "metadata": {
        "id": "continental-pitch"
      },
      "outputs": [],
      "source": [
        "grad_f_ex_3_1(w_star, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "radio-defeat",
      "metadata": {
        "id": "radio-defeat"
      },
      "outputs": [],
      "source": [
        "f_ex_3_1(w_star, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "universal-coalition",
      "metadata": {
        "id": "universal-coalition"
      },
      "outputs": [],
      "source": [
        "grad_f_ex_3_1(w_star, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "american-enough",
      "metadata": {
        "id": "american-enough"
      },
      "source": [
        "> In order to implement the stochastic gradient we need to select at every iteration a value $j_k$ as either 0 or 1.\n",
        "> To this end, we can use [`numpy.random.randint`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randint.html#numpy.random.randint). Notice that we have to pass the number `2` as argument to get samples made of 0 or 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exterior-edition",
      "metadata": {
        "id": "exterior-edition"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient(\n",
        "    m: int, f: typing.Callable, grad_f: typing.Callable, alpha: float, epsilon: float, maxit: int, w_0: np.ndarray\n",
        ") -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run the stochastic gradient method with constant step length.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    m : int\n",
        "        number of addends in the expression of the cost function.\n",
        "    f, grad_f : Python function\n",
        "        callable evaluating the cost function and its gradient, respectively.\n",
        "    alpha : float\n",
        "        constant step length.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the norm of the gradient of the cost.\n",
        "    maxit : int\n",
        "        maximum number of allowed iterations.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    \"\"\"\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_f = [f(w_0)]\n",
        "    all_grad_f = [grad_f(w_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Use the norm of the gradient as stopping criterion.\n",
        "    while np.linalg.norm(all_grad_f[k]) > epsilon:\n",
        "        w_k = all_w[k]\n",
        "\n",
        "        # Draw a random index\n",
        "        j_k = np.random.randint(m)\n",
        "\n",
        "        # Compute the update direction\n",
        "        g_k = - grad_f(w_k, addend=j_k)\n",
        "\n",
        "        # Compute w_{k + 1}\n",
        "        w_k_plus_1 = w_k + alpha * g_k\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f(w_k_plus_1))\n",
        "        all_grad_f.append(grad_f(w_k_plus_1))\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "        # Bail out if exceeded allowed number of iterations\n",
        "        if k >= maxit:\n",
        "            print(\"WARNING: stochastic gradient method exceeded number of allowed iterations\")\n",
        "            break\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_f), np.array(all_grad_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "friendly-timer",
      "metadata": {
        "id": "friendly-timer"
      },
      "source": [
        "15. Choose $\\alpha = 1 / 18$, $\\varepsilon = 10^{-2}$, $K_{\\max} = 1000$ and $\\boldsymbol{w}_0 = (-8, -8)$. Visualize:\n",
        "    * an animation of the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$ on a contour plot of $f$;\n",
        "    * a semilogarithimic plot of error in the function value $\\{f(\\boldsymbol{w}_k) - f(\\boldsymbol{w}^*)\\}_k$ versus the iteration counter $k$;\n",
        "    * a semilogarithimic plot of the norm of the gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$ versus the iteration counter $k$.\n",
        " \n",
        "    Since the choice of addend is random, set a seed (for reproducibility) and run the stochastic gradient method four times (and expect slightly different results every time).\n",
        " \n",
        "*Solution*:\n",
        "> We set the seed and run the stochastic gradient method implementation four times, and collect the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparative-pharmaceutical",
      "metadata": {
        "id": "comparative-pharmaceutical"
      },
      "outputs": [],
      "source": [
        "all_w = [None] * 4\n",
        "all_f = [None] * 4\n",
        "all_grad_f = [None] * 4\n",
        "\n",
        "np.random.seed(31)\n",
        "for run in range(4):\n",
        "    all_w[run], all_f[run], all_grad_f[run] = stochastic_gradient(\n",
        "        2, f_ex_3_1, grad_f_ex_3_1, 1 / 18, 1e-2, 1000, np.array([-8.0, -8.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "colonial-gazette",
      "metadata": {
        "id": "colonial-gazette"
      },
      "source": [
        "> We create a table that compares the cost function for different iterations $k$ (rows) and different runs (columns). Since different runs may have reach the prescribed tolerance $\\varepsilon$ in a different number of iterations, we will compare different runs using the minimum of such numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "relevant-glucose",
      "metadata": {
        "id": "relevant-glucose"
      },
      "outputs": [],
      "source": [
        "all_f[0].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "statewide-accent",
      "metadata": {
        "id": "statewide-accent"
      },
      "outputs": [],
      "source": [
        "all_f[1].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "south-mother",
      "metadata": {
        "id": "south-mother"
      },
      "outputs": [],
      "source": [
        "all_f[2].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "expired-treasure",
      "metadata": {
        "id": "expired-treasure"
      },
      "outputs": [],
      "source": [
        "all_f[3].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sought-eight",
      "metadata": {
        "id": "sought-eight"
      },
      "outputs": [],
      "source": [
        "min_K = min([all_f[run].shape[0] for run in range(4)])\n",
        "min_K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "included-track",
      "metadata": {
        "id": "included-track"
      },
      "outputs": [],
      "source": [
        "max_K = max([all_f[run].shape[0] for run in range(4)])\n",
        "max_K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "solar-providence",
      "metadata": {
        "id": "solar-providence"
      },
      "outputs": [],
      "source": [
        "np.vstack([all_f[run][:min_K] for run in range(4)]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "italic-local",
      "metadata": {
        "id": "italic-local"
      },
      "source": [
        "> We notice that the overall behavior is quite similar between the different runs, even though the cost function in the last row of the table may be different of about an order of magnitude. Furthermore, we notice that, especially towards the final iterations, the cost function may increase from one iteration to the next!\n",
        ">\n",
        "> The non monotone behavior is particularly evident looking at the plots of the error on the function value, as well as the plot of the norms of the gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "precious-durham",
      "metadata": {
        "id": "precious-durham"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "for run in range(4):\n",
        "    fig.add_scatter(\n",
        "        x=np.arange(all_f[run].shape[0]), y=all_f[run],\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[run], size=10),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[run], width=2),\n",
        "        mode=\"lines+markers\", name=\"run \" + str(run)\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=\"Booth function - error on the function value\",\n",
        "    width=768, height=768, autosize=False\n",
        ")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "breeding-illness",
      "metadata": {
        "id": "breeding-illness"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "for run in range(4):\n",
        "    fig.add_scatter(\n",
        "        x=np.arange(all_f[run].shape[0]), y=np.linalg.norm(all_grad_f[run], axis=1),\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[run], size=10),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[run], width=2),\n",
        "        mode=\"lines+markers\", name=\"run \" + str(run)\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=\"Booth function - norm of the gradient\",\n",
        "    width=768, height=768, autosize=False\n",
        ")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "historical-product",
      "metadata": {
        "id": "historical-product"
      },
      "source": [
        "> Judging from these plots we are led to believe that, at least on average (to cancel out the oscillations), we still have a linear convergance rate.\n",
        ">\n",
        "> Oscillations are also quite visible when plotting the optimization variables over a contour plot. First iterations have considerably larger oscillations, but the optimization variable still oscillate even at later iterations (to see this uncomment the lines that apply a restriction of the horizontal and vertical axes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "played-pride",
      "metadata": {
        "id": "played-pride"
      },
      "outputs": [],
      "source": [
        "# Add opaque contour plot\n",
        "fig = go.Figure(data=[go.Contour(\n",
        "    x=w_component_0, y=w_component_1, z=f_w,\n",
        "    showscale=False, visible=True, opacity=0.5\n",
        ")])\n",
        "\n",
        "# Add a red cross marker to locate w*\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=[w_star[0]], y=[w_star[1]],\n",
        "               marker=dict(color=\"red\", size=10, symbol=\"x\"),\n",
        "               mode=\"markers\", name=\"w*\", visible=True))\n",
        "\n",
        "# Prepare a slider for each iteration k\n",
        "slides = []\n",
        "for k in range(max_K):\n",
        "    for run in range(4):\n",
        "        # Set non uniform marker size to highlight the current iteration k\n",
        "        marker_size = np.zeros((k + 1, ))\n",
        "        marker_size[-1] = 10\n",
        "        # Add lines\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=all_w[run][:k + 1, 0],\n",
        "                       y=all_w[run][:k + 1, 1],\n",
        "                       visible=False,\n",
        "                       marker=dict(color=plotly.colors.qualitative.Set1[run], size=marker_size),\n",
        "                       line=dict(color=plotly.colors.qualitative.Set1[run], width=2),\n",
        "                       mode=\"lines+markers\",\n",
        "                       name=\"run \" + str(run) + \" at k = \" + str(k).zfill(3)))\n",
        "\n",
        "    # Add slider tick\n",
        "    slide = {\n",
        "        \"method\": \"update\",\n",
        "        \"args\": [\n",
        "            {\"visible\": [False] * (4 * max_K + 2)},\n",
        "            {}\n",
        "        ]\n",
        "    }\n",
        "    slide[\"args\"][0][\"visible\"][0] = True\n",
        "    slide[\"args\"][0][\"visible\"][1] = True\n",
        "    for run in range(4):\n",
        "        slide[\"args\"][0][\"visible\"][4 * k + run + 2] = True\n",
        "    slides.append(slide)\n",
        "\n",
        "for run in range(4):\n",
        "    fig.data[run + 2].visible = True\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Booth function - optimization variable iterations over contour plot\",\n",
        "    width=612, height=512, autosize=False,\n",
        "    sliders=[dict(steps=slides)]\n",
        ")\n",
        "# fig.update_xaxes(range=[1 - 0.1, 1 + 0.1])\n",
        "# fig.update_yaxes(range=[3 - 0.1, 3 + 0.1])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "valuable-tournament",
      "metadata": {
        "id": "valuable-tournament"
      },
      "source": [
        "## Exercise 3.2\n",
        "Let $\\boldsymbol{w} \\in \\mathbb{R}^2$. Consider the following function, known as *Trid function*\n",
        "$$f(\\boldsymbol{w}) = (w^{(0)} - 1)^2 + (w^{(1)} - 1)^2 - w^{(0)}w^{(1)}$$\n",
        "as the sum of three functions\n",
        "$$f_0(\\boldsymbol{w}) = (w^{(0)} - 1)^2, \\qquad f_1(\\boldsymbol{w}) = (w^{(1)} - 1)^2, \\qquad f_2(\\boldsymbol{w}) = - w^{(0)}w^{(1)}.$$\n",
        "\n",
        "1. Draw a contour plot of the functions $f$, $f_0$, $f_1$ and $f_2$ on the square domain $[-4, 4]^2$.\n",
        "\n",
        "*Solution*:\n",
        "> The code is very similar to the previous exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparable-improvement",
      "metadata": {
        "id": "comparable-improvement"
      },
      "outputs": [],
      "source": [
        "domain_component_0 = [-4, 4]\n",
        "domain_component_1 = [-4, 4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "soviet-championship",
      "metadata": {
        "id": "soviet-championship"
      },
      "outputs": [],
      "source": [
        "w_component_0 = np.linspace(domain_component_0[0], domain_component_0[1], 100)\n",
        "w_component_1 = np.linspace(domain_component_1[0], domain_component_1[1], 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "twelve-center",
      "metadata": {
        "id": "twelve-center"
      },
      "outputs": [],
      "source": [
        "def f_ex_3_2(w: np.ndarray, addend: int = None) -> float:\n",
        "    r\"\"\"Evaluate f(w) if addend is None, or f_{addend}(w) if addend is an integer.\"\"\"\n",
        "    if addend == 0:\n",
        "        return (w[0] - 1)**2\n",
        "    elif addend == 1:\n",
        "        return (w[1] - 1)**2\n",
        "    elif addend == 2:\n",
        "        return - w[0] * w[1]\n",
        "    elif addend is None:\n",
        "        return (w[0] - 1)**2 + (w[1] - 1)**2 - w[0] * w[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "suffering-wonder",
      "metadata": {
        "id": "suffering-wonder"
      },
      "outputs": [],
      "source": [
        "f_w = np.zeros((len(w_component_0), len(w_component_1)))\n",
        "f0_w = np.zeros((len(w_component_0), len(w_component_1)))\n",
        "f1_w = np.zeros((len(w_component_0), len(w_component_1)))\n",
        "f2_w = np.zeros((len(w_component_0), len(w_component_1)))\n",
        "for i in range(f_w.shape[0]):\n",
        "    for j in range(f_w.shape[1]):\n",
        "        f_w[j, i] = f_ex_3_2([w_component_0[i], w_component_1[j]])\n",
        "        f0_w[j, i] = f_ex_3_2([w_component_0[i], w_component_1[j]], 0)\n",
        "        f1_w[j, i] = f_ex_3_2([w_component_0[i], w_component_1[j]], 1)\n",
        "        f2_w[j, i] = f_ex_3_2([w_component_0[i], w_component_1[j]], 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beneficial-thong",
      "metadata": {
        "id": "beneficial-thong"
      },
      "outputs": [],
      "source": [
        "fig = plotly.subplots.make_subplots(rows=4, cols=1, vertical_spacing=0.05)\n",
        "fig.add_contour(x=w_component_0, y=w_component_1, z=f_w, row=1, col=1, coloraxis=\"coloraxis\")\n",
        "fig.add_contour(x=w_component_0, y=w_component_1, z=f0_w, row=2, col=1, coloraxis=\"coloraxis\")\n",
        "fig.add_contour(x=w_component_0, y=w_component_1, z=f1_w, row=3, col=1, coloraxis=\"coloraxis\")\n",
        "fig.add_contour(x=w_component_0, y=w_component_1, z=f2_w, row=4, col=1, coloraxis=\"coloraxis\")\n",
        "fig.update_layout(title=\"Trid function - contour plot\", width=512, height=3.25 * 512, autosize=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "small-option",
      "metadata": {
        "id": "small-option"
      },
      "source": [
        "> The plot of (especially) the third function $f_2$ is very different from $f$. There is a poor correlation between $f$ and $f_2$, meaning that the gradient of $f_2$ may not give an accurate information about $f$. How will the stochastic gradient method will be affected by this?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cooked-cameroon",
      "metadata": {
        "id": "cooked-cameroon"
      },
      "source": [
        "2. Compute the gradient $\\nabla f$ and determine the global minimum of the function $f$. Also compute the gradient $\\nabla f_0$, $\\nabla f_1$ and $\\nabla f_2$.\n",
        "\n",
        "*Solution*:\n",
        "> Taking the partial derivatives of\n",
        "$$f(\\boldsymbol{w}) = (w^{(0)} - 1)^2 + (w^{(1)} - 1)^2 - w^{(0)}w^{(1)}$$\n",
        "and\n",
        "$$f_0(\\boldsymbol{w}) = (w^{(0)} - 1)^2, \\qquad f_1(\\boldsymbol{w}) = (w^{(1)} - 1)^2, \\qquad f_2(\\boldsymbol{w}) = - w^{(0)}w^{(1)}$$\n",
        "we can easily see that\n",
        "$$\\nabla f(\\boldsymbol{w}) = \\begin{bmatrix}\n",
        "2 w^{(0)} - w^{(1)} - 2 \\\\\n",
        "- w^{(0)} + 2 w^{(1)} - 2\n",
        "\\end{bmatrix},\n",
        "\\quad\n",
        "\\nabla f_0(\\boldsymbol{w}) = \\begin{bmatrix}\n",
        "2 w^{(0)} - 2\\\\\n",
        "0\n",
        "\\end{bmatrix},\n",
        "\\quad\n",
        "\\nabla f_1(\\boldsymbol{w}) = \\begin{bmatrix}\n",
        "0\\\\\n",
        "2 w^{(1)} - 2\n",
        "\\end{bmatrix},\n",
        "\\quad\n",
        "\\nabla f_2(\\boldsymbol{w}) = \\begin{bmatrix}\n",
        "- w^{(1)}\\\\\n",
        "- w^{(0)}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "The only stationary point of $f$ is $\\boldsymbol{w}^* = (2, 2)$, as can be seen by solving $\\nabla f(\\boldsymbol{w}) = 0$. Furthermore, it is a global minimum because the hessian of $f$ is always positive definite, as the following calculation show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "characteristic-island",
      "metadata": {
        "id": "characteristic-island"
      },
      "outputs": [],
      "source": [
        "hessian_f = np.array([[2, -1], [-1, 2]])\n",
        "hessian_f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "destroyed-agency",
      "metadata": {
        "id": "destroyed-agency"
      },
      "outputs": [],
      "source": [
        "eigs, _ = np.linalg.eig(hessian_f)\n",
        "eigs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "blind-director",
      "metadata": {
        "id": "blind-director"
      },
      "outputs": [],
      "source": [
        "assert (eigs > 0).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "impressed-capacity",
      "metadata": {
        "id": "impressed-capacity"
      },
      "source": [
        "> From the calculated eigenvalues we see that $f$ is $L$-smooth with $L = 3$ and $\\mu$-strongly convex with $\\mu = 1$.\n",
        ">\n",
        "> We next implement the gradient evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thirty-involvement",
      "metadata": {
        "id": "thirty-involvement"
      },
      "outputs": [],
      "source": [
        "def grad_f_ex_3_2(w: np.ndarray, addend: int = None) -> np.ndarray:\n",
        "    r\"\"\"Evaluate \\nabla f(w) if addend is None, or \\nabla f_{addend}(w) if addend is an integer.\"\"\"\n",
        "    if addend == 0:\n",
        "        return np.array([2 * w[0] - 2, 0])\n",
        "    elif addend == 1:\n",
        "        return np.array([0, 2 * w[1] - 2])\n",
        "    elif addend == 2:\n",
        "        return np.array([- w[0], - w[1]])\n",
        "    elif addend is None:\n",
        "        return np.array([2 * w[0] - w[1] - 2, - w[0] + 2 * w[1] - 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "macro-anderson",
      "metadata": {
        "id": "macro-anderson"
      },
      "source": [
        "> We test the implemented functions at $\\boldsymbol{w}^*$. First of all, we get the optimal function value $f(\\boldsymbol{w}^*)$ and check that $\\nabla f(\\boldsymbol{w}^*) = \\boldsymbol{0}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mounted-struggle",
      "metadata": {
        "id": "mounted-struggle"
      },
      "outputs": [],
      "source": [
        "w_star = np.array([2, 2])\n",
        "w_star"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "available-tumor",
      "metadata": {
        "id": "available-tumor"
      },
      "outputs": [],
      "source": [
        "f_ex_3_2(w_star)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "residential-bullet",
      "metadata": {
        "id": "residential-bullet"
      },
      "outputs": [],
      "source": [
        "grad_f_ex_3_2(w_star)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "relevant-recording",
      "metadata": {
        "id": "relevant-recording"
      },
      "source": [
        "> We then evaluate $f_j(\\boldsymbol{w}^*)$ and $\\nabla f_j(\\boldsymbol{w}^*)$, for $j = 0, 1, 2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "expanded-contemporary",
      "metadata": {
        "id": "expanded-contemporary"
      },
      "outputs": [],
      "source": [
        "f_ex_3_2(w_star, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ranking-conjunction",
      "metadata": {
        "id": "ranking-conjunction"
      },
      "outputs": [],
      "source": [
        "f_ex_3_2(w_star, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "alone-earth",
      "metadata": {
        "id": "alone-earth"
      },
      "outputs": [],
      "source": [
        "f_ex_3_2(w_star, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pursuant-sussex",
      "metadata": {
        "id": "pursuant-sussex"
      },
      "outputs": [],
      "source": [
        "grad_f_ex_3_2(w_star, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hollow-childhood",
      "metadata": {
        "id": "hollow-childhood"
      },
      "outputs": [],
      "source": [
        "grad_f_ex_3_2(w_star, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "informational-romance",
      "metadata": {
        "id": "informational-romance"
      },
      "outputs": [],
      "source": [
        "grad_f_ex_3_2(w_star, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "answering-section",
      "metadata": {
        "id": "answering-section"
      },
      "source": [
        "> Notice how *none* of $\\nabla f_j(\\boldsymbol{w}^*) = \\boldsymbol{0}$, for $j = 0, 1, 2$. How will the stochastic gradient method will be affected by this? This is particularly worrisome, because $\\nabla f_j(\\boldsymbol{w}^*) \\neq 0$ means that the stochastic gradient method believes that it should take a non-zero step even from the optimum $\\boldsymbol{w}^*$!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "linear-onion",
      "metadata": {
        "id": "linear-onion"
      },
      "source": [
        "3. Choose $\\alpha = 0.1 / 3$, $\\varepsilon = 10^{-2}$, $K_{\\max} = 1000$ and $\\boldsymbol{w}_0 = (0, -2)$. Visualize:\n",
        "   * an animation of the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$ on a contour plot of $f$;\n",
        "   * a semilogarithimic plot of error in the function value $\\{f(\\boldsymbol{w}_k) - f(\\boldsymbol{w}^*)\\}_k$ versus the iteration counter $k$;\n",
        "   * a semilogarithimic plot of the norm of the gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$ versus the iteration counter $k$.\n",
        " \n",
        "   Since the choice of addend is random, set a seed (for reproducibility) and run the stochastic gradient method four times (and expect slightly different results every time).\n",
        " \n",
        "*Solution*:\n",
        "> We may reuse the implementation of the stochastic gradient method from the previous exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adjustable-hartford",
      "metadata": {
        "id": "adjustable-hartford"
      },
      "outputs": [],
      "source": [
        "all_w = [None] * 4\n",
        "all_f = [None] * 4\n",
        "all_grad_f = [None] * 4\n",
        "\n",
        "np.random.seed(32)\n",
        "for run in range(4):\n",
        "    all_w[run], all_f[run], all_grad_f[run] = stochastic_gradient(\n",
        "        3, f_ex_3_2, grad_f_ex_3_2, 0.1 / 3, 1e-2, 1000, np.array([0.0, -2.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "upset-chase",
      "metadata": {
        "id": "upset-chase"
      },
      "source": [
        "> We notice that none of the runs converged. The computation of `min_K` below indeed returns 1001."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "heated-translator",
      "metadata": {
        "id": "heated-translator"
      },
      "outputs": [],
      "source": [
        "min_K = min([all_f[run].shape[0] for run in range(4)])\n",
        "min_K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "forty-premiere",
      "metadata": {
        "id": "forty-premiere"
      },
      "outputs": [],
      "source": [
        "max_K = max([all_f[run].shape[0] for run in range(4)])\n",
        "max_K"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beneficial-knife",
      "metadata": {
        "id": "beneficial-knife"
      },
      "source": [
        "> A table of the evolution of the cost function clearly shows that the method is oscillating close to the optimum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abroad-revelation",
      "metadata": {
        "id": "abroad-revelation"
      },
      "outputs": [],
      "source": [
        "np.vstack([all_f[run][:min_K] for run in range(4)]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "superb-impossible",
      "metadata": {
        "id": "superb-impossible"
      },
      "source": [
        "> A plot of the error on the function value clearly shows that after an almost monotone improvement in the first iterations, the stochastic gradient method starts to oscillate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comic-halifax",
      "metadata": {
        "id": "comic-halifax"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "for run in range(4):\n",
        "    fig.add_scatter(\n",
        "        x=np.arange(all_f[run].shape[0]), y=(all_f[run] - f_ex_3_2(w_star)),\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[run], size=10),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[run], width=2),\n",
        "        mode=\"lines+markers\", name=\"run \" + str(run)\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=\"Trid function - error on the function value\",\n",
        "    width=768, height=768, autosize=False\n",
        ")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acute-plasma",
      "metadata": {
        "id": "acute-plasma"
      },
      "source": [
        "> A similar phenomen happens for the norm of the gradient of the cost function $f$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "practical-intensity",
      "metadata": {
        "id": "practical-intensity"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "for run in range(4):\n",
        "    fig.add_scatter(\n",
        "        x=np.arange(all_f[run].shape[0]), y=np.linalg.norm(all_grad_f[run], axis=1),\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[run], size=10),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[run], width=2),\n",
        "        mode=\"lines+markers\", name=\"run \" + str(run)\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=\"Trid function - norm of the gradient\",\n",
        "    width=768, height=768, autosize=False\n",
        ")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "supposed-disposal",
      "metadata": {
        "id": "supposed-disposal"
      },
      "source": [
        "> The animation of the optimization variables over the countour plot provides us with an intution of what is happening. After a first phase in which the stochastic method is getting closer to the optimum, iterations start to have large oscillatations around the minimum point. This is a peculiar phenomenon which happens quite frequently with the stochastic gradient method with constant step size. It is due to the fact that, as we saw at item 2, minimum points of $f$ are not necessarily minum points of $f_j$, and therefore a non zero (and possibly big) update step is performed even close to the global minimum. We will discuss more about why it happens in the lecture.\n",
        ">\n",
        "> Uncomment the axis updates line to zoom in close to the optimum and better appreciate the oscillations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "herbal-attribute",
      "metadata": {
        "id": "herbal-attribute"
      },
      "outputs": [],
      "source": [
        "# Add opaque contour plot\n",
        "fig = go.Figure(data=[go.Contour(\n",
        "    x=w_component_0, y=w_component_1, z=f_w,\n",
        "    showscale=False, visible=True, opacity=0.5\n",
        ")])\n",
        "\n",
        "# Add a red cross marker to locate w*\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=[w_star[0]], y=[w_star[1]],\n",
        "               marker=dict(color=\"red\", size=10, symbol=\"x\"),\n",
        "               mode=\"markers\", name=\"w*\", visible=True))\n",
        "\n",
        "# Prepare a slider every 10 iterations\n",
        "slides = []\n",
        "for k in range(int(max_K / 10)):\n",
        "    for run in range(4):\n",
        "        # Set non uniform marker size to highlight the current iteration k\n",
        "        marker_size = np.zeros((10 * k + 1, ))\n",
        "        marker_size[-1] = 10\n",
        "        # Add lines\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=all_w[run][:10 * k + 1, 0],\n",
        "                       y=all_w[run][:10 * k + 1, 1],\n",
        "                       visible=False,\n",
        "                       marker=dict(color=plotly.colors.qualitative.Set1[run], size=marker_size),\n",
        "                       line=dict(color=plotly.colors.qualitative.Set1[run], width=2),\n",
        "                       mode=\"lines+markers\",\n",
        "                       name=\"run \" + str(run) + \" at k = \" + str(10 * k).zfill(3)))\n",
        "\n",
        "    # Add slider tick\n",
        "    slide = {\n",
        "        \"method\": \"update\",\n",
        "        \"args\": [\n",
        "            {\"visible\": [False] * (4 * int(max_K / 10) + 2)},\n",
        "            {}\n",
        "        ]\n",
        "    }\n",
        "    slide[\"args\"][0][\"visible\"][0] = True\n",
        "    slide[\"args\"][0][\"visible\"][1] = True\n",
        "    for run in range(4):\n",
        "        slide[\"args\"][0][\"visible\"][4 * k + run + 2] = True\n",
        "    slides.append(slide)\n",
        "\n",
        "for run in range(4):\n",
        "    fig.data[run + 2].visible = True\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Trid function - optimization variable iterations over contour plot\",\n",
        "    width=612, height=512, autosize=False,\n",
        "    sliders=[dict(steps=slides)]\n",
        ")\n",
        "# fig.update_xaxes(range=[0, 4])\n",
        "# fig.update_yaxes(range=[0, 4])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "voluntary-label",
      "metadata": {
        "id": "voluntary-label"
      },
      "source": [
        "> Notice that the step length suggested by the gradient method theory was $1 / L = 1 / 3$. Here we used $0.1 / 3$, which is 10 times smaller. To convince us that the oscillation are an issue for the stochastic gradient method, but not for the standard gradient descent, we will also run a standard gradient descent method with same step length. The implementation of the gradient method is copied from Exercise 1.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "developmental-vitamin",
      "metadata": {
        "id": "developmental-vitamin"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(\n",
        "    f: typing.Callable, grad_f: typing.Callable, alpha: float, epsilon: float, maxit: int, w_0: np.ndarray\n",
        ") -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run the gradient descent method with constant step length.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    f, grad_f : Python function\n",
        "        callable evaluating the cost function and its gradient, respectively.\n",
        "    alpha : float\n",
        "        constant step length.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the norm of the gradient of the cost.\n",
        "    maxit : int\n",
        "        maximum number of allowed iterations.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    \"\"\"\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_f = [f(w_0)]\n",
        "    all_grad_f = [grad_f(w_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Use the norm of the gradient as stopping criterion.\n",
        "    while np.linalg.norm(all_grad_f[k]) > epsilon:\n",
        "        w_k = all_w[k]\n",
        "        grad_f_k = all_grad_f[k]\n",
        "        w_k_plus_1 = w_k - alpha * grad_f_k\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f(w_k_plus_1))\n",
        "        all_grad_f.append(grad_f(w_k_plus_1))\n",
        "\n",
        "        # Bail out if the descent condition is not satisfied\n",
        "        if all_f[k + 1] >= all_f[k]:\n",
        "            print(all_f[k + 1], all_f[k])\n",
        "            print(\"WARNING: descent conditions is not satisfied\")\n",
        "            break\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "        # Bail out if exceeded allowed number of iterations\n",
        "        if k >= maxit:\n",
        "            print(\"WARNING: gradient method exceeded number of allowed iterations\")\n",
        "            break\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_f), np.array(all_grad_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "human-witch",
      "metadata": {
        "id": "human-witch"
      },
      "source": [
        "> The gradient descent method does converge, in approximately 200 iterations, and the function values do not oscillate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "round-iceland",
      "metadata": {
        "id": "round-iceland"
      },
      "outputs": [],
      "source": [
        "all_w_gradient, all_f_gradient, all_grad_f_gradient = gradient_descent(\n",
        "    f_ex_3_2, grad_f_ex_3_2, 0.1 / 3, 1e-2, 1000, np.array([0.0, -2.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "upper-circular",
      "metadata": {
        "id": "upper-circular"
      },
      "outputs": [],
      "source": [
        "all_w_gradient.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "downtown-shift",
      "metadata": {
        "id": "downtown-shift"
      },
      "outputs": [],
      "source": [
        "all_f_gradient - f_ex_3_2(w_star)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "modern-newman",
      "metadata": {
        "id": "modern-newman"
      },
      "source": [
        "## Exercise 3.3 (continuation of Exercise 3.2)\n",
        "Let $\\boldsymbol{w} \\in \\mathbb{R}^2$. Consider the *Trid function*\n",
        "$$f(\\boldsymbol{w}) = (w^{(0)} - 1)^2 + (w^{(1)} - 1)^2 - w^{(0)}w^{(1)}$$\n",
        "as the sum of three functions\n",
        "$$f_0(\\boldsymbol{w}) = (w^{(0)} - 1)^2, \\qquad f_1(\\boldsymbol{w}) = (w^{(1)} - 1)^2, \\qquad f_2(\\boldsymbol{w}) = - w^{(0)}w^{(1)}.$$\n",
        "\n",
        "4. Implement the stochastic gradient method with decreasing step length in a Python function. Such function should:\n",
        "   * take as input the number $m$ of addends, the function $f$, its gradient $\\nabla f$, the value $\\beta$ and $\\gamma$ used in the computation of the step length, the tolerance $\\varepsilon$ for the stopping criterion, maximum number $K_{\\max}$ of allowed iterations, and the initial condition $\\boldsymbol{w}_{0}$;\n",
        "   * return as outputs the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$, the corresponding function values $\\{f(\\boldsymbol{w}_k)\\}_k$ and gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$.\n",
        "\n",
        "   Use the stopping criterion based on the norm of the gradient.\n",
        " \n",
        "*Solution*:\n",
        "> We start from our previous implementation of the stochastic gradient and change the computation of the step length $\\alpha_k$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "asian-italic",
      "metadata": {
        "id": "asian-italic"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_decreasing_step_length(\n",
        "    m: int, f: typing.Callable, grad_f: typing.Callable, beta: float, gamma: float, epsilon: float,\n",
        "    maxit: int, w_0: np.ndarray\n",
        ") -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run the stochastic gradient method with decreasing step length.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    m : int\n",
        "        number of addends in the expression of the cost function.\n",
        "    f, grad_f : Python function\n",
        "        callable evaluating the cost function and its gradient, respectively.\n",
        "    beta, gamma : float\n",
        "        constants used in the computation of the step length.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the norm of the gradient of the cost.\n",
        "    maxit : int\n",
        "        maximum number of allowed iterations.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    \"\"\"\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_f = [f(w_0)]\n",
        "    all_grad_f = [grad_f(w_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Use the norm of the gradient as stopping criterion.\n",
        "    while np.linalg.norm(all_grad_f[k]) > epsilon:\n",
        "        w_k = all_w[k]\n",
        "\n",
        "        # Draw a random index\n",
        "        j_k = np.random.randint(m)\n",
        "\n",
        "        # Compute the update direction\n",
        "        g_k = - grad_f(w_k, addend=j_k)\n",
        "\n",
        "        # Compute alpha_k\n",
        "        alpha_k = beta / (k + gamma)\n",
        "\n",
        "        # Compute w_{k + 1}\n",
        "        w_k_plus_1 = w_k + alpha_k * g_k\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f(w_k_plus_1))\n",
        "        all_grad_f.append(grad_f(w_k_plus_1))\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "        # Bail out if exceeded allowed number of iterations\n",
        "        if k >= maxit:\n",
        "            print(\"WARNING: stochastic gradient method exceeded number of allowed iterations\")\n",
        "            break\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_f), np.array(all_grad_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "invisible-amendment",
      "metadata": {
        "id": "invisible-amendment"
      },
      "source": [
        "5. Choose $\\beta = 1.01$, $\\gamma = 3$, $\\varepsilon = 10^{-2}$, $K_{\\max} = 10000$ and $\\boldsymbol{w}_0 = (0, -2)$. Visualize:\n",
        "   * an animation of the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$ on a contour plot of $f$;\n",
        "   * a semilogarithimic plot of error in the function value $\\{f(\\boldsymbol{w}_k) - f(\\boldsymbol{w}^*)\\}_k$ versus the iteration counter $k$;\n",
        "   * a semilogarithimic plot of the norm of the gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$ versus the iteration counter $k$.\n",
        " \n",
        "   Since the choice of addend is random, set a seed (for reproducibility) and run the stochastic gradient method with decreasing step size four times (and expect slightly different results every time).\n",
        "\n",
        "*Solution*:\n",
        "> Notice that $\\mu = 1$ and $L = 3$, so indeed $\\beta > \\frac{1}{\\mu}$ and $\\gamma = \\frac{L}{\\mu}$ as required by the convergence result. We run the our implementation of the stochastic gradient method with diminishing step size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "occupational-custody",
      "metadata": {
        "id": "occupational-custody"
      },
      "outputs": [],
      "source": [
        "def f_ex_3_3(w: np.ndarray, addend: int = None) -> float:\n",
        "    r\"\"\"Evaluate f(w) if addend is None, or f_{addend}(w) if addend is an integer.\"\"\"\n",
        "    if addend == 0:\n",
        "        return (w[0] - 1)**2\n",
        "    elif addend == 1:\n",
        "        return (w[1] - 1)**2\n",
        "    elif addend == 2:\n",
        "        return - w[0] * w[1]\n",
        "    elif addend is None:\n",
        "        return (w[0] - 1)**2 + (w[1] - 1)**2 - w[0] * w[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "infectious-impression",
      "metadata": {
        "id": "infectious-impression"
      },
      "outputs": [],
      "source": [
        "def grad_f_ex_3_3(w: np.ndarray, addend: int = None) -> np.ndarray:\n",
        "    r\"\"\"Evaluate \\nabla f(w) if addend is None, or \\nabla f_{addend}(w) if addend is an integer.\"\"\"\n",
        "    if addend == 0:\n",
        "        return np.array([2 * w[0] - 2, 0])\n",
        "    elif addend == 1:\n",
        "        return np.array([0, 2 * w[1] - 2])\n",
        "    elif addend == 2:\n",
        "        return np.array([- w[0], - w[1]])\n",
        "    elif addend is None:\n",
        "        return np.array([2 * w[0] - w[1] - 2, - w[0] + 2 * w[1] - 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shaped-protocol",
      "metadata": {
        "id": "shaped-protocol"
      },
      "outputs": [],
      "source": [
        "all_w = [None] * 4\n",
        "all_f = [None] * 4\n",
        "all_grad_f = [None] * 4\n",
        "\n",
        "np.random.seed(33 + 500)\n",
        "for run in range(4):\n",
        "    all_w[run], all_f[run], all_grad_f[run] = stochastic_gradient_decreasing_step_length(\n",
        "        3, f_ex_3_3, grad_f_ex_3_3, 1.01, 3, 1e-2, 10000, np.array([0.0, -2.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "brilliant-possession",
      "metadata": {
        "id": "brilliant-possession"
      },
      "source": [
        "> We notice that none of the runs converged in the prescribed number of iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "higher-allen",
      "metadata": {
        "id": "higher-allen"
      },
      "outputs": [],
      "source": [
        "min_K = min([all_f[run].shape[0] for run in range(4)])\n",
        "min_K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "psychological-faith",
      "metadata": {
        "id": "psychological-faith"
      },
      "outputs": [],
      "source": [
        "max_K = max([all_f[run].shape[0] for run in range(4)])\n",
        "max_K"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "advanced-adjustment",
      "metadata": {
        "id": "advanced-adjustment"
      },
      "source": [
        "> A plot of the error on the function values, as well as a plot of the norm of the gradient, reveals that:\n",
        "> * the convergence is not affected anymore by oscillations, but\n",
        "> * it is extremely slow, because the step lengths are decreasing very rapidly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "marine-sperm",
      "metadata": {
        "id": "marine-sperm"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "for run in range(4):\n",
        "    fig.add_scatter(\n",
        "        x=np.arange(all_f[run].shape[0])[::10], y=(all_f[run][::10] - f_ex_3_3(w_star)),\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[run], size=10),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[run], width=2),\n",
        "        mode=\"lines+markers\", name=\"run \" + str(run)\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=\"Trid function - error on the function value\",\n",
        "    width=768, height=768, autosize=False\n",
        ")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "opposed-yugoslavia",
      "metadata": {
        "id": "opposed-yugoslavia"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "for run in range(4):\n",
        "    fig.add_scatter(\n",
        "        x=np.arange(all_f[run].shape[0])[::10], y=np.linalg.norm(all_grad_f[run][::10], axis=1),\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[run], size=10),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[run], width=2),\n",
        "        mode=\"lines+markers\", name=\"run \" + str(run)\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=\"Trid function - norm of the gradient\",\n",
        "    width=768, height=768, autosize=False\n",
        ")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "capable-visit",
      "metadata": {
        "id": "capable-visit"
      },
      "source": [
        "> The slow convergence is quite clear also from the animation of the optimization variables. Zoom in close to the optimum by uncommenting two lines towards the end of the last cell, and compare iteration 3000 to iteration 10000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stainless-executive",
      "metadata": {
        "id": "stainless-executive"
      },
      "outputs": [],
      "source": [
        "domain_component_0 = [-4, 4]\n",
        "domain_component_1 = [-4, 4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "brutal-bicycle",
      "metadata": {
        "id": "brutal-bicycle"
      },
      "outputs": [],
      "source": [
        "w_component_0 = np.linspace(domain_component_0[0], domain_component_0[1], 100)\n",
        "w_component_1 = np.linspace(domain_component_1[0], domain_component_1[1], 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "chronic-karma",
      "metadata": {
        "id": "chronic-karma"
      },
      "outputs": [],
      "source": [
        "f_w = np.zeros((len(w_component_0), len(w_component_1)))\n",
        "for i in range(f_w.shape[0]):\n",
        "    for j in range(f_w.shape[1]):\n",
        "        f_w[j, i] = f_ex_3_3([w_component_0[i], w_component_1[j]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "reliable-blond",
      "metadata": {
        "id": "reliable-blond"
      },
      "outputs": [],
      "source": [
        "w_star = np.array([2, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "affected-poland",
      "metadata": {
        "id": "affected-poland"
      },
      "outputs": [],
      "source": [
        "# Add opaque contour plot\n",
        "fig = go.Figure(data=[go.Contour(\n",
        "    x=w_component_0, y=w_component_1, z=f_w,\n",
        "    showscale=False, visible=True, opacity=0.5\n",
        ")])\n",
        "\n",
        "# Add a red cross marker to locate w*\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=[w_star[0]], y=[w_star[1]],\n",
        "               marker=dict(color=\"red\", size=10, symbol=\"x\"),\n",
        "               mode=\"markers\", name=\"w*\", visible=True))\n",
        "\n",
        "# Prepare a slider every 100 iterations\n",
        "slides = []\n",
        "for k in range(int(max_K / 100)):\n",
        "    for run in range(4):\n",
        "        # Set non uniform marker size to highlight the current iteration k\n",
        "        marker_size = np.zeros((100 * k + 1, ))\n",
        "        marker_size[-1] = 10\n",
        "        # Add lines\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=all_w[run][:100 * k + 1:10, 0],\n",
        "                       y=all_w[run][:100 * k + 1:10, 1],\n",
        "                       visible=False,\n",
        "                       marker=dict(color=plotly.colors.qualitative.Set1[run], size=marker_size),\n",
        "                       line=dict(color=plotly.colors.qualitative.Set1[run], width=2),\n",
        "                       mode=\"lines+markers\",\n",
        "                       name=\"run \" + str(run) + \" at k = \" + str(100 * k).zfill(4)))\n",
        "\n",
        "    # Add slider tick\n",
        "    slide = {\n",
        "        \"method\": \"update\",\n",
        "        \"args\": [\n",
        "            {\"visible\": [False] * (4 * int(max_K / 100) + 2)},\n",
        "            {}\n",
        "        ]\n",
        "    }\n",
        "    slide[\"args\"][0][\"visible\"][0] = True\n",
        "    slide[\"args\"][0][\"visible\"][1] = True\n",
        "    for run in range(4):\n",
        "        slide[\"args\"][0][\"visible\"][4 * k + run + 2] = True\n",
        "    slides.append(slide)\n",
        "\n",
        "for run in range(4):\n",
        "    fig.data[run + 2].visible = True\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Trid function - optimization variable iterations over contour plot\",\n",
        "    width=612, height=512, autosize=False,\n",
        "    sliders=[dict(steps=slides)]\n",
        ")\n",
        "# fig.update_xaxes(range=[0, 4])\n",
        "# fig.update_yaxes(range=[0, 4])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "understanding-pavilion",
      "metadata": {
        "id": "understanding-pavilion"
      },
      "source": [
        "> The learning schedule $\\alpha_k = \\frac{\\beta}{k + \\gamma}$ is decreasing the step length too aggressively, to the point that the step $\\alpha_k$ vanishes for $k$ large. Note that we have already encountered a similar issue when discussing Adagrad (and RMSProp, which solves the issue).\n",
        "> Overall, the linearly decaying schedule is not satisfying. Feel free to try your implementation of RMSProp or Adam (Homework $\\alpha.1$) to see if they perform better than the linearly decaying schedule. We will propose an alternative method later on this exercise that gives more satisfactory results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "given-turner",
      "metadata": {
        "id": "given-turner"
      },
      "source": [
        "6. Implement the mini-batch stochastic gradient method with constant step length in a Python function. Such function should:\n",
        "   * take as input the number $m$ of addends, the size $m_b$ of a mini-batch, the function $f$, its gradient $\\nabla f$, the value $\\alpha$ of the step length, the tolerance $\\varepsilon$ for the stopping criterion, maximum number $K_{\\max}$ of allowed iterations, and the initial condition $\\boldsymbol{w}_{0}$;\n",
        "   * return as outputs the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$, the corresponding function values $\\{f(\\boldsymbol{w}_k)\\}_k$ and gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$.\n",
        "\n",
        "   Use the stopping criterion based on the norm of the gradient.\n",
        " \n",
        "*Solution*:\n",
        "> We start from our previous implementation of the stochastic gradient and change the computation of the update direction. To extract multiple indices we use the [`np.random.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html). Note that, in contrast to the convergence results in the slides, we use index selection *without* replacement, because we do not want some of the extracted indices to be repeated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rocky-coast",
      "metadata": {
        "id": "rocky-coast"
      },
      "outputs": [],
      "source": [
        "def mini_batch_stochastic_gradient(\n",
        "    m: int, m_b: int, f: typing.Callable, grad_f: typing.Callable, alpha: float, epsilon: float,\n",
        "    maxit: int, w_0: np.ndarray\n",
        ") -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run the mini-batch stochastic gradient method with constant step length.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    m : int\n",
        "        number of addends in the expression of the cost function.\n",
        "    m_b : int\n",
        "        size of the mini-batch.\n",
        "    f, grad_f : Python function\n",
        "        callable evaluating the cost function and its gradient, respectively.\n",
        "    alpha : float\n",
        "        constant step length.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the norm of the gradient of the cost.\n",
        "    maxit : int\n",
        "        maximum number of allowed iterations.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    \"\"\"\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_f = [f(w_0)]\n",
        "    all_grad_f = [grad_f(w_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Use the norm of the gradient as stopping criterion.\n",
        "    while np.linalg.norm(all_grad_f[k]) > epsilon:\n",
        "        w_k = all_w[k]\n",
        "\n",
        "        # Draw random indices\n",
        "        J_k = np.random.choice(m, size=m_b, replace=False)\n",
        "\n",
        "        # Compute the update direction\n",
        "        g_k = - 1 / m_b * sum([grad_f(w_k, addend=j) for j in J_k])\n",
        "\n",
        "        # Compute w_{k + 1}\n",
        "        w_k_plus_1 = w_k + alpha * g_k\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f(w_k_plus_1))\n",
        "        all_grad_f.append(grad_f(w_k_plus_1))\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "        # Bail out if exceeded allowed number of iterations\n",
        "        if k >= maxit:\n",
        "            print(\"WARNING: stochastic gradient method exceeded number of allowed iterations\")\n",
        "            break\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_f), np.array(all_grad_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conventional-blogger",
      "metadata": {
        "id": "conventional-blogger"
      },
      "source": [
        "7. Choose $\\alpha = 0.1 / 3$, $\\varepsilon = 10^{-2}$, $K_{\\max} = 1000$ and $\\boldsymbol{w}_0 = (0, -2)$. For $m_b = 1, 2$ or $3$, visualize:\n",
        "   * a semilogarithimic plot of the norm of the gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$ versus the iteration counter $k$.\n",
        " \n",
        "   Since the choice of addend is random, set a seed (for reproducibility) and run four times the mini-batch stochastic gradient (and expect slightly different results every time).\n",
        " \n",
        "*Solution*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "classical-custom",
      "metadata": {
        "id": "classical-custom"
      },
      "outputs": [],
      "source": [
        "all_w = {m_b: [None] * 4 for m_b in range(1, 4)}\n",
        "all_f = {m_b: [None] * 4 for m_b in range(1, 4)}\n",
        "all_grad_f = {m_b: [None] * 4 for m_b in range(1, 4)}\n",
        "\n",
        "np.random.seed(33 + 700)\n",
        "for m_b in range(1, 4):\n",
        "    for run in range(4):\n",
        "        all_w[m_b][run], all_f[m_b][run], all_grad_f[m_b][run] = mini_batch_stochastic_gradient(\n",
        "            3, m_b, f_ex_3_3, grad_f_ex_3_3, 0.1 / 3, 1e-2, 1000, np.array([0.0, -2.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "widespread-former",
      "metadata": {
        "id": "widespread-former"
      },
      "source": [
        "> Many runs have exceeded the number of allowed iterations, so we expect to see oscillations in the plot of the norm of the gradient. We plot the three cases $m_b = 1, 2$ or $3$ from the top to the bottom."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "drawn-assistant",
      "metadata": {
        "id": "drawn-assistant"
      },
      "outputs": [],
      "source": [
        "fig = plotly.subplots.make_subplots(rows=3, cols=1, vertical_spacing=0.05)\n",
        "for m_b in range(1, 4):\n",
        "    for run in range(4):\n",
        "        fig.add_scatter(\n",
        "            x=np.arange(all_f[m_b][run].shape[0]), y=np.linalg.norm(all_grad_f[m_b][run], axis=1),\n",
        "            marker=dict(color=plotly.colors.qualitative.Set1[run], size=10),\n",
        "            line=dict(color=plotly.colors.qualitative.Set1[run], width=2),\n",
        "            mode=\"lines+markers\", name=\"run \" + str(run), showlegend=(m_b == 1),\n",
        "            row=m_b, col=1\n",
        "        )\n",
        "fig.update_layout(\n",
        "    title=\"Trid function - norm of the gradient\",\n",
        "    width=768, height=2.5 * 768, autosize=False\n",
        ")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fitting-nightlife",
      "metadata": {
        "id": "fitting-nightlife"
      },
      "source": [
        "> * The most striking difference between the three plots is the fact that the third one ($m_b = 3$) does not have any oscillation. This is indeed because, since we are extracting indices without replacment, the case $m_b = m = 3$ corresponds to the gradient method!\n",
        "> * The first and second plot ($m_b = 1$ and $m_b = 2$, respectively), look similar, as they both present oscillations. However, one should notice that the second plot is better than the first one (i.e., oscillations are less pronounced). Compare for instance in $k \\in [800, 1000]$: the error oscillates between 2 and 20 for $m_b = 1$, while it is between $0.5$ and $5$ for $m_b = 2$.\n",
        ">\n",
        "> Unfortunately, even if the two options proposed in the lecture gave some improvement compared to the basic stochastic gradient method, none of them gave fully satisfactory results. Therefore, in the final question of this exercise we will propose a (very popular) combination of them: using a mini-batch method with step sizes that are decreasing (but not as fast as $1/k$).\n",
        "\n",
        "8. Consider a mini-batch stochastic method with the following learning schedule:\n",
        "   1. the initial step length is set as $\\alpha_0 = \\frac{\\beta}{\\gamma}$.\n",
        "   2. proceed with the iterations of a mini-batch stochastic method with constant step $\\alpha_0$ until oscillations are detected.\n",
        "   3. as soon as oscillations are detected, decrease the step length to $\\alpha_1 = \\frac{\\beta}{(1 + \\gamma)}$.\n",
        "   4. proceed with the iterations of a mini-batch stochastic method with constant step $\\alpha_1$ until oscillations are detected.\n",
        "   5. as soon as oscillations are detected, decrease the step length to $\\alpha_2 = \\frac{\\beta}{(2 + \\gamma)}$.\n",
        "   6. ... and so on ...\n",
        " \n",
        "   The proposed learning schedule is decreasing the step size less agressively than the linear decay shown in the lecture. Indeed, the proposed sequence of step size is $[\\alpha_0, \\alpha_0, \\dots, \\alpha_0, \\alpha_1, \\alpha_1, \\dots, \\alpha_1, \\alpha_2, \\dots]$, while the linearly decaying strategy would be $[\\alpha_0, \\alpha_1, \\alpha_2, \\alpha_3, \\dots]$.\n",
        " \n",
        "   Implement the proposed method in a Python function. Such function should:\n",
        "   * take as input the number $m$ of addends, the function $f$, its gradient $\\nabla f$, the value $\\beta$ and $\\gamma$ used in the computation of the step length, the tolerance $\\varepsilon$ for the stopping criterion, maximum number $K_{\\max}$ of allowed iterations, and the initial condition $\\boldsymbol{w}_{0}$;\n",
        "   * return as outputs the optimization variable iterations $\\{\\boldsymbol{w}_k\\}_k$, the corresponding function values $\\{f(\\boldsymbol{w}_k)\\}_k$ and gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$, and the resulting sequence of stepsizes.\n",
        "\n",
        "   Use the stopping criterion based on the norm of the gradient. Use the following criterion to detect oscillations:\n",
        "   * if the function value at iteration $k$ is larger than the function value of iteration $k - 10$, and\n",
        "   * at least 10 iterations have passed since the last oscillations were detected.\n",
        " \n",
        "*Solution*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "analyzed-duration",
      "metadata": {
        "id": "analyzed-duration"
      },
      "outputs": [],
      "source": [
        "def mini_batch_stochastic_gradient_variable_step_size(\n",
        "    m: int, m_b: int, f: typing.Callable, grad_f: typing.Callable, beta: float, gamma: float, epsilon: float,\n",
        "    maxit: int, w_0: np.ndarray\n",
        ") -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run the mini-batch stochastic gradient method with variable step length.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    m : int\n",
        "        number of addends in the expression of the cost function.\n",
        "    m_b : int\n",
        "        size of the mini-batch.\n",
        "    f, grad_f : Python function\n",
        "        callable evaluating the cost function and its gradient, respectively.\n",
        "    beta, gamma : float\n",
        "        constants used in the computation of the step length.\n",
        "    epsilon : float\n",
        "        tolerance for the stopping criterion on the error on the norm of the gradient of the cost.\n",
        "    maxit : int\n",
        "        maximum number of allowed iterations.\n",
        "    w_0 : 1d numpy array\n",
        "        numpy array containing the initial condition.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    2d numpy array\n",
        "        history of the optimization variables iterations.\n",
        "    1d numpy array\n",
        "        history of the cost function values.\n",
        "    2d numpy array\n",
        "        history of the gradient of the cost function.\n",
        "    \"\"\"\n",
        "    # Prepare lists collecting the required outputs over the iterations\n",
        "    all_w = [w_0]\n",
        "    all_f = [f(w_0)]\n",
        "    all_grad_f = [grad_f(w_0)]\n",
        "\n",
        "    # Prepare iteration counter\n",
        "    k = 0\n",
        "\n",
        "    # Last iteration at which the step length was changed\n",
        "    k_osc = 0\n",
        "    alpha = beta / gamma\n",
        "    all_alphas = [(k_osc, alpha)]\n",
        "\n",
        "    # Use the norm of the gradient as stopping criterion.\n",
        "    while np.linalg.norm(all_grad_f[k]) > epsilon:\n",
        "        w_k = all_w[k]\n",
        "\n",
        "        # Draw random indices\n",
        "        J_k = np.random.choice(m, size=m_b, replace=False)\n",
        "\n",
        "        # Compute the update direction\n",
        "        g_k = - 1 / m_b * sum([grad_f(w_k, addend=j) for j in J_k])\n",
        "\n",
        "        # Compute w_{k + 1}\n",
        "        w_k_plus_1 = w_k + alpha * g_k\n",
        "\n",
        "        # Update required outputs\n",
        "        all_w.append(w_k_plus_1)\n",
        "        all_f.append(f(w_k_plus_1))\n",
        "        all_grad_f.append(grad_f(w_k_plus_1))\n",
        "\n",
        "        # Detect oscillations\n",
        "        if k >= k_osc + 10 and all_f[-1] > all_f[-11]:\n",
        "            alpha = beta / (len(all_alphas) + gamma)\n",
        "            k_osc = k\n",
        "            all_alphas.append((k_osc, alpha))\n",
        "\n",
        "        # Increment iteration counter\n",
        "        k += 1\n",
        "\n",
        "        # Bail out if exceeded allowed number of iterations\n",
        "        if k >= maxit:\n",
        "            print(\"WARNING: stochastic gradient method exceeded number of allowed iterations\")\n",
        "            break\n",
        "\n",
        "    # For convenience we transform the outputs into numpy array before returning\n",
        "    return np.array(all_w), np.array(all_f), np.array(all_grad_f), np.array(all_alphas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "insured-corpus",
      "metadata": {
        "id": "insured-corpus"
      },
      "source": [
        "9. Choose $m_b = 2$, $\\beta = 1$, $\\gamma = 3$, $\\varepsilon = 10^{-2}$, $K_{\\max} = 10000$ and $\\boldsymbol{w}_0 = (0, -2)$. Visualize:\n",
        "   * a semilogarithimic plot of the norm of the gradients $\\{\\nabla f(\\boldsymbol{w}_k)\\}_k$ versus the iteration counter $k$.\n",
        " \n",
        "   Since the choice of addend is random, set a seed (for reproducibility) and run four times our proposed stochastic gradient method (and expect slightly different results every time).\n",
        " \n",
        "*Solution*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "excessive-culture",
      "metadata": {
        "id": "excessive-culture"
      },
      "outputs": [],
      "source": [
        "all_w = [None] * 4\n",
        "all_f = [None] * 4\n",
        "all_grad_f = [None] * 4\n",
        "all_alphas = [None] * 4\n",
        "\n",
        "np.random.seed(33 + 900)\n",
        "for run in range(4):\n",
        "    all_w[run], all_f[run], all_grad_f[run], all_alphas[run] = mini_batch_stochastic_gradient_variable_step_size(\n",
        "        3, 2, f_ex_3_3, grad_f_ex_3_3, 1, 3, 1e-2, 10000, np.array([0.0, -2.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vocal-patio",
      "metadata": {
        "id": "vocal-patio"
      },
      "source": [
        "> Note that now all runs converged in less than the required number of iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "developmental-geometry",
      "metadata": {
        "id": "developmental-geometry"
      },
      "outputs": [],
      "source": [
        "min_K = min([all_f[run].shape[0] for run in range(4)])\n",
        "min_K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bibliographic-blood",
      "metadata": {
        "id": "bibliographic-blood"
      },
      "outputs": [],
      "source": [
        "max_K = max([all_f[run].shape[0] for run in range(4)])\n",
        "max_K"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "static-eight",
      "metadata": {
        "id": "static-eight"
      },
      "source": [
        "> Our criterion for oscillations ensures that the same step length will be kept for at least 10 iterations. With the help of the code below, note that it often happens that the same step length is kept for more than 10 iterations. This guarantees that the step length does not decay to fast, as it happened in the case of linearly decaying sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "professional-england",
      "metadata": {
        "id": "professional-england"
      },
      "outputs": [],
      "source": [
        "for run in range(4):\n",
        "    print(\"Run =\", run)\n",
        "    print(np.diff(all_alphas[run][:, 0]))\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "determined-planner",
      "metadata": {
        "id": "determined-planner"
      },
      "outputs": [],
      "source": [
        "for run in range(4):\n",
        "    print(\"Run =\", run)\n",
        "    print(\"\\t final step with our sequence is \", all_alphas[run][-1, 1])\n",
        "    print(\"\\t linearly decaying step at the same iteration would have been\", 1 / (all_f[run].shape[0] + 3))\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "musical-landing",
      "metadata": {
        "id": "musical-landing"
      },
      "source": [
        "> Thanks to the improved gradient estimation (mini-batch) and the better sequence of the step lengths, the oscillations in the norm of the gradient are less pronounced and such norm decreases below the provided tolerance in less than 10000 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hindu-sydney",
      "metadata": {
        "id": "hindu-sydney"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "for run in range(4):\n",
        "    fig.add_scatter(\n",
        "        x=np.arange(all_f[run].shape[0])[::10], y=np.linalg.norm(all_grad_f[run], axis=1)[::10],\n",
        "        marker=dict(color=plotly.colors.qualitative.Set1[run], size=10),\n",
        "        line=dict(color=plotly.colors.qualitative.Set1[run], width=2),\n",
        "        mode=\"lines+markers\", name=\"run \" + str(run)\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title=\"Trid function - norm of the gradient\",\n",
        "    width=768, height=768, autosize=False\n",
        ")\n",
        "fig.update_yaxes(type=\"log\", exponentformat=\"power\")\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}